{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#mmg3320-advanced-bioinformatics","title":"MMG3320: Advanced Bioinformatics","text":""},{"location":"#introduction","title":"Introduction","text":"<p>This course is intended for students in the biological sciences who have already completed the introductory course, Survey to Bioinformatic Databases. The advent of next-generation sequencing (NGS) platforms has made sequencing more accessible, faster, and less expensive, resulting in over 150,000 publicly available datasets that can be utilized to support research findings or testing hypotheses. In this course, students will learn and execute key steps in the bioinformatic workflow by processing a publicly available genomics dataset. By the end of the course, students will have accessed, processed, analyzed, visualized, and interpreted an NGS dataset of their choosing. The course covers several topics, including an introduction to UNIX, data processing, R programming basics (including data frames, cleaning, and fundamentals with ggplot2), as well as an in-depth overview of selected genomics analysis. Throughout the course, best practices for reproducible data and data management will be emphasized. The course uses a direct, hands-on approach, as such most classes are interactive and require student participation.</p>"},{"location":"#required-platforms-and-software","title":"Required platforms and software:","text":"<p>Each student will be provided with an account on the Vermont Advanced Computing Cluster (VACC). Students will use VACC - Open OnDemand (OOD) to access their VACC account, command line, and R/RStudio.</p>"},{"location":"#citation","title":"Citation","text":"<p>To cite material from this course in your publications, please use:</p> <p>Jihe Liu, William J. Gammerdinger, Meeta Mistry, Mary E. Piper, &amp; Radhika S. Khetani. (2022, January 6). hbctraining/Intro-to-shell-flipped: Shell and HPC Lessons from HCBC (first release). Zenodo. https://doi.org/10.5281/zenodo.5826091</p> <p>These materials have been developed by members of the teaching team at the Harvard Chan Bioinformatics Core (HBC). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p> <p>Some materials used in these lessons were derived from work that is Copyright \u00a9 Data Carpentry (http://datacarpentry.org/).  All Data Carpentry instructional material is made available under the Creative Commons Attribution license (CC BY 4.0).</p> <p>Other materials were derived from Babraham Bioinformatics - Training Courses. (n.d.). https://www.bioinformatics.babraham.ac.uk/training.html</p>"},{"location":"FAQ/","title":"Frequently Asked Questions","text":""},{"location":"FAQ/#frequently-asked-questions","title":"Frequently Asked Questions","text":"<p>On this page I will update as they come common problems experienced by students when using the VACC cluster with suggestions on how to resolve them. </p>"},{"location":"FAQ/#why-do-i-see-rcommand-not-found","title":"Why do I see <code>$'\\r':command not found</code>?","text":"<p>It is likely that you created the job script on a Windows machine, which uses different newline characters. To overcome this, you can try the following command: </p> <pre><code>sed -i 's/\\r$//' your-script-name.sh\n</code></pre> <p>then run: </p> <pre><code>sh your-script-name.sh\n</code></pre>"},{"location":"FAQ/#setup-of-sra-tools","title":"Setup of SRA tools","text":"<p>When running SRA Tools for the first time it is necessary to run <code>vdb-config --interactive</code> and then press 'x' to save a basic configuration. This is not necessary for further usage. </p>"},{"location":"FAQ/#why-do-i-see-unable-to-locate-a-modulefile-for-gcc","title":"Why do I see <code>Unable to locate a modulefile for 'gcc/'</code>?","text":"<p>It is most likely that you are logging into the \"old\" VACC cluster. This class has transitioned to using the \"new\" VACC cluster which can be accessed with: </p> <pre><code>ssh uvmid@login.vacc.uvm.edu\n\n#replace uvmid id with your netid\n</code></pre> <p>The website for the new VACC cluster on OpenOnDemand can be found here</p>"},{"location":"FAQ/#which-modules-require-loading-gcc1330-xp3epyt-first","title":"Which modules require loading <code>gcc/13.3.0-xp3epyt</code> first?","text":"<p>Using <code>module avail</code> you will be able to view the list of modules that require this compiler:</p> <pre><code>-------- /gpfs1/sw/spack/0.22.2/modules/linux-rhel9-x86_64/gcc/13.3.0 ---------\n   angsd/0.935-4asngpy          hisat2/2.2.1-x7h4grf\n   bamtools/2.5.2-twq7d2p       htslib/1.19.1-6ivqauw\n   bcftools/1.19-iq5mwek        kraken2/2.1.3-5jimqoc\n   bedtools2/2.31.1-xip5kr5     minimap2/2.28-qcu5ixf\n   bismark/0.24.1-zmqux7v       openmpi/4.1.6-67ovor6\n   blast-plus/2.14.1-ca7iit2    picard/3.1.1-otrgwkh\n   boost/1.85.0-bhfhhhm         py-htseq/2.0.3-mb7ap7s\n   bowtie2/2.5.2-qd4omrm        py-multiqc/1.15-fmpaaj7\n   bwa/0.7.17-iqv3cxl           samtools/1.19.2-pfmpoam\n   cgal/5.6-ng5gssh             sratoolkit/3.0.0-y2rspiu\n   eigen/3.4.0-auuuz67          star/2.7.11a-cp575va\n   fastp/0.23.4-mjw7rak         trimgalore/0.6.10-namgrp2\n   fastqc/0.12.1-qxseug5        trimmomatic/0.39-vdnktze\n   freebayes/1.3.6-r67va2b      xtb/6.6.0-ivf4oeb\n</code></pre>"},{"location":"FAQ/#how-do-i-transfer-files-html-pdfs-etc-off-the-vacc","title":"How do I transfer files (HTML, PDFs, etc) off the VACC?","text":"<p>Navigate to the Files tab on OpenOnDemand, then Home Directory. Navigate to the file you would like to download and click on the white box beside it to select. Then click Download!</p>"},{"location":"FAQ/#when-using-hisat2-build-i-encountered-this-error-warning-encountered-reference-sequence-with-only-gaps-what-does-it-mean","title":"When using <code>hisat2-build</code> I encountered this error <code>Warning: Encountered reference sequence with only gaps</code>, what does it mean?","text":"<p>This program requires a FASTA file that is not compressed. To decompress the FASTA file perform:</p> <pre><code>gunzip fasta-file.fa.gz\n\n## example\ngunzip Arabidopsis_thaliana.TAIR10.dna.toplevel.fa.gz\n</code></pre>"},{"location":"answers/","title":"Index","text":""},{"location":"answers/#completed-lessons","title":"Completed Lessons","text":"<p>Here, I will make available the completed Rmarkdown HTML file for each lesson. These will be made available AFTER the lesson is taught. </p> <p>[Introduction to Tidyverse (L19)]- (Intro_to_Tidyverse.html)</p> <p>[Introduction to Tidyverse Part II (L20)] - (Intro_to_Tidyverse_partII.html)</p> <p>[DESeq2 Part I(L21)]- (RNA-Seq_DESeq2_tutorial_040225.html)</p> <p>[RNA-Seq Visualizations Part II (L22)]- (RNA-Seq_DESeq2_PartII_post.html)</p> <p>[RNA-Seq Visualizations Part III (L23)] - (RNA-Seq_Visualization_PartIII_Post.html)</p> <p>[RNA-Seq Visualizations Part IV (L24)]- (RNASeq_Visualizations_PartIV_revise.html)</p>"},{"location":"assignments/","title":"Final Project","text":""},{"location":"assignments/#final-project-materials","title":"Final Project Materials","text":"<p>Final Project Materials will be made available here as well as Brightspace. </p> <ul> <li> <p>Final Project Overview PowerPoint</p> </li> <li> <p>Final Project Guidelines PDF</p> </li> </ul>"},{"location":"assignments/#green-trail-examples","title":"Green Trail Examples","text":"<p>For the green trail, the goal is to recreate a figure or figure panel from a publication. In the Green Trail Example #1, this student was able to recreate Figure 1 from Xian et al. 2022</p>"},{"location":"assignments/#blue-trail-examples","title":"Blue Trail Examples","text":"<p>For the blue trail, the goal is compare bioinformatic programs and contrast results. In the Blue Trail Example #1 &amp; #2, the students successfully compared a genomic (STAR) vs transcriptomic (SALMON) aligner or fast-aligner (HISAT2). In Blue Trail Example #2, the students used nf-core/rnaseq to compare multiple aligners at once. </p>"},{"location":"assignments/#black-trail-examples","title":"Black Trail Examples","text":"<p>For the black trail, the goal is to test an original hypothesis and to generate results not seen in the published manuscript. </p>"},{"location":"assignments/Helpful_Tips_HW7/","title":"Helpful Tips HW7","text":""},{"location":"assignments/Helpful_Tips_HW7/#helpful-tips-for-hw7","title":"Helpful Tips for HW#7","text":""},{"location":"assignments/Helpful_Tips_HW7/#step-1-is-trimming-required-or-not","title":"Step 1: Is Trimming Required or Not?","text":"<p>Determine whether your samples require trimming. If trimming is necessary, we covered this process here.. In this tutorial, you were asked to make a copy of the <code>trimmomatic_exercise/</code> folder. This folder contains a subfolder called <code>trimmomatic_adapters/</code>. You will need to specify the path to <code>trimmomatic_adapters/</code> along with the appropriate FASTA file containing the adapters to trim. See your options below: </p>"},{"location":"assignments/Helpful_Tips_HW7/#step-2-aligning-using-hisat2","title":"Step 2: Aligning using HISAT2","text":"<p>During Week 6, we covered the aligner HISAT2. There were two key exercises:</p> <ul> <li><code>HISAT2_example</code> (L11) \u2013 For single-end (SE) samples</li> <li><code>HISAT2_modify</code> (L13) \u2013 For paired-end (PE) samples</li> </ul> <p>Use the appropriate script based on your sample type:</p> <ul> <li>SE samples \u2192 Use the script in <code>HISAT2_example</code></li> <li>PE samples \u2192 Use the modified script in <code>HISAT2_modify</code></li> </ul> <p>Tip</p> <p>Need a refresher? Review the slides on Alignment Outputs (L15) to revisit the differences between these scripts. </p> <p>During alignment, you will need to specify the location to the Genomic Index. More information regarding the indexes and where to find them can be found here. </p> <p>Warning</p> <p>You will ONLY need to specify the LOCATION/PATH of the Genomic Index when running HISAT2. Please DO NOT make a copy of these indexes! They are extremely large and should not be duplicated.</p> <p>Strandedness: Don't Forget this Step!</p> <ul> <li> <p>Before aligning your full dataset, run the alignment on one sample first to determine the strandedness of your samples.</p> </li> <li> <p>If you do not specify <code>--rna-strandness</code> during alignment, HISAT2 assumes unstranded data, which can lead to incorrect alignments and downstream errors.</p> </li> </ul>"},{"location":"assignments/Helpful_Tips_HW7/#how-to-specify-strandedness-in-hisat2","title":"How to Specify Strandedness in HISAT2","text":"<pre><code>--rna-strandness &lt;string&gt;\n</code></pre> <p>For single-end reads, use <code>F</code> or <code>R</code>.</p> <ul> <li><code>F</code> means a read corresponds to a transcript.</li> <li><code>R</code> means a read corresponds to the reverse complement. </li> </ul> <p>For paired-end reads, use either <code>FR</code> or <code>RF</code>.</p> <p>Tip</p> <p>Need a refresher? Review the slides on Alignment Outputs (L15) to revisit the differences between these scripts. </p>"},{"location":"assignments/Helpful_Tips_HW7/#rseqc-and-multiqc-struggles","title":"RSeQC and MultiQC Struggles","text":"<p>MultiQC sometimes struggles with processing RSeQC outputs.</p> <pre><code>+ To determine strandedness, continue using the [Sequera.io MultiQC](https://seqera.io/multiqc/) website.\n</code></pre>"},{"location":"assignments/Helpful_Tips_HW7/#step-3-counting-reads","title":"Step 3: Counting Reads","text":"<ul> <li>Your <code>htseq_2025_demo</code> folder contains the script you created to run htseq-count on your samples.</li> <li>Each sample takes approximately 30 minutes to process.<ul> <li>To be safe, request <code>24:00:00</code> for time.</li> </ul> </li> <li>More information regarding the GTF file and its location can be found: </li> </ul>"},{"location":"assignments/RseQC_update-2025/","title":"RseQC update 2025","text":""},{"location":"assignments/RseQC_update-2025/#additional-resources-for-rseqc","title":"Additional Resources for RseQC","text":"<p><code>RseQC_apptainer-loop.sh</code> script. Instead of using conda, this script now uses apptainer to call in a container I created with the extension <code>.sif</code>. You will need to modify the lines <code>BAM_DIR</code> and <code>BAM_FILE</code>. </p> <pre><code>#!/bin/bash\n#SBATCH --partition=general\n#SBATCH --nodes=1\n#SBATCH --ntasks=8  # Single task running sequentially\n#SBATCH --mem=20G  # Adjust based on the number of BAM files\n#SBATCH --time=24:00:00\n#SBATCH --job-name=rseqc-loop\n#SBATCH --output=run-%x_%j.out  # %x=job name, %j=job ID\n\n# Load the Apptainer module\nmodule load apptainer/1.3.4\n\n# Path to the RSeQC container\nCONTAINER_PATH=\"/gpfs1/cl/mmg3320/course_materials/containers/rseqc.sif\"\n\n# Define input directory (update if needed)\nBAM_DIR=\"/users/p/d/pdrodrig/htseq_2025/bams\"\nBED_FILE=\"/users/p/d/pdrodrig/htseq_2025/refseq.hg19.bed12\"\nOUTPUT_DIR=\"rseqc_results\"\n\n# Create output directory if it doesn't exist\nmkdir -p \"$OUTPUT_DIR\"\n\n# Loop through all BAM files in the directory\nfor BAM_FILE in \"$BAM_DIR\"/*.bam; do\n    # Extract filename without extension\n    NAME=$(basename \"$BAM_FILE\" .bam)\n\n    echo \"Processing: $NAME\"\n\n    # Run infer_experiment.py inside the Apptainer container\n    apptainer exec \"$CONTAINER_PATH\" infer_experiment.py -r \"$BED_FILE\" -i \"$BAM_FILE\" &gt; \"$OUTPUT_DIR/${NAME}.infer_experiment.txt\"\n\n    # Run read_distribution.py inside the Apptainer container\n    apptainer exec \"$CONTAINER_PATH\" read_distribution.py -r \"$BED_FILE\" -i \"$BAM_FILE\" &gt; \"$OUTPUT_DIR/${NAME}.read_distribution.txt\"\ndone\n</code></pre>"},{"location":"assignments/RseQC_update-2025/#where-can-i-find-the-bed12-files","title":"Where can I find the bed12 files?","text":"<p>The bed12 files can be found in this location: </p> <pre><code>/gpfs1/cl/mmg3320/course_materials/RseQC_bed12\n</code></pre>"},{"location":"assignments/RseQC_update-2025/#i-get-this-error-when-running-multiqc-oops-the-rseqc-multiqc-module-broke","title":"I get this error when running multiqc \"Oops! The 'rseqc' MultiQC module broke...\"","text":"<p>Please use the sequera.io website to aid in your interpretations of the <code>infer_experiment.txt</code> and <code>read_distribution.txt</code> outputs. </p>"},{"location":"assignments/assignment_schedule/","title":"Assignment Schedule","text":""},{"location":"assignments/assignment_schedule/#assignment-schedule","title":"Assignment Schedule","text":"<p>Please note these dates may change but provides a good overview of what to expect </p> Assignment Tentative Due Date Homework Assignment #1 Wednesday, January 21st Homework Assignment #2 Wednesday, January 28th Homework Assignment #3 Friday, February 6th Homework Assignment #4 Wednesday, February 11th Homework Assignment #5 Friday, February 20th Homework Assignment #6 Friday, March 6th Homework Assignment #7 Monday, April 6th Primary Literature Review #1  (RNA-Seq paper) Friday, March 27th Primary Literature Review #2  (your paper) Friday, April 24th Final Project  Wednesday, April 27th"},{"location":"assignments/genome_index-2025/","title":"Genome index 2025","text":""},{"location":"assignments/genome_index-2025/#genomes-available-for-mmg3320-spring-2025","title":"Genomes Available for MMG3320 (Spring 2025)","text":"<p>For the alignment stage, you will need to specify the location of an indexed reference genome. Then during the counting stage you will need to specify the location of the GTF annotation file. The locations to these files can be found on the <code>mmg3320/course_materials</code> folder in the paths specified below. </p>"},{"location":"assignments/genome_index-2025/#information-about-the-indexed-reference-genomes-and-gtf","title":"Information about the Indexed Reference Genomes and GTF","text":"<p>The reference genome have been indexed and are available for the following organisms: human, mouse, thale cress, and rice. The FASTA files for these organisms were downloaded from either Gencode or Ensembl. </p> <p>Below is a depiction of the folder structure and location for the reference indexes. </p> <p>Warning</p> <p>YOU WILL NOT NEED TO MAKE A COPY OF ANY REFERENCE INDEXES IN YOUR HOME DIRECTORY</p> <p>Simply specify the location of the reference genome/GTF file of interest in your script!</p>"},{"location":"assignments/genome_index-2025/#file-paths","title":"File Paths","text":"<p>GTF &amp; FASTA files</p> <pre><code>\u251c\u2500\u2500 Arabidopsis_thaliana.TAIR10.60.gtf\n\u251c\u2500\u2500 Arabidopsis_thaliana.TAIR10.dna.toplevel.fa\n\u251c\u2500\u2500 Homo_sapiens.GRCh38.113.gtf\n\u251c\u2500\u2500 Homo_sapiens.GRCh38.dna.primary_assembly.fa\n\u251c\u2500\u2500 Mus_musculus.GRCm39.113.gtf\n\u251c\u2500\u2500 Mus_musculus.GRCm39.dna.primary_assembly.fa\n\u251c\u2500\u2500 Oryza_sativa.IRGSP-1.0.60.gtf\n\u2514\u2500\u2500 Oryza_sativa.IRGSP-1.0.dna.toplevel.fa\n</code></pre> <p>Location of GTF &amp; FASTA files </p><pre><code>/gpfs1/cl/mmg3320/course_materials/genome_index_mmg3320/genome_reference\n</code></pre> <p>Human Reference Index, GRCh38</p> <pre><code>\u251c\u2500\u2500 Homo_sapiens.GRCh38.1.ht2\n\u251c\u2500\u2500 Homo_sapiens.GRCh38.2.ht2\n\u251c\u2500\u2500 Homo_sapiens.GRCh38.3.ht2\n\u251c\u2500\u2500 Homo_sapiens.GRCh38.4.ht2\n\u251c\u2500\u2500 Homo_sapiens.GRCh38.5.ht2\n\u251c\u2500\u2500 Homo_sapiens.GRCh38.6.ht2\n\u251c\u2500\u2500 Homo_sapiens.GRCh38.7.ht2\n\u2514\u2500\u2500 Homo_sapiens.GRCh38.8.ht2\n</code></pre> <p>Location of Human Reference Index, GRCh38</p> <pre><code>/gpfs1/cl/mmg3320/course_materials/genome_index_mmg3320/HISAT2_indexes/Ensembl/Hsapiens_GRCh38\n</code></pre> <p>Mouse Reference Index, GRCm39</p> <pre><code>\u251c\u2500\u2500 Mus_musculus.GRCm39.1.ht2\n\u251c\u2500\u2500 Mus_musculus.GRCm39.2.ht2\n\u251c\u2500\u2500 Mus_musculus.GRCm39.3.ht2\n\u251c\u2500\u2500 Mus_musculus.GRCm39.4.ht2\n\u251c\u2500\u2500 Mus_musculus.GRCm39.5.ht2\n\u251c\u2500\u2500 Mus_musculus.GRCm39.6.ht2\n\u251c\u2500\u2500 Mus_musculus.GRCm39.7.ht2\n\u2514\u2500\u2500 Mus_musculus.GRCm39.8.ht2\n</code></pre> <p>Location of Mouse Reference Index, GRCm39</p> <pre><code>/gpfs1/cl/mmg3320/course_materials/genome_index_mmg3320/HISAT2_indexes/Ensembl/Mmus_GRCm39\n</code></pre> <p>Rice Reference Index, OSativa</p> <pre><code>\u251c\u2500\u2500 Osativa_v60.1.ht2\n\u251c\u2500\u2500 Osativa_v60.2.ht2\n\u251c\u2500\u2500 Osativa_v60.3.ht2\n\u251c\u2500\u2500 Osativa_v60.4.ht2\n\u251c\u2500\u2500 Osativa_v60.5.ht2\n\u251c\u2500\u2500 Osativa_v60.6.ht2\n\u251c\u2500\u2500 Osativa_v60.7.ht2\n\u2514\u2500\u2500 Osativa_v60.8.ht2\n</code></pre> <p>Location of Rice Reference Index, OSativa</p> <pre><code>/gpfs1/cl/mmg3320/course_materials/genome_index_mmg3320/HISAT2_indexes/Ensembl/Osativa_IRGSP\n</code></pre> <p>Thale Cress Reference Index, TAIR10</p> <pre><code>\u251c\u2500\u2500 TAIR10-v60.1.ht2\n\u251c\u2500\u2500 TAIR10-v60.2.ht2\n\u251c\u2500\u2500 TAIR10-v60.3.ht2\n\u251c\u2500\u2500 TAIR10-v60.4.ht2\n\u251c\u2500\u2500 TAIR10-v60.5.ht2\n\u251c\u2500\u2500 TAIR10-v60.6.ht2\n\u251c\u2500\u2500 TAIR10-v60.7.ht2\n\u2514\u2500\u2500 TAIR10-v60.8.ht2\n</code></pre> <p>Location of Thale Cress Reference Index, TAIR10</p> <pre><code>/gpfs1/cl/mmg3320/course_materials/genome_index_mmg3320/HISAT2_indexes/Ensembl/TAIR10_v60\n</code></pre>"},{"location":"assignments/genome_index-2025/#information-about-gtf-fasta-files-downloaded-from-ensembl","title":"Information about GTF &amp; FASTA files downloaded from Ensembl","text":"<p>Organisms included: Rice, Thale Cress, Human and Mouse</p> <p>Website: </p> <p>https://ftp.ensemblgenomes.ebi.ac.uk/pub/plants/current/gtf/ https://ftp.ensemblgenomes.ebi.ac.uk/pub/plants/current/fasta/</p> <p>Files:</p> <pre><code>+ Arabidopsis_thaliana.TAIR10.dna.toplevel.fa.gz\n+ Arabidopsis_thaliana.TAIR10.60.gtf.gz \n+ Oryza_sativa.IRGSP-1.0.dna.toplevel.fa.gz\n+ Oryza_sativa.IRGSP-1.0.60.gtf.gz\n</code></pre> <p>File naming for Ensembl: </p> <p>Files are consistently named following this pattern:</p> <pre><code>&lt;species&gt;.&lt;assembly&gt;.&lt;version&gt;.gtf.gz\n</code></pre> <pre><code>&lt;species&gt;:       The systematic name of the species.\n&lt;assembly&gt;:      The assembly build name.\n&lt;version&gt;:       The version of Ensembl from which the data was exported.\n</code></pre> <p>gtf : All files in these directories are in GTF format gz : All files are compacted with GNU Zip for storage efficiency.</p> <p>Toplevel:These files contains all sequence regions flagged as toplevel in an Ensembl schema. This includes chromsomes, regions not assembled into chromosomes and N padded haplotype/patch regions.</p>"},{"location":"assignments/genome_index-2025/#more-information-about-the-gtf-file","title":"More information about the GTF file","text":"<p>The GTF (General Transfer Format) is an extension of GFF version 2 and used to represent transcription models. GFF (General Feature Format) consists of one line per feature, each containing 9 columns of data. </p>"},{"location":"assignments/genome_index-2025/#fields","title":"Fields","text":"<p>Fields are tab-separated. Also, all but the final field in each feature line must contain a value; \"empty\" columns are denoted with a '.'</p> <pre><code>seqname   - name of the chromosome or scaffold; chromosome names \n            without a 'chr' \nsource    - name of the program that generated this feature, or \n            the data source (database or project name)\nfeature   - feature type name. Current allowed features are\n            {gene, transcript, exon, CDS, Selenocysteine, start_codon,\n            stop_codon and UTR}\nstart     - start position of the feature, with sequence numbering \n            starting at 1.\nend       - end position of the feature, with sequence numbering \n            starting at 1.\nscore     - a floating point value indiciating the score of a feature\nstrand    - defined as + (forward) or - (reverse).\nframe     - one of '0', '1' or '2'. Frame indicates the number of base pairs\n            before you encounter a full codon. '0' indicates the feature \n            begins with a whole codon. '1' indicates there is an extra\n            base (the 3rd base of the prior codon) at the start of this feature.\n            '2' indicates there are two extra bases (2nd and 3rd base of the \n            prior exon) before the first codon. All values are given with\n            relation to the 5' end.\nattribute - a semicolon-separated list of tag-value pairs (separated by a space), \n            providing additional information about each feature. A key can be\n            repeated multiple times.\n</code></pre>"},{"location":"assignments/genome_index-2025/#attributes","title":"Attributes","text":"<p>The following attributes are available. All attributes are semi-colon separated pairs of keys and values.</p> <ul> <li>gene_id: The stable identifier for the gene</li> <li>gene_version: The stable identifier version for the gene</li> <li>gene_name: The official symbol of this gene</li> <li>gene_source: The annotation source for this gene</li> <li>gene_biotype: The biotype of this gene</li> <li>transcript_id: The stable identifier for this transcript</li> <li>transcript_version: The stable identifier version for this transcript</li> <li>transcript_name: The symbold for this transcript derived from the gene name</li> <li>transcript_source: The annotation source for this transcript</li> <li>transcript_biotype: The biotype for this transcript</li> <li>exon_id: The stable identifier for this exon</li> <li>exon_version: The stable identifier version for this exon</li> <li>exon_number: Position of this exon in the transcript</li> <li>ccds_id: CCDS identifier linked to this transcript</li> <li>protein_id: Stable identifier for this transcript's protein</li> <li>protein_version: Stable identifier version for this transcript's protein</li> <li>tag: A collection of additional key value tags</li> <li>transcript_support_level: Ranking to assess how well a transcript is supported (from 1 to 5)</li> </ul>"},{"location":"assignments/genome_index-2025/#information-about-gtf-fasta-files-downloaded-from-gencode","title":"Information about GTF &amp; FASTA files downloaded from Gencode","text":"<p>Organisms included: Human and Mouse</p> <p>Website: </p> <p>https://www.gencodegenes.org/mouse/release_M36.html </p> <p>Version: </p> <ul> <li>GRCm39/mm39 - version M36</li> <li>GRCh38/hg38 - version 47 </li> </ul> <p>Description of files:  </p> <p>FASTA information Genome sequence, primary assembly (GRCm39) </p> <pre><code>+ Nucleotide sequence of the GRCm39 primary genome assembly (chromosomes and scaffolds)\n+ The sequence region names are the same as in the GTF/GFF3 files\n</code></pre> <p>GTF information </p> <pre><code>+ It contains the comprehensive gene annotation on the primary assembly (chromosomes and scaffolds) sequence regions\n</code></pre>"},{"location":"assignments/index_1/","title":"Index 1","text":""},{"location":"assignments/index_1/#final-project-materials","title":"Final Project Materials","text":"<p>Final Project Materials will be made available here as well as Brightspace. </p> <ul> <li> <p>Final Project Overview PowerPoint</p> </li> <li> <p>Final Project Guidelines PDF</p> </li> </ul>"},{"location":"assignments/index_1/#past-final-project-submissions","title":"Past Final Project Submissions","text":"<p>Below are examples of final project submissions from previous years. Please note that the guidelines and rubric have changed for Spring 2026\u2014these examples are provided only for general reference. They are intended to illustrate the green, blue, and black trail projects, not to serve as templates for formatting or content.</p>"},{"location":"assignments/index_1/#green-trail-examples","title":"Green Trail Examples","text":"<p>For the green trail, the goal is to recreate a figure or figure panel from a publication. In the Green Trail Example #1, this student was able to recreate Figure 1 from Xian et al. 2022</p> <p>[Green Trail Example #1]</p> <p>[Green Trail Example #2]</p> <p>[Green Trail Example #3]</p>"},{"location":"assignments/index_1/#blue-trail-examples","title":"Blue Trail Examples","text":"<p>For the blue trail, the goal is compare bioinformatic programs and contrast results. In the Blue Trail Example #1 &amp; #2, the students successfully compared a genomic (STAR) vs transcriptomic (SALMON) aligner or fast-aligner (HISAT2). In Blue Trail Example #2, the students used nf-core/rnaseq to compare multiple aligners at once. </p> <p>[Blue Trail Example #1]</p> <p>[Blue Trail Example #2]</p> <p>[Blue Trail Example #3]</p>"},{"location":"assignments/index_1/#black-trail-examples","title":"Black Trail Examples","text":"<p>For the black trail, the goal is to test an original hypothesis and to generate results not seen in the published manuscript. The powerpoint presentation given by the student for Black Trail Example #1 are provided here. Please note that all of these analysis contained multiple parts since the students did multiple pairwise comparisons. </p> <p>[Black Trail Example #1]</p> <p>[Black Trail Example #2]</p> <p>[Black Trail Example #3]</p>"},{"location":"assignments/week1_assignment/","title":"Homework 1 (L1)","text":""},{"location":"assignments/week1_assignment/#homework-assignment-1-30-points","title":"Homework Assignment #1 (30 points)","text":"<p>For this assignment you will have until 10AM on Wednesday, January 21st to submit on Brightspace. Late assignments will NOT be accepted. </p>"},{"location":"assignments/week1_assignment/#directions-for-students","title":"Directions for Students:","text":"<p>Open a new Microsoft Word Document, the first four lines of your document should contain the following:  </p> <ul> <li>Your name</li> <li>MMG3320</li> <li>Today's date</li> <li>Homework Assignment # 1</li> </ul>"},{"location":"assignments/week1_assignment/#task-1","title":"Task 1:","text":"<ul> <li>In the terminal, open the manual page for the <code>rm</code> command and skim through the information. What effect does adding the <code>-i</code> option to <code>rm</code> have?  </li> </ul>"},{"location":"assignments/week1_assignment/#task-2","title":"Task 2:","text":"<ul> <li>Use the <code>-lh</code> option for the <code>ls</code> command to display more information for each item within the <code>unit1_unix/genomics_data</code> folder.   Submit a screenshot of your terminal screen and underneath describe what additional information is provided that you didn't see with the bare <code>ls</code> command. </li> </ul>"},{"location":"assignments/week1_assignment/#task-3","title":"Task 3:","text":"<p>Submit one final screenshot of your terminal screen. Be sure to clear your terminal screen using control + L before proceeding </p> <p>1) Create a new folder in <code>unit1_unix</code> called <code>selected_fastq</code> 2) Make a copy of <code>Irrel_kd_2.subset.fq</code> and <code>Mov10_oe_2.subset.fq</code> from <code>raw_fastq</code> into the <code>selected_fastq</code> folder 3) Rename the <code>selected_fastq</code> folder and call it <code>exercise1</code> </p>"},{"location":"assignments/week2_assignment/","title":"Homework 2 (L3)","text":""},{"location":"assignments/week2_assignment/#homework-assignment-2-50-points","title":"Homework Assignment #2 (50 points)","text":"<p>For this assignment you will have until 10AM on Wednesday, January 28th to submit on Brightspace. Late assignments will NOT be accepted.</p>"},{"location":"assignments/week2_assignment/#directions-for-students","title":"Directions for Students:","text":"<p>Open a new Microsoft Word Document and submit answers to the questions below. The first four lines of your document should contain the following:  </p> <ul> <li>Your name</li> <li>MMG3320</li> <li>Today's date</li> <li>Homework Assignment #2</li> </ul>"},{"location":"assignments/week2_assignment/#part-a-practice-using-less","title":"Part A: Practice using <code>Less</code>","text":"<ol> <li> <p>This is a multi-part question:   </p> <p>a. Navigate into the <code>genomics_data</code> folder.</p> <p>b. Use the <code>less</code> command to open up the file <code>Encode-hesc-Nanog.bed</code>.  </p> <p>c. Use the shortcut to get to the end of the file.  </p> <p>d. Search for the string <code>chr11</code>.     </p> <p>e. Report two rows that start with <code>chr11</code>. Include the start and end position in your answer. </p> <p>Exit the <code>less</code> buffer.  </p> </li> <li> <p>Print to screen the last 5 lines of the file <code>Encode-hesc-Nanog.bed</code>. Submit a screenshot of the output as your answer.</p> </li> <li> <p>How many commands have you typed after going through this exercise? Submit a screenshot of the output as your answer.</p> </li> </ol>"},{"location":"assignments/week2_assignment/#part-b-generating-your-own-script","title":"Part B: Generating your own script","text":"<p>You got the following line of codes from a trusted source but need to modify it so you can submit it to the VACC-Bluemoon server. You decide its time to make your own script. Follow the steps below: </p> <ol> <li> <p>Create a new file in the <code>other</code> directory called <code>script.sh</code>. </p> <ul> <li> <p>The .sh file extension typically indicates that a file is a shell script. </p> </li> <li> <p>In Unix-like operating systems (such as Linux and macOS), shell scripts are plain text files containing a sequence of commands that can be executed by a shell.</p> </li> </ul> </li> <li> <p>Paste in the code below to <code>script.sh</code>. </p> <pre><code>STAR --runThreadN 4 \\\n--runMode genomeGenerate \\\n--genomeDir /username/chr1_hg19_STAR_index/ \\\n--genomeFastaFiles /username/reference_data_ensembl/Homo_sapiens.GRCh19.dna.chromosome.1.fa \\\n--sjdbGTFfile /username/reference_data_ensembl/Homo_sapiens.GRCh19.gtf \n</code></pre> </li> <li> <p>Replace every occurrence of \"username\" with your netid. </p> </li> <li> <p>Delete the line containing <code>--runMode</code></p> </li> <li> <p>Change the <code>--runThreadN</code> from 4 to 6  </p> </li> <li> <p>You would also like to use the newest genome assembly, human reference 38 (hg38/GRCh38). Change this as well in your script. </p> </li> <li> <p>Submit a screenshot of your script in the Nano buffer as homework Part B. </p> </li> </ol> <p>Save the file and EXIT. </p> <p>Please Take Note: </p> <ul> <li> <p>The argument <code>--genomeDir</code> is pointing to an entire directory while <code>--genomeFastaFiles</code> is pointing to a specific file. This is really important as the program is looking for specific files or entire directories (with files in them!) to run successfully. </p> </li> <li> <p>Each line here ends with a <code>\\</code>. The <code>\\</code> can also be used as an escape character that signals that the character following it has a special meaning in this case its a continuation. </p> </li> </ul>"},{"location":"assignments/week3_assignment/","title":"Homework 3 (L5)","text":""},{"location":"assignments/week3_assignment/#homework-assignment-3-50-points","title":"Homework Assignment #3 (50 points)","text":"<p>For this assignment you will have until 5PM on Friday, February 6th to submit on Brightspace. Late assignments will NOT be accepted.</p>"},{"location":"assignments/week3_assignment/#directions-for-students","title":"Directions for Students:","text":"<p>Open a new Microsoft Word Document and submit answers to the questions below. The first four lines of your document should contain the following:  </p> <ul> <li>Your name</li> <li>MMG3320</li> <li>Today's date</li> <li>Homework Assignment #3</li> </ul>"},{"location":"assignments/week3_assignment/#part-a-find-the-number-of-unique-exons-in-chromosome-1-of-humans-25-points","title":"Part A: Find the number of unique exons in Chromosome 1 of Humans (25 points)","text":"<p>We will be using <code>chr1-hg19_genes.gtf</code>?</p> <p>To determine the number of unique exons in chromosome 1 you will write and perform a series of steps that will: </p> <ol> <li>Extract only exon features from the GTF file</li> <li>Subset dataset to keep only genomic coordinates</li> <li>Remove duplicate exons</li> <li>Redirect and check </li> </ol> <p>Your end goal is to have a single line of code, wherein you have strung together multiple commands using the pipe operator. These steps are detailed a little more below:</p>"},{"location":"assignments/week3_assignment/#1-extract-only-exon-features-from-gtf-file","title":"1. Extract only exon features from GTF file","text":"<p>We only want the exons (not CDS or start_codon features), so let's use the command to search for the word \"exon\". You can perform a \"sanity check\" first by piping the result to the <code>head</code> command for the first 10 lines. </p>"},{"location":"assignments/week3_assignment/#2-subset-the-extracted-information-from-step-1-to-only-keep-genomic-coordinates","title":"2. Subset the extracted information from step 1 to only keep genomic coordinates","text":"<p>We will define the uniqueness of an exon by its genomic coordinates (start and end position) and strand (positive or negative). Therefore, we would like to keep 4 columns (chr, start, stop, and strand). </p>"},{"location":"assignments/week3_assignment/#3-remove-duplicate-exons","title":"3. Remove duplicate exons","text":"<p>Now, we need to remove exons that show up multiple times for different transcripts.</p>"},{"location":"assignments/week3_assignment/#4-redirect-and-check","title":"4. Redirect and check","text":"<p>Finally, redirect the output to a new file and call it <code>chr1-hg19_exons.txt</code>. Do a final check on the number of lines and file size for this newly created file. </p> <p>Report the entire command you have at this stage, the number of lines output, and the file size in your homework as an answer for Part 1. </p>"},{"location":"assignments/week3_assignment/#part-b-jupiter-notebook-class-exercise-6-25-points-intro-to-shell-scripting-l5","title":"Part B: Jupiter Notebook Class Exercise #6 (25 points) - Intro to Shell Scripting (L5)","text":"<p>Directions for Part 2: Submit two screenshots. </p> <ul> <li> <p>One of your final script (output from #11). </p> </li> <li> <p>The second screenshot will be of your terminal screen after running your script (output from #12).  </p> </li> </ul>"},{"location":"assignments/week4_assignment/","title":"Homework 4 (L5/L6)","text":""},{"location":"assignments/week4_assignment/#homework-assignment-4-50-points","title":"Homework Assignment #4 (50 points)","text":"<p>For this assignment you will have until 1PM on Wednesday, February 11th to submit on Brightspace. Late assignments will NOT be accepted.</p>"},{"location":"assignments/week4_assignment/#directions-for-students","title":"Directions for Students:","text":"<p>Open a new Microsoft Word Document and submit answers to the questions below. The first four lines of your document should contain the following:  </p> <ul> <li>Your name</li> <li>MMG3320</li> <li>Today's date</li> <li>Homework Assignment #4</li> </ul>"},{"location":"assignments/week4_assignment/#instructions-please-review-lesson-l6-loops-after-reviewing-follow-the-prompts-below-and-then-submit-two-screenshots-one-screenshot-will-be-of-your-final-script-output-from-14-the-second-screenshot-will-be-of-your-terminal-screen-after-opening-badreadscountsummary","title":"Instructions: Please review lesson L6 - Loops. After reviewing, follow the prompts below and then submit two screenshots. One screenshot will be of your final script (output from #14). The second screenshot will be of your terminal screen after opening <code>badreads.count.summary</code>.","text":"<p>Homework Assignment #4</p> <ol> <li> <p>Create a directory called <code>badreads</code> in <code>unit1_unix</code></p> </li> <li> <p>Use Jupyter Notebooks/Nano to create a new script called <code>generate_bad_reads_summary.sh</code> in <code>badreads/</code></p> </li> <li> <p>At the beginning of your script add a shebang line. </p> <pre><code>#!/bin/bash\n</code></pre> <p>This line is the absolute path to the Bash interpreter. The shebang line ensures that the bash shell interprets the script even if it is executed using a different shell.</p> </li> <li> <p>After the shebang line, skip a line and copy-and-paste the following in Line 3: </p> <pre><code># enter directory with raw FASTQs\n</code></pre> </li> <li> <p>In line 4 write a command to change directories into the <code>raw_fastq</code> directory. </p> </li> <li> <p>Add the following comment as Line 6. </p> <pre><code># loop over each FASTQ file\n</code></pre> </li> <li> <p>Now you are ready to begin writing the for loop. To create the first line of your for loop use the following: </p> Loop component Value variable_name <code>filename</code> list all FASTQ files </li> <li> <p>Type <code>do</code> in Line 8. </p> </li> <li> <p>Skip a line. On line 10 copy-and-paste the following comment. Yes it is okay to skip a line! </p> <pre><code># create a prefix for all output files\n</code></pre> </li> <li> <p>Now you are ready to move on to create a prefix for all (6) fastq files. These prefixes will be stored in a second variable called <code>samplename</code> in line 11. To write this line of code successfully, remember the following:</p> <pre><code>variable_name=value_of_variable\n</code></pre> <ul> <li>The value_of_variable should be equal to the <code>basename</code> of <code>filename</code>. </li> <li>Be sure to trim off the file extension <code>.subset.fq</code></li> </ul> <p>Why are we doing this? Storing the prefixes in <code>samplename</code> will allow us to uniquely label our output files later on! </p> </li> <li> <p>Copy-and-paste the following into lines 12 and 13. The <code>echo</code> statement will keep the user informed on which file is being processed in real-time. </p> <pre><code># tell us what file we're working on\necho $filename\n</code></pre> </li> <li> <p>Complete the command below to extract and save all \"bad reads\" into an output file. A read is considered \"bad\" if it contains 10 consecutive N's. Below, you are given the right side of the command, which specifies the output file location. Your task is to complete the left side of the command using <code>grep</code>. These will be lines 15 and 16 of your script.</p> <p></p><pre><code># Extract all bad read records and save them to a new file  \nWRITE-THE-COMMAND-HERE &gt; ~/unit1_unix/badreads/${samplename}_badreads.fq  \n</code></pre> + Ensure that all four lines of each matching sequence read are included in the output.  </li> <li> <p>You are almost finished! Copy and paste the lines below in lines 18-20. </p> <pre><code># grab the number of bad reads and write it to a summary file\ngrep -cH NNNNNNNNNN $filename &gt;&gt; ~/unit1_unix/badreads/badreads.count.summary\ndone\n</code></pre> </li> <li> <p>Save and exit, and voila! You now have a script you can use to assess the quality of all your new datasets. </p> </li> </ol> <p>To run this script simply enter the following command:</p> <pre><code>sh generate_bad_reads_summary.sh\n</code></pre>"},{"location":"assignments/week4_assignment/#why-do-you-need-a-shebang-line","title":"Why do you need a shebang line?","text":"<p>Having a shebang line is best practice. While your script will run fine without it in environments where bash is the default shell, it won't if the user of this script is using a different shell. To avoid any issues, we explicitly state that this script needs to executed using the bash shell.</p>"},{"location":"assignments/week4_assignment/#explanation-of-command-above","title":"Explanation of command above","text":"<p>You are using <code>grep</code> to find all the bad reads (in this case, bad reads are defined as those with 10 consecutive N's), and then extracting the four lines associated with each sequence read and writing them to a file. The output file is named using the <code>samplename</code> variable you created earlier in the loop. You will also notice we are adding a path to redirect the output into the <code>badreads</code> directory.</p>"},{"location":"assignments/week4_assignment/#why-are-we-using-curly-brackets-with-the-variable-name","title":"Why are we using curly brackets with the variable name?","text":"<p>When we append a variable to some other free text, we need shell to know where our variable name ends. By encapsulating the variable name in curly brackets we are letting shell know that everything inside it is the variable name. This way when we reference it, shell knows to print the variable <code>$base</code> and not to look for a variable called <code>$base_badreads.fq</code>.</p>"},{"location":"assignments/week4_assignment/#explanation-of-command-above_1","title":"Explanation of command above","text":"<p>Above, you are counting the number of identified bad reads using the count flag of <code>grep</code>, <code>-c</code>, which will return the number of matches rather than the actual matching lines. Here, you are using the <code>-H</code> flag; this will report the filename along with the count value. This is useful because you are writing this information to a running summary file. So rather than just reporting a count value you will also know which file it is associated with. You then closed the loop with <code>done</code>. </p>"},{"location":"assignments/week4_assignment/#how-do-we-know-if-your-script-worked","title":"How do we know if your script worked?","text":"<p>Take a look inside the <code>badreads</code> directory. You should see that for every one of the original FASTQ files, one bad read file was created. You should also have a summary file documenting the total number of bad reads from each file.</p> <pre><code>badreads.count.summary  Irrel_kd_2_badreads.fq  Mov10_oe_1_badreads.fq  Mov10_oe_3_badreads.fq\nIrrel_kd_1_badreads.fq  Irrel_kd_3_badreads.fq  Mov10_oe_2_badreads.fq\n</code></pre>"},{"location":"assignments/week5_assignment/","title":"Homework 5 (L7/L8)","text":""},{"location":"assignments/week5_assignment/#homework-assignment-5-100-points","title":"Homework Assignment #5 (100 points)","text":"<p>The due date for this homework is Friday, February 20th by 5pm. Please upload Part A and B as a .pdf or .docx onto Brightspace (50 points). Then please upload Part C separately as a .xlsx file onto Brightspace (50 points). Late homework will be docked 10% of the overall grade for every day that the assignment is late. An assignment is considered late if it is not submitted by the date and time specified. Three days past the due date (weekend included), the assignment will no longer be accepted, and the student(s) will receive a ZERO.</p> <p>Homework #5 Guidelines</p> <p>Metadata file</p>"},{"location":"assignments/past/week6_assignment/","title":"Week6 assignment","text":""},{"location":"assignments/past/week6_assignment/#homework-assignment-6-120-points","title":"Homework Assignment #6 (120 points)","text":"<p>The due date for this homework is Friday, March 7th by 5pm.</p> <p>PART A: The final MULTIQC output (.html) should be emailed to me. One file per group. Brightspace will not accept <code>.html</code> files.  PART B: The Microsoft Word (.docx) file containing your interpretation should be submitted via Brightspace. </p> <p>Late homework will be docked 10% of the overall grade for every day that the assignment is late. An assignment is considered late if it is not submitted by the date and time specified. Three days past the due date (weekend included), the assignment will no longer be accepted, and the student(s) will receive a ZERO.</p> <p>Homework #6 Guidelines</p>"},{"location":"assignments/past/week9_assignment/","title":"Week9 assignment","text":""},{"location":"assignments/past/week9_assignment/#homework-assignment-7-200-points","title":"Homework Assignment #7 (200 points)","text":"<p>The due date for this homework is Monday, April 7th by 11:59pm.</p> <ul> <li>PART A (50 points): BASH script (.sh) used to perform the alignment and create BAM file</li> <li>PART B (50 points): BASH script (.sh) used to count reads or other downstream processing files.</li> <li>PART C (50 points): Final MULTIQC (.html) output displaying plots from the alignment, counting, and RSeQC stage. </li> <li>PART D (50 points):  Fill out the table below for your samples. Submit the filled-out table in a separate Microsoft Word document (.docx).</li> </ul> <p>Most of these file types will not be accepted by Brightspace, therefore, please send me ONE email with PARTS A-D included. Late homework will be docked 10% of the overall grade for every day that the assignment is late. An assignment is considered late if it is not submitted by the date and time specified. Three days past the due date (weekend included), the assignment will no longer be accepted, and the student(s) will receive a ZERO.</p> <p>Homework #7 Guidelines</p> <p>Please try your best to get through this homework assignment on your own using the following materials:</p> <p>Helpful Tips for HW#7</p> <p>Information about Genome Indexes</p> <p>Additional Resources for RseQC</p> <p>We will have a course-catch up day dedicated to answering your questions regarding HW#7 on Monday, March 31st. I decided that April 2nd would be too late.</p>"},{"location":"ch01/","title":"Overview","text":""},{"location":"ch01/#unit-1","title":"Unit 1","text":""},{"location":"ch01/#introduction","title":"Introduction","text":"<p>In this unit, we will focus on building foundational skills for using the command-line interface (CLI), specifically the <code>bash</code> shell. The <code>bash</code> shell is a versatile scripting environment widely used on Unix-based systems for file management, task automation, and software interaction. By the end of this unit, you will be able to execute basic commands, write simple scripts, and grasp the core principles of shell scripting, equipping you to perform a variety of computational tasks efficiently.</p>"},{"location":"ch01/#learning-objectives","title":"Learning Objectives","text":"<p>At the end of this unit students will be able to: </p> <ol> <li>Navigate the command line interface (CLI) using the <code>bash</code> shell. </li> <li>Create, organize, and manipulate data files and directories.  </li> <li>Gain familiarity with using the VACC, an high-performance computing (HPC) cluster. </li> </ol>"},{"location":"ch01/#how-to-use-these-materials","title":"How to use these materials","text":"<p>This guide is intended for the reader to follow along step by step in chronological order. As you read, make sure to practice your UNIX skills by completing the exercises and examples in each days' lesson. While the code is already written out in the text, typing out the code manually (rather than copy and pasting) will help speed up the learning process \u2013 practice makes progress!</p> <p>Important note: </p> <p>Examples of code will be arranged as a code chunk. This is distinguished by a slightly greyed out background behind the code text. </p> <pre><code>ssh username@vacc-user1.uvm.edu \n</code></pre> <p>The output of executed code will occasionally be displayed as well in grey code chunks. An output is the consequence that occurs after code is executed. Outputs in this tutorials will begin with ##. This line of code is not meant to be typed only to compare. </p> <pre><code>## genomics_data  other  raw_fastq  README.txt  reference_data \n</code></pre>"},{"location":"ch01/#advice-for-beginners","title":"Advice for beginners","text":"<p>You should:</p> <ul> <li> <p>Be patient. Learning a new skill can be frustrating. </p> </li> <li> <p>Know that the hardest part is the beginning. Learning how to code is hard, but it\u2019s not impossible. All it takes is practice and I can assure you that it will get easier. </p> </li> <li> <p>We will spend a lot of time learning to manage your data in addition to graphing it. Most people want to get right into graphing their data, but if the data is not already fully prepared, you will actually spend most of your time organizing/structuring your data. Graphing, by comparison, is far easier to learn.</p> </li> <li> <p>Ask for help. Do not suffer in silence.</p> </li> <li> <p>Some of you may need to physically write code down to remember it. Do it! I've also had other students print these pages and highlight during class. Do it! You may need to practice outside of class or read the tutorials before class to fully understand. Do it! You know how you learn best. Like any course, I can not go over the same concept over and over again. To learn something for a exam, you need to review it. You need to study. So although I do not require you to do these things, you might have to do this to become competent and independent in coding. </p> </li> </ul>"},{"location":"ch01/01_week1_mmg3320/","title":"Introducing the Shell (L1)","text":""},{"location":"ch01/01_week1_mmg3320/#learning-objectives-for-todays-lesson","title":"Learning Objectives for Today's Lesson","text":"<ul> <li>Log in to the VACC, a high-performance computing (HPC) cluster. </li> <li>Learn basic command-line navigation.   </li> <li>Copy data into your home directory. </li> <li>List files within a directory. </li> </ul>"},{"location":"ch01/01_week1_mmg3320/#introduction-to-command-line","title":"Introduction to Command Line","text":"<p>The command-line interface (CLI) and graphical user interface (GUI) are two different ways of interacting with a computer's operating system. Most people are familiar with the GUI, as it is the default interface for most software. In a GUI, you interact with visual representations of files, folders, applications, and other elements. In contrast, the CLI involves working primarily with text-based representations of files, folders, input, and output.</p> <p></p>"},{"location":"ch01/01_week1_mmg3320/#what-is-a-shell","title":"What is a Shell?","text":"<p>Most data processing and remote access will be command-line based. For this we need an interpreter. </p> <p>A shell is a command-line interpreter that allows users to type commands to launch programs. </p> <p>The most popular UNIX shell is BASH (the Bourne Again SHell) \u2014 so named because it is derived from a shell written by Stephen Bourne. Learning to use the shell requires time and effort. </p> A shell is a command-line interpreter.  <p>While a GUI presents you with choices to select, CLI does not automatically display these options to you. Instead, you will need to learn specific commands. This will resemble learning a new language!</p>"},{"location":"ch01/01_week1_mmg3320/#benefits-of-using-the-shell","title":"Benefits of Using the Shell","text":"<p>Using the shell provides access to internal system controls, remote servers, and customizable workflows through scripting. With the shell, you can create, edit, and delete files, as well as perform many other tasks efficiently.</p> <p>The command line is often the easiest and most effective way to interact with remote machines and supercomputers. Familiarity with the shell is essential for running specialized tools and accessing high-performance computing (HPC) resources. As clusters and cloud computing systems become increasingly important for scientific data analysis, proficiency in the shell has become a critical skill. The command-line skills introduced here will enable you to address a wide range of scientific questions and computational challenges.</p>"},{"location":"ch01/01_week1_mmg3320/#how-to-access-the-shell","title":"How to access the shell","text":"<p>On Mac or Linux machines, you can access the shell locally through a program called Terminal. However, for simplicity and convenience, we will use the shell through the open-source web portal Vermont Advanced Computing Center - Open OnDemand (VACC-OOD). Once you open the terminal, you will begin learning the basics of shell programming with the Bourne Again Shell (Bash).</p>"},{"location":"ch01/01_week1_mmg3320/#working-with-remote-machines-vermont-advance-computing-center-cluster-vacc","title":"Working with Remote Machines: Vermont Advance Computing Center Cluster (VACC)","text":""},{"location":"ch01/01_week1_mmg3320/#why-work-on-the-vacc","title":"Why work on the VACC?","text":"<p>Most data-processing tasks in bioinformatics require more computing power than we have on our workstations. For all bioinformatics projects performed in this course, you will work over a network connection with the VACC. </p> <p>Cluster Basics</p> The image above illustrates the multiple computers that make up a cluster. Each individual computer in the cluster, referred to as a \"node\", is significantly more powerful than a typical laptop or desktop computer. A \"cluster\" is a large system composed of hundreds to thousands of nodes, each serving a specific purpose. <p>Nodes are generally classified by their roles: login nodes and compute nodes. Login nodes are used for accessing the cluster, setting up jobs, and managing workflows, while compute nodes handle the actual computational analysis or work. Most clusters have a few login nodes and many compute nodes to efficiently handle diverse workloads.</p> <p>Common characteristics of a Cluster:</p> <ul> <li>Large memory </li> <li>Storage shared across nodes </li> <li>High speed interconnection network; suitable for high-throughput applications </li> <li>Shared by many users</li> </ul> <p>As of March 2022, the VACC provides three Clusters:</p> <ul> <li>BlackDiamond </li> <li>Bluemoon </li> <li>DeepGreen</li> </ul> We will primarily use the **Bluemoon** cluster for any downstream analysis. <p>Please note that more information can always be found at the Vermont Advanced Computing Center website.</p>"},{"location":"ch01/01_week1_mmg3320/#connecting-to-the-vacc","title":"Connecting to the VACC","text":"To connect to the VACC cluster you can either use SSH or VACC-OOD."},{"location":"ch01/01_week1_mmg3320/#vacc-ood-overview","title":"VACC-OOD Overview","text":"<p>Each student has been provided with their own personal VACC account that they can use to access VACC-Open OnDemand (OOD).</p>"},{"location":"ch01/01_week1_mmg3320/#what-is-open-ondemand-ood","title":"What is Open OnDemand (OOD)?","text":"<p>Open OnDemand (OOD) is an open source web portal for high performance computing (HPC) that provides users with an easy-to-use web interface to HPC clusters.</p> <p>Benefits of using OOD:</p> <ol> <li> <p>Web-based, no additional software needs to be installed on your local machine</p> </li> <li> <p>The easiest way to run graphical user interface (GUI) applications remotely on a cluster</p> </li> <li> <p>Typical computing with command-line requires a high learning curve whereas OOD is easy to use and simple to learn</p> </li> </ol> <p>GUI applications offered by VACC-OOD:</p> <ul> <li>Equipped with Terminal: this is used to perform tasks on the command line (shell), both locally and on remote machines.</li> </ul> <p></p> <ul> <li>RStudio: an integrated development environment for R</li> </ul>"},{"location":"ch01/01_week1_mmg3320/#how-to-log-in-to-vacc-ood","title":"How to log-in to VACC-OOD:","text":"<ol> <li> <p>Use the VACC-OOD link to access the site </p> </li> <li> <p>Add your UVM netid and password</p> </li> <li> <p>You should be viewing the following dashboard</p> <p></p> </li> <li> <p>To access the Terminal Go to Clusters  and click <code>&gt;_VACC Shell Access</code></p> <p></p> </li> </ol>"},{"location":"ch01/01_week1_mmg3320/#additional-information-about-vacc-ood","title":"Additional Information about VACC-OOD","text":"<p>More information about VACC-OOD usage can be found here VACC_OOD-start-guide</p>"},{"location":"ch01/01_week1_mmg3320/#connecting-to-vacc-with-ssh","title":"Connecting to VACC with SSH","text":"<p>If you already had a VACC account and/or are currently working towards generating and analyzing your own data, you may want to learn to log-in without VACC-OOD.</p> <p>To do so, first open your terminal locally on your computer.</p> <p>Once you open your terminal, your screen should look similar to below: </p> <p></p> <p>You see the \"$\" symbol?</p> <p>That is where you write the \"commands\" that will be executed by shell (bash in this case) and your computer's kernel.</p> <p>The \"$\" is called the \"command prompt\".</p>"},{"location":"ch01/01_week1_mmg3320/#what-is-ssh","title":"What is SSH?","text":"<p>There are many ways to connect to another machine over a network, but by far the most common is through the secure shell (SSH). We use SSH because its encrypted. This makes it secure to send passwords and edit private data files. </p>"},{"location":"ch01/01_week1_mmg3320/#step-by-step-instructions","title":"Step-by-step instructions:","text":"<p>Step by step instructions can be found here at the VACC_start_guide</p>"},{"location":"ch01/01_week1_mmg3320/#using-vacc-ood-off-campus","title":"Using VACC-OOD OFF-campus","text":"<p>To use OFF-campus you will need to VPN first. See install-cisco-vpn for more information!</p>"},{"location":"ch01/01_week1_mmg3320/#running-programs","title":"Running Programs","text":"<p>Now that we are logged-in to the VACC, lets explore terminal. Your screen should look similar to the following:</p> <p></p> <p>To run a program, we will follow these basic steps:  </p> <p>The \"$\" is called the \"command prompt\".</p> <p></p> <p>The command prompt on VACC will have some characters before the <code>$</code>, something like <code>[username@vacc-user1 ~]</code>, this is telling you your username and the name of the login node you have connected to.</p> <p>The dollar sign is a prompt which shows us that the shell is waiting for input. Moving forward, when typing commands, either from these lessons or from other sources, do not type in the command prompt $, only the command that follows it.</p> <p>The first thing to do is to check if there are any files in the data folder we are currently in. When you log in to a cluster, you will land within a folder designated specifically for your use, and is referred to as your \"home directory\". We will begin by listing the contents of our home directory using a command called <code>ls</code>.</p> <pre><code>ls \n</code></pre> <p>Tip - <code>ls</code> stands for \"list\" and it lists the contents of a directory.  </p> <p>First let's remove this directory from last semester: </p> <pre><code>rm -r unix_lesson\n</code></pre> <p>Now let's bring in a data folder from a different location on the cluster to our home directory by using the <code>cp</code> command. Copy and paste the following command all the way from <code>cp</code> and including the period symbol at the end <code>.</code></p> <pre><code>cp -r /gpfs1/cl/mmg3320/course_materials/tutorials/unit1_unix .\n</code></pre> <p>Let's break this down. 'cp' is the command for copy. This command required you to specify the location of the item you want to copy (/gpfs1/cl/mmg3320/course_materials/tutorials/unit1_unix) and the location of the destination (.); please note the space between the two in the command. The \u201c-r\u201d is an option that modifies the copy command to do something slightly different than usual. The \".\" means \"here\", i.e. the destination location is where you currently are.</p> <p>Now, you should see \"unit1_unix\" show up as the output of <code>ls</code>. This is a folder we should all have in our home directory.</p> <pre><code>ls \n</code></pre>"},{"location":"ch01/01_week1_mmg3320/#listing-contents-of-data-folder","title":"Listing contents of data folder","text":"<p>Let's look at what is inside the folder \"unit1_unix\" and explore this further. We are use to clicking on a folder name to open it, however, now we are forced to change our mindset and open a folder or \"directory\" differently within the shell environment.</p> <p>To look inside the <code>unit1_unix</code> directory, we need to change which directory we are in. To do this we can use the <code>cd</code> command, which stands for \"change directory\".</p> <pre><code>cd unit1_unix\n</code></pre> <p>Notice the change in your command prompt. The \"~\" symbol from before should have been replaced by the string <code>unit1_unix</code>. This means that our <code>cd</code> command ran successfully and we are now in the new directory. Let's see what is in here by listing the contents:</p> <pre><code>ls \n</code></pre> <p>You should see:</p> <pre><code>## genomics_data  other  raw_fastq  README.txt  reference_data \n</code></pre> <p>Notice that <code>ls</code> has printed the name of the files and directories in the current directory in alphabetical order, arranged neatly into columns.</p>"},{"location":"ch01/01_week1_mmg3320/#arguments","title":"Arguments","text":"<p>There are five items listed when you run <code>ls</code>, but how do you know if these are files or directories with more items inside?</p> <p>To answer this question, we can modify the default behavior of <code>ls</code> by adding an \"argument\" to get more information.</p> <pre><code>ls -F \n</code></pre> <pre><code>## genomics_data/  other/  raw_fastq/  README.txt  reference_data/ \n</code></pre> <p>Anything with a \"/\" after its name is a directory. Things with an asterisk \"*\" after them are programs.  If there are no \"decorations\" after the name, it's a normal text file.</p> <p>Each line of output represents a file or a directory. The directory lines start with <code>d</code>.</p>"},{"location":"ch01/01_week1_mmg3320/#how-to-get-more-information-on-arguments","title":"How to get more information on Arguments","text":"<p>Most commands will take additional arguments that control their behavior. How do we know what arguments are available for a particular command? The most commonly used shell commands have a manual available that can be accessed using the <code>man</code> command. Let's try this command with <code>ls</code>:</p> <pre><code>man ls \n</code></pre> <p></p> <p>This will open the manual page for <code>ls</code> and you will lose the command prompt. It will bring you to a so-called \"buffer\" page, a page you can navigate with your mouse or if you want to use your keyboard we have listed some basic key strokes:</p> <ul> <li>'spacebar' to go forward </li> <li>'b' to go backward </li> <li>Up or down arrows to go forward or backward, respectively</li> </ul> <p>To get out of the <code>man</code> \"buffer\" page and to be able to type commands again on the command prompt, press the <code>q</code> key!</p> <p>Class Exercise</p> <p>Use the <code>-l</code> option for the <code>ls</code> command to display more information for each item in the <code>unit1_unix</code> folder. What additional information is provided that you didn't see with the bare <code>ls</code> command?</p> <p></p>"},{"location":"ch01/01_week1_mmg3320/#summary-of-commands","title":"Summary of Commands","text":"<pre><code>cd          \n  + Change Directory \n      +   used to move throughout the filesystem of a computer\n\nls          \n  + List \n      +   list the contents of a directory\n\nrm \n  + Remove \n      + used to remove a file \n</code></pre>"},{"location":"ch01/01_week1_mmg3320/#citation","title":"Citation","text":"<p>This lesson has been developed by members of the teaching team at the Harvard Chan Bioinformatics Core (HBC). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p> <ul> <li>The materials used in this lesson were derived from work that is Copyright \u00a9 Data Carpentry (http://datacarpentry.org/). All Data Carpentry instructional material is made available under the Creative Commons Attribution license (CC BY 4.0). * Adapted from the lesson by Tracy Teal. Original contributors: Paul Wilson, Milad Fatenejad, Sasha Wood and Radhika Khetani for Software Carpentry (http://software-carpentry.org/) authors: Sheldon  McKay, Mary Piper, Radhika Khetani, Meeta Mistry, Jihe Liu date posted: September 28, 2020</li> </ul> <p>*Other parts of this lesson were derived from:</p> <ul> <li> <p>Erin Alison Becker, Anita Sch\u00fcrch, Tracy Teal, Sheldon John McKay, Jessica Elizabeth Mizzi, Fran\u00e7ois Michonneau, et al. (2019, June). datacarpentry/shell-genomics: Data Carpentry: Introduction to the shell for genomics data, June 2019 (Version v2019.06.1). Zenodo. http://doi.org/10.5281/zenodo.3260560</p> </li> <li> <p>Babraham Bioinformatics - Training Courses. (n.d.). https://www.bioinformatics.babraham.ac.uk/training.html#unix</p> </li> </ul>"},{"location":"ch01/02_week1_mmg3320/","title":"Navigating the Shell (L2)","text":""},{"location":"ch01/02_week1_mmg3320/#learning-objectives-for-todays-lesson","title":"Learning Objectives for Today's Lesson","text":"<ul> <li>Understand the organization of the Filesystem </li> <li>Distinguish between Full versus Relative paths</li> <li>listing, copying, creating, moving and removing data</li> </ul>"},{"location":"ch01/02_week1_mmg3320/#navigating-the-filesystem","title":"Navigating the Filesystem","text":"<p>A filesystem organizes a computer's files and directories into a tree structure. </p> The image above illustrates the Filesystem.  <ul> <li>The first directory in the filesystem is the root directory. It is the parent of all other directories and files in the filesystem. That <code>/</code> or root is the 'top' level.</li> <li>Each parent directory contains child directories and/or files. </li> <li>Each child directory can also contain more files</li> </ul> <p>Note: When you log in to a remote computer you land on one of the branches of that tree, i.e. your pre-designated \"home\" directory that usually has your login name as its name (e.g. <code>/users/username</code>).</p> <p>To navigate the file system with ease we will now introduce the concept of tab completion. </p>"},{"location":"ch01/02_week1_mmg3320/#shortcut-tab-completion","title":"Shortcut: Tab Completion","text":"<p>tab caps lock</p> <p>Typing out file or directory names can waste a lot of time and its easy to make typing mistakes. Instead we should get in the habit of using tab complete as a shortcut. The <code>tab</code> key is located on the left side of your keyboard, right above the <code>caps lock</code> key. When you start typing out the first few characters of a directory name, then hit the <code>tab</code> key, Shell will try to fill in the rest of the directory name. Let's put this into practice now. </p> <p>Navigate into the <code>raw_fastq</code> directory and see what's inside. Remember to use tab!</p> <pre><code>cd raw_fastq/\n</code></pre> <p>Class Exercise</p> <p>List the file sizes of raw_fastq/ folder. </p>"},{"location":"ch01/02_week1_mmg3320/#paths","title":"Paths","text":"<p>Where is the <code>raw_fastq</code> folder in relation to our home directory? To answer this, let's learn more about the \"addresses\" of directories, called \"paths\" and move around the file system.</p> <p>Let's check to see what directory we currently are in. The command prompt tells us which directory we are in, but it doesn't give information about where the <code>raw_fastq</code> directory is with respect to our \"home\" directory. </p> <p>The command to check our current location is <code>pwd</code>, this command does not take any arguments and it returns the path or address of your present working directory (the folder you are in currently).</p> <p></p><pre><code>pwd\n</code></pre> <p>In the output above, each folder is separated from its \"parent\" or \"child\" folder by a \"/\", and the output starts with the root <code>/</code> directory. So, now you are able to determine the location of <code>raw_fastq</code> directory relative to the root directory. </p> <p>But what if you would like to navigate back to your home directory? To do so, one way would be to type <code>cd ~</code> and this will bring you back to your home directory. The \"~\" is an abbreviation for the current user's home folder. </p> <pre><code>cd ~\n</code></pre> <p>After doing this what is your present working directory now?</p> <pre><code>pwd\n</code></pre> <p>It should look something like this: </p> <pre><code>/users/p/d/pdrodrig\n</code></pre> <p>This should display a shorter string of directories starting with root. This is the full address to your home directory, also referred to as \"full path\". </p>"},{"location":"ch01/02_week1_mmg3320/#using-paths-with-commands","title":"Using paths with commands","text":"<p>You can do a lot more with the idea of stringing together parent/child directories. Let's say we want to look at the contents of the <code>raw_fastq</code> folder, but do it from our current directory (the home directory). We can use the list command and follow it up with the path to the folder we want to list!</p> <pre><code>cd\nls ~/unit1_unix/reference_data\n</code></pre> <pre><code>## chr1.fa  chr1-hg19_genes.gtf\n</code></pre> <p>Remember that these two scripts indicate the same path:</p> <pre><code>/users/p/d/pdrodrig/unit1_unix/reference_data\n\nEQUALS \n\n~/unit1_unix/reference_data\n</code></pre> <p>Class Exercise</p> <p>Change directories from <code>~</code> (home) to <code>raw_fastq</code> in a single step then print working directory. </p> <p> Answer <p>cd ~/unit1_unix/raw_fastq</p> </p> <p>Now, what if we want to move back up a level back into the <code>unix_lesson</code> directory?  Type <code>cd unix_lesson</code> and see what happens.</p> <pre><code>unix_lesson/: No such file or directory\n</code></pre> <p>Unfortunately, that won't work because when you say <code>cd unix_lesson</code>, shell is looking for a folder called <code>unix_lesson</code> within your current directory, i.e. <code>raw_fastq</code>.</p> <p>Can you think of an alternative? </p> <ul> <li>You can use the full path to <code>unix_lesson</code></li> <li>You can do cd .. (allows you to move one folder up)</li> </ul> <pre><code>cd .. \n</code></pre>"},{"location":"ch01/02_week1_mmg3320/#file-names","title":"File Names","text":"<p>Probably one of the most frustrating parts of bioinformatics is the lack of consistency with how files are labeled. Files often have obscure names that is only relevant to the researcher, or have names that are very similar to one another. But nonetheless we will persevere! </p> <p>Let's go into the <code>raw_fastq</code>, then type <code>ls Mov10_oe_</code>, followed by pressing the <code>tab</code> key once:</p> <pre><code>cd raw_fastq/\nls Mov10_oe_&lt;tab&gt;\n</code></pre> <p>Notice that nothing happens!!</p> <p>The reason is there are multiple files in the <code>raw_fastq</code> directory that start with <code>Mov10_oe_</code>. As a result, shell does not know which one to fill in. When you hit <code>tab</code> a second time again, the shell will then list all the possible choices.</p> <pre><code>ls Mov10_oe_&lt;tab&gt;&lt;tab&gt;\n</code></pre> <pre><code>## Mov10_oe_1.subset.fq  Mov10_oe_2.subset.fq  Mov10_oe_3.subset.fq\n</code></pre> <p>Now you can select the one you are interested in listed, and enter the number and hit tab again to fill in the complete name of the file.</p> <pre><code>ls Mov10_oe_1&lt;tab&gt;\n</code></pre> <p>Tab completion is your friend! It helps prevent spelling mistakes, and speeds up the process of typing in the full command. We encourage you to use this when working on the command line. </p>"},{"location":"ch01/02_week1_mmg3320/#relative-paths","title":"Relative paths","text":"<p>We have talked about full paths so far, but there are other ways to specify paths to folders and files without having to worry about the root directory. </p> <p>Let's create a folder in raw_fastq using the <code>mkdir</code> command. </p> <pre><code>syntax: `mkdir  name-of-folder-to-be-created`\n</code></pre> <pre><code>mkdir data\n</code></pre> <p>Now let's navigate back into <code>unit1_unix</code> </p> <p>Class Exercise</p> <p>Change directories from <code>unit1_unix</code> to <code>data</code> in a single step. </p> <p> Answer <p>cd raw_fastq/data/</p> </p> <p>In this case we are using a relative path, relative to our current location - wherein we know that the <code>raw_fastq</code> folder is within <code>unit1_unix</code> and <code>data</code> is within the <code>raw_fastq</code> folder. </p>"},{"location":"ch01/02_week1_mmg3320/#synopsis-of-full-versus-relative-paths","title":"Synopsis of Full versus Relative paths","text":"<p>A full path always starts with a <code>/</code>, a relative path does not.</p> <p>A relative path is like getting directions from someone on the street. They tell you to \"go right at the Stop sign, and then turn left on Main Street\". That works great if you're standing there together, but not so well if you're trying to tell someone how to get there from another country. A full path is like GPS coordinates. It tells you exactly where something is no matter where you are right now.</p> <p>You can usually use either a full path or a relative path depending on what is most convenient. If we are in the home directory, it is more convenient to just enter the relative path since it involves less typing. However, when using some programs, full paths are required. </p> <p></p>"},{"location":"ch01/02_week1_mmg3320/#example-using-full-paths","title":"Example using Full Paths:","text":"<p>It can get really complex, real fast!  </p> <pre><code>samtools merge WT_Ikaros_rep2_merged.bam \n/gpfs2/scratch/jrboyd/pipelines/cutruntools/output_mm10_cutnrun_bcell_stim_032621/aligned.aug10/sorted/WTU_Ikaros_H100_rep1_2_S23_L002_aligned_reads.bam \n/users/p/d/pdrodrig/cutnrun_bcell/unmerged_bams/WT_H100_IK_rep1.IK2_S2_L001_aligned_reads.bam;\n</code></pre> <p>Lets break this down! </p> <ul> <li>Program + argument = samtools merge  </li> <li>New file to be created = merge WT_Ikaros_rep2_merged.bam </li> <li>1st file = WTU_Ikaros_H100_rep1_2_S23_L002_aligned_reads.bam</li> <li>2nd file = WT_H100_IK_rep1.IK2_S2_L001_aligned_reads.bam</li> </ul> <p>Over time, it will become easier for you to keep a mental note of the structure of the directories that you are using and how to quickly navigate among them.</p>"},{"location":"ch01/02_week1_mmg3320/#copying-creating-moving-and-removing-data","title":"Copying, creating, moving and removing data","text":"<p>Now we can move around within the directory structure using the command line. But what if we want to do things like copy files or move them from one directory to another, or rename them? </p> <p>Let's move into the <code>raw_fastq</code> directory, this contains some fastq files which are the output of sequencing. </p> <pre><code>cd ~/unit1_unix/raw_fastq\n</code></pre> <p>Tip - These files are referred to as \"raw\" data since it has not been changed or analyzed after being generated.</p>"},{"location":"ch01/02_week1_mmg3320/#copying","title":"Copying","text":"<p>Let's use the copy (<code>cp</code>) command to make a copy of one of the files in this folder, <code>Mov10_oe_1.subset.fq</code>, and call the copied file <code>Mov10_oe_1.subset-copy.fq</code>.  The copy command has the following syntax: </p> <p><code>cp  path/to/item-being-copied  path/to/new-copied-item</code></p> <p>In this case the files are in our current directory, so we just have to specify the name of the file being copied, followed by whatever we want to call the newly copied file.</p> <pre><code>cp Mov10_oe_1.subset.fq Mov10_oe_1.subset-copy.fq\nls -l\n</code></pre> <p>The copy command can also be used for copying over whole directories, but the <code>-r</code> argument has to be added after the <code>cp</code> command. The <code>-r</code> stands for \"recursively copy everything from the directory and its sub-directories\". We used it earlier when we copied over the <code>unit1_unix</code> directory to our home directories.</p>"},{"location":"ch01/02_week1_mmg3320/#creating","title":"Creating","text":"<p>Next, let's create a directory called <code>fastq_backup</code> and we can move the copy of the fastq file into that directory. </p> <pre><code>mkdir fastq_backup\n</code></pre> <p>Tip - File/directory/program names with spaces in them do not work well in Unix, use characters like hyphens or underscores instead. Using underscores instead of spaces is called \"snake_case\". Alternatively, some people choose to skip spaces and rather just capitalize the first letter of each new word (i.e. MyNewFile). This alternative technique is called \"CamelCase\".</p>"},{"location":"ch01/02_week1_mmg3320/#moving","title":"Moving","text":"<p>We can now move our copied fastq file in to the new directory. We can move files around using the move command, <code>mv</code>, syntax: </p> <p><code>mv  path/to/item-being-moved  path/to/destination</code> </p> <p>In this case we can use relative paths and just type the name of the file and folder.</p> <pre><code>mv  Mov10_oe_1.subset-copy.fq  fastq_backup\n</code></pre> <p>Let's check if the move command worked like we wanted:</p> <pre><code>ls -l fastq_backup\n</code></pre>"},{"location":"ch01/02_week1_mmg3320/#renaming","title":"Renaming","text":"<p>The <code>mv</code> command has a second functionality. You can use <code>mv</code> to rename files too. The syntax is identical to when we used <code>mv</code> for moving, but this time instead of giving a directory as its destination, we just give a new name as its destination. </p> <p>Let's try out this functionality!</p> <p>The name Mov10_oe_1.subset-copy.fq is not very informative, we want to make sure that we have the word \"backup\" in it so we don't accidentally delete it.</p> <pre><code>cd fastq_backup\nmv  Mov10_oe_1.subset-copy.fq   Mov10_oe_1.subset-backup.fq\nls\n</code></pre> <p>Tip - You can use move to move a file and rename it at the same time!</p> <p>Important notes about <code>mv</code>:</p> <ul> <li>When using <code>mv</code>, shell will not ask if you are sure that you want to \"replace existing file\" or similar unless you use the -i option. </li> <li>Once replaced, it is not possible to get the replaced file back!</li> </ul>"},{"location":"ch01/02_week1_mmg3320/#removing","title":"Removing","text":"<p>We found out that we did not need to create backups of our fastq files manually as backups were already generated by our collaborator. So in the interest of saving space on the cluster, we want to delete the contents of the <code>fastq-backup</code> folder and the folder itself. </p> <pre><code>rm  Mov10_oe_1.subset-backup.fq\n</code></pre> <p>Important notes about <code>rm</code>:</p> <ul> <li><code>rm</code> permanently removes/deletes the file/folder. </li> <li>There is no concept of \"Trash\" or \"Recycle Bin\" on the command-line. When you use <code>rm</code> to remove/delete they're really gone. </li> <li>Be careful with this command!</li> <li>You can use the <code>-i</code> argument if you want it to ask before removing, <code>rm -i file-name</code>.</li> </ul> <p>Let's delete the fastq_backup folder too. First, we'll have to navigate our way to the parent directory (we can't delete the folder we are currently in/using). </p> <pre><code>cd ..\nrm  fastq_backup \n</code></pre> <p>Did that work or did you get an error?</p> Explanation <p>By default, <code>rm</code>, will NOT delete directories, but you use the <code>-r</code> flag if you are sure that you want to delete the directories and everything within them. To be safe, let's use it with the <code>-i</code> flag.</p> <p></p> <p>Try this instead:  </p><pre><code>rm -ri fastq_backup\n</code></pre> <ul> <li><code>-r</code>: recursive, commonly used as an option when working with directories, e.g. with <code>cp</code>. </li> <li><code>-i</code>: prompt before every removal.</li> </ul>"},{"location":"ch01/02_week1_mmg3320/#summary-of-commands","title":"Summary of Commands","text":"<pre><code>cd          # Change Directory\n               +   used to move throughout the filesystem of a computer \n\nls          # List \n              +   list the contents of a directory\n\npwd         # Print Working Directory   \n              +  displays the file path from the root directory to the current working directory \n\n\ncp          # Copy\n              +   used to copy files or directories \n\nmkdir       # Make Directory\n              +   used to make a new directory \n\nmv          # Move \n              +   move a file into a directory \n\nrm          # Remove\n              +   used to delete files and directories \n</code></pre>"},{"location":"ch01/02_week1_mmg3320/#citation","title":"Citation","text":"<p>This lesson has been developed by members of the teaching team at the Harvard Chan Bioinformatics Core (HBC). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p> <ul> <li>The materials used in this lesson were derived from work that is Copyright \u00a9 Data Carpentry (http://datacarpentry.org/).  All Data Carpentry instructional material is made available under the Creative Commons Attribution license (CC BY 4.0).</li> <li>Adapted from the lesson by Tracy Teal. Original contributors: Paul Wilson, Milad Fatenejad, Sasha Wood and Radhika Khetani for Software Carpentry (http://software-carpentry.org/) authors: Sheldon  McKay, Mary Piper, Radhika Khetani, Meeta Mistry, Jihe Liu date posted: September 28, 2020</li> </ul> <p>Other parts of this lesson were derived from: Erin Alison Becker, Anita Sch\u00fcrch, Tracy Teal, Sheldon John McKay, Jessica Elizabeth Mizzi,  Fran\u00e7ois Michonneau, et al. (2019, June). datacarpentry/shell-genomics: Data Carpentry: Introduction to the shell for genomics data,  June 2019 (Version v2019.06.1). Zenodo. http://doi.org/10.5281/zenodo.3260560</p>"},{"location":"ch01/gitbash-install/","title":"Git Bash Install","text":""},{"location":"ch01/gitbash-install/#download-the-git-for-windows-installer","title":"Download the Git for Windows installer","text":""},{"location":"ch01/gitbash-install/#introduction","title":"Introduction","text":"<p>Git Bash is a command-line interface that provides a Unix-like environment on Windows, allowing you to interact with your system and manage Git repositories efficiently. It combines Git, a popular version control system, with Bash, a shell commonly used in Linux. This tool is essential for anyone learning programming, working on collaborative projects, or managing code for research purposes.</p>"},{"location":"ch01/gitbash-install/#why-use-git-bash","title":"Why Use Git Bash?","text":"<p>Unix Commands on Windows: Git Bash provides Unix command-line utilities like ls, cp, and rm, which are standard in many programming environments.</p> <p>Essential for Programming and Research: Whether you\u2019re working on a software project, analyzing data, or managing code in a research lab, Git Bash simplifies file management and collaboration.</p>"},{"location":"ch01/gitbash-install/#installation","title":"Installation","text":""},{"location":"ch01/gitbash-install/#step-1-watch-this-youtube-video-first","title":"Step 1: Watch this YouTube Video first","text":"<p>This is also another very nice tutorial that will show you the installation process. </p>"},{"location":"ch01/gitbash-install/#step-2-download-and-run-the-installer","title":"Step 2: Download and run the installer","text":"<p>Download and run the git installer from here at gitforwindows.org</p>"},{"location":"ch01/gitbash-install/#step-3-run-the-installer-and-then-follow-the-steps-below","title":"Step 3: Run the installer and then follow the steps below","text":"<ol> <li> <p>Click on \u201cNext\u201d four times (two times if you\u2019ve previously installed Git). You don\u2019t need to change anything in the information, location, components, and start menu screens.</p> </li> <li> <p>Select \u201cUse the nano editor by default\u201d and click on \u201cNext\u201d.</p> </li> <li> <p>Select \"Let Git decide\" and click on \"Next\"</p> </li> <li> <p>Select \"Git from the command line and also from 3rd party software\" and click on \"Next\"</p> </li> <li> <p>Select \u201cUse bundled OpenSSH\u201d and click on \u201cNext\u201d.</p> </li> <li> <p>Select \u201cUse the OpenSSL Library\u201d and click \u201cNext\u201d.</p> </li> <li> <p>Keep \u201cCheckout Windows-style, commit Unix-style line endings\u201d selected and click on \u201cNext\u201d.</p> </li> <li> <p>Select \u201cUse Windows\u2019 default console window\u201d and click on \u201cNext\u201d.</p> </li> <li> <p>Select \u201cDefault (fast-forward on merge)\u201d and click on \u201cNext\u201d.</p> </li> <li> <p>Select \u201cNone\u201d (Do not use a credential helper) and click on \u201cNext\u201d.</p> </li> <li> <p>Select \u201cEnable file system caching\u201d and click on \u201cNext\u201d.</p> </li> <li> <p>Ignore \u201cConfiguring experimental options\u201d and click on \u201cInstall\u201d.</p> </li> <li> <p>Click on \u201cInstall\u201d.</p> </li> <li> <p>Click on \u201cFinish\u201d.</p> </li> <li> <p>If your \u201cHOME\u201d environment variable is not set (or you don\u2019t know what this is):</p> <ol> <li>Open command prompt (Open Start Menu, then type cmd and press [Enter])</li> <li>Type the following line into the command prompt window exactly as shown:  <pre><code>setx HOME \"%USERPROFILE%\"\n</code></pre></li> <li>Press [Enter], and you should see SUCCESS: Specified value was saved.</li> <li>Quit the command prompt by typing exit and then pressing [Enter]</li> </ol> </li> </ol>"},{"location":"ch01/quick-start/","title":"Quick Start","text":""},{"location":"ch01/quick-start/#how-to-log-in-to-vacc-ood","title":"How to log-in to VACC-OOD:","text":"<ol> <li> <p>Use the VACC-OOD link to access the site </p> </li> <li> <p>Add your UVM netid and password</p> </li> <li> <p>You should be viewing the following dashboard</p> <p></p> </li> <li> <p>To access the Terminal Go to Clusters  and click <code>&gt;_VACC Shell Access</code></p> <p></p> </li> </ol>"},{"location":"ch01/quick-start/#additional-information-about-vacc-ood","title":"Additional Information about VACC-OOD","text":"<p>More information about VACC-OOD usage can be found here VACC_OOD-start-guide</p>"},{"location":"ch01/quick-start/#connecting-to-vacc-with-ssh","title":"Connecting to VACC with SSH","text":"<p>Step by step instructions can be found here at the VACC_start_guide</p>"},{"location":"ch01/quick-start/#using-vacc-ood-off-campus","title":"Using VACC-OOD OFF-campus","text":"<p>To use OFF-campus you will need to VPN first. See install-cisco-vpn for more information. </p>"},{"location":"ch02/","title":"Overview","text":""},{"location":"ch02/#learning-objectives","title":"Learning Objectives","text":"<p>At the end of this week, students will be able to: </p> <ol> <li>Learn to use tab completion to simplify typing file and directory paths.  </li> <li>Use of the asterisk <code>*</code> wildcard to match and select multiple files in a directory. </li> <li>View the contents of a file using commands such as <code>less</code>, <code>head</code>, or <code>cat</code>. </li> <li>Use Nano, a simple text editor, to create and edit files directly from terminal. </li> </ol>"},{"location":"ch02/#ongoing-list-of-commands","title":"Ongoing List of Commands","text":"<pre><code>cd          # Change Directory\n               +   used to move throughout the filesystem of a computer \n\nls          # List \n              +   list the contents of a directory\n\npwd         # Print Working Directory   \n              +  displays the file path from the root directory to the current working directory \n\ncp          # Copy\n              +   used to copy files or directories \n\nmkdir       # Make Directory\n              +   used to make a new directory \n\nmv          # Move \n              +   move a file into a directory \n\nrm          # Remove\n              +   used to delete files and directories \n</code></pre>"},{"location":"ch02/#home-directory-or","title":"Home directory or <code>~</code>","text":"<p>Dealing with the home directory is very common. In shell, the tilde character <code>~</code> is a shortcut for your home directory. Let's navigate to the <code>raw_fastq</code> directory: </p> <p>Then enter the command:</p> <pre><code>cd ~\n</code></pre> <p>This allows you to easily navigate to your home directory. The tilde \"~\" is equivalent to <code>/gpfs1/home/p/d/pdrodrig</code> as we had mentioned in the previous lesson.</p>"},{"location":"ch02/#parent-directory-or","title":"Parent directory or <code>..</code>","text":"<p>Another shortcut you encountered in the previous lesson is <code>..</code></p> <pre><code>cd ..\n</code></pre> <p>The shortcut <code>..</code> always refers to the parent directory of whatever directory you are in currently. Let navigate to <code>reference_data</code> to test <code>..</code> once again. </p> <p>We can also chain <code>..</code> together and separate them by <code>/</code> to navigate to two directories above. </p> <pre><code>cd ../..\n</code></pre>"},{"location":"ch02/#current-directory-or","title":"Current directory or <code>.</code>","text":"<p>Finally, <code>.</code> always refers to your current directory. Recall that we use <code>.</code> a number of times, when copying data to your home directory.</p>"},{"location":"ch02/03_week2_mmg3320/","title":"Wild Cards & Nano (L3)","text":""},{"location":"ch02/03_week2_mmg3320/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Learn to use tab completion to simplify typing file and directory paths.  </li> <li>Use of the asterisk <code>*</code> wildcard to match and select multiple files in a directory. </li> <li>View the contents of a file using commands such as <code>less</code>, <code>head</code>, or <code>cat</code>. </li> <li>Use Nano, a simple text editor, to create and edit files directly from terminal. </li> </ul> <p>Class Exercise</p> <p>Please complete the following class activity below. You will have ~5 minutes to complete. </p> <p>Class-activity</p>"},{"location":"ch02/03_week2_mmg3320/#saving-time-with-wildcards-and-other-shortcuts","title":"Saving time with wildcards and other shortcuts","text":"<p>Remember to use tab completion! </p>"},{"location":"ch02/03_week2_mmg3320/#wild-cards","title":"Wild cards","text":"<p>The \"*\" wildcard:</p> <p>Navigate to the <code>~/unit1_unix/raw_fastq</code> directory. This directory contains FASTQ files and these files contain the sequencing reads (nucleotide sequences) output from a high throughput sequencer. </p> <p>Let's see what is inside:  </p> <pre><code>ls\n</code></pre> <pre><code>## Irrel_kd_1.subset.fq  Irrel_kd_2.subset.fq  Irrel_kd_3.subset.fq  Mov10_oe_1.subset.fq  Mov10_oe_2.subset.fq  Mov10_oe_3.subset.fq\n</code></pre> <p>Let's take a moment to point a few things out: </p> <ul> <li>.fq vs .fastq </li> <li>.gz </li> </ul> <p>.gz is a file format used for compressed files. This is especially important when dealing with large data. </p> <p>Class Exercise</p> <p>Create a directory called <code>fastq</code> in <code>~/unit1_unix/raw_fastq</code> directory. Then check to see that it was created. </p> <p>The <code>*</code> or wildcard character is a shortcut for \"everything\". </p> <p>You can use the <code>*</code> by typing in shift + 8. </p> <pre><code>ls *.fq\n</code></pre> <p>Notice, this lists every file that ends with a <code>fq</code> and our newly made directory fastq is missing. </p> <pre><code>Irrel_kd_1.subset.fq    Irrel_kd_3.subset.fq    Mov10_oe_2.subset.fq\nIrrel_kd_2.subset.fq    Mov10_oe_1.subset.fq    Mov10_oe_3.subset.fq\n</code></pre>"},{"location":"ch02/03_week2_mmg3320/#command-history","title":"Command History","text":"<p>You can easily access previous commands by hitting the up arrow key on your keyboard, this way you can step backwards through your command history. On the other hand, the down arrow key takes you forward in the command history.</p> <p>Try it out! While on the command prompt hit the up arrow a few times, and then hit the down arrow a few times until you are back to where you started.</p> <p>You can also review your recent commands with the <code>history</code> command. Just enter:</p> <pre><code>history\n</code></pre> <p>You should see a numbered list of commands, including the <code>history</code> command you just ran! </p> <p>The wildcard <code>*</code> can be placed anywhere in your pattern. For example:</p> <pre><code>ls Mov10*fq\n</code></pre> <p>This lists only the files that begin with 'Mov10' and end with <code>fq</code>.</p> <pre><code>## Mov10_oe_1.subset.fq  Mov10_oe_2.subset.fq  Mov10_oe_3.subset.fq\n</code></pre> <p>So how does this actually work? The Shell (bash) considers an asterisk <code>*</code> to be a wildcard character that can match one or more occurrences of any character, including no character. In the example above the <code>*</code> took place of 13 characters! </p> <p>Tip - An asterisk/star is only one of the many wildcards in Unix, but this is the most powerful one and we will be using this one the most for our exercises.</p>"},{"location":"ch02/03_week2_mmg3320/#the-wildcard","title":"The \"?\" wildcard","text":"<p>Another wildcard that is sometimes helpful is <code>?</code> </p> <ul> <li> <p><code>?</code> is similar to <code>*</code> except that it is a placeholder for exactly one position. </p> </li> <li> <p>Recall that <code>*</code> can represent any number of following positions, including no positions. </p> </li> <li> <p>To highlight this distinction lets look at a few examples. First, try this command:</p> </li> </ul> <p><code>ls /bin/d*</code></p> <p>This will display all files in <code>/bin/</code> that start with \"d\" regardless of length. The <code>bin</code> directory is where some built-in programs are stored. However, if you only wanted the things in <code>/bin/</code> that start with \"d\" and are two characters long then you can use:</p> <p><code>ls /bin/d?</code></p> <p>Lastly, you can chain together multiple \"?\" marks to help specify a length. In the example below, you would be looking for all things in <code>/bin/</code> that start with a \"d\" and have a name length of three characters.  </p> <p><code>ls /bin/d??</code> </p> <p>Class Exercise</p> <p>Perform each of the following tasks using a single <code>ls</code> command without navigating to a different directory.</p> <ol> <li> <p>List all of the files in <code>/bin</code> that start with the letter 'c'</p> </li> <li> <p>List all of the files in <code>/bin</code> that contain the letter 'a'</p> </li> <li> <p>List all of the files in <code>/bin</code> that end with the letter 'o'</p> </li> <li> <p>BONUS: List all of the files in <code>/bin</code> that start with 'ch' and are only 5 letters in length. </p> </li> </ol> Answers <p>Click each question below to reveal the answer.</p> Question 1 <code>ls /bin/c*</code> Question 2 <code>ls /bin/*a*</code> Question 3 <code>ls /bin/*o</code> BONUS <code>ls /bin/ch???</code>"},{"location":"ch02/03_week2_mmg3320/#examining-files","title":"Examining Files","text":"<p>Now let's explore a few more commands to examine files. </p>"},{"location":"ch02/03_week2_mmg3320/#cat-command","title":"<code>cat</code> command","text":"<p>The easiest way to examine a file is to print out all of its contents using the command <code>cat</code>. We can test this out by printing the contents of <code>~/unit1_unix/other/sequences.fa</code></p> <pre><code>cat sequences.fa\n</code></pre> <p>The <code>cat</code> command prints out the all the contents of <code>sequences.fa</code> to the screen.</p> <p><code>cat</code> stands for catenate; it has many uses and printing the contents of a files onto the terminal is one of them.</p> <p>What does this file contain?</p> <pre><code>&gt;SRR014849.1 EIXKN4201CFU84 length=93 \nGGGGGGGGGGGGGGGGCTTTTTTTGTTTGGAACCGAAAGGGTTTTGAATTTCAAACCCTTTTCGGTTTCCAACCTTCCAAAGCAATGCCAATA\n\n&gt;gi|340780744|ref|NC_015850.1| Acidithiobacillus caldus SM-1 chromosome, complete genome\nATGAGTAGTCATTCAGCGCCGACAGCGTTGCAAGATGGAGCCGCGCTGTGGTCCGCCCTATGCGTCCAACTGGAGCTCGTCACGAG\nTCCGCAGCAGTTCAATACCTGGCTGCGGCCCCTGCGTGGCGAATTGCAGGGTCATGAGCTGCGCCTGCTCGCCCCCAATCCCTTCG\nTCCGCGACTGGGTGCGTGAACGCATGGCCGAACTCGTCAAGGAACAGCTGCAGCGGATCGCTCCGGGTTTTGAGCTGGTCTTCGCT\nCTGGACGAAGAGGCAGCAGCGGCGACATCGGCACCGACCGCGAGCATTGCGCCCGAGCGCAGCAGCGCACCCGGTGGTCACCGCCT\nCAACCCAGCCTTCAACTTCCAGTCCTACGTCGAAGGGAAGTCCAATCAGCTCGCCCTGGCGGCAGCCCGCCAGGTTGCCCAGCATC\nCAGGCAAATCCTACAACCCACTGTACATTTATGGTGGTGTGGGCCTCGGCAAGACGCACCTCATGCAGGCCGTGGGCAACGATATC\nCTGCAGCGGCAACCCGAGGCCAAGGTGCTCTATATCAGCTCCGAAGGCTTCATCATGGATATGGTGCGCTCGCTGCAACACAATAC\nCATCAACGACTTCAAACAGCGTTATCGCAAGCTGGACGCCCTGCTCATCGACGACATCCAGTTCTTTGCGGGCAAGGACCGCACCC\n\n&gt;gi|129295|sp|P01013|OVAX_CHICK GENE X PROTEIN (OVALBUMIN-RELATED)\nQIKDLLVSSSTDLDTTLVLVNAIYFKGMWKTAFNAEDTREMPFHVTKQESKPVQMMCMNNSFNVATLPAE\n</code></pre> <p>This is a FASTA file. FASTA format is a text-based format for representing either nucleotide or peptide sequences. The structure of a FASTA file is represented below where the header row always begins with the \"&gt;\" symbol. </p> <p>Question: What command would I use to clear my terminal screen? </p> Answer <p>Ctrl + L </p>"},{"location":"ch02/03_week2_mmg3320/#less-command","title":"<code>less</code> command","text":"<p><code>cat</code> is a terrific command, but notice what it is doing. It is PRINTING the file contents on the screen. </p> <p>When the file is really big and has a ton of lines, this can be cumbersome to use. In practice, when you are running your analyses on the command-line you will most likely be dealing with large files so you need to learn how to view them. </p> <p>Instead, we will use the <code>less</code> command </p> <p></p><pre><code>less Mov10_oe_1.subset.fq\n</code></pre> Rather than printing to screen, the <code>less</code> command opens the file in a new buffer allowing you to navigate through it. Does this look familiar? You might remember encountering a similar interface when you used the <code>man</code> command. This is because <code>man</code> is using the <code>less</code> command to open up the documentation files! The keys used to move around the file are identical to the <code>man</code> command. Below we have listed some additional shortcut keys for navigating through your file when using <code>less</code>. <p>Shortcuts for <code>less</code></p> key action SPACE to go forward b to go backwards g to go to the beginning of the file G to go to the end of a file q to quit or exit <code>less</code> <p>Use the shortcut keys to move through your FASTQ file, we will explore these files in more detail later in the workshop. </p>"},{"location":"ch02/03_week2_mmg3320/#searching-files-with-less","title":"Searching files with <code>less</code>","text":"<p><code>less</code> also gives you a way of searching through files. </p> <p>Just type in / to begin a search, you will see that the <code>/</code> will show up at the  bottom of the <code>less</code> buffer. Let's say you are interested in searching for the following 8-letter adapter sequence: </p> <pre><code>/GGGATAA\n</code></pre> <p>Enter the name of the string of characters you would like to search for and hit the enter key. The interface will move to show you the location where that string is found, and highlight the string. </p> <p>If you hit / then ENTER, <code>less</code> will just repeat the previous search. </p> <p><code>less</code> searches from the current location and works its way forward. For instance, the sequence <code>GGCGAATT</code> was found in our file, but if we started the search at the end of the file, <code>less</code> will not find it. You need to go to the beginning of the file and search.</p> <p>To exit hit q. </p>"},{"location":"ch02/03_week2_mmg3320/#head-and-tail-commands","title":"<code>head</code> and <code>tail</code> commands","text":"<p>There is another way that we can peek inside files. In particular, if we just want to see the beginning or end of the file to see how it's formatted.</p> <p>The commands are <code>head</code> and <code>tail</code> and they just let you look at the beginning and end of a file respectively.</p> <pre><code>head Mov10_oe_1.subset.fq\n</code></pre> <pre><code>tail Mov10_oe_1.subset.fq\n</code></pre> <p>By default, the first or last 10 lines will be printed to screen. The <code>-n</code> option can be used with either of these commands to specify the number <code>n</code> lines of a file to display. For example, let's print the first/last line of the file:</p> <pre><code>head -n 20 Mov10_oe_1.subset.fq\n\ntail -n 20 Mov10_oe_1.subset.fq\n</code></pre>"},{"location":"ch02/03_week2_mmg3320/#good-names-for-files-and-directories","title":"Good names for files and directories","text":"<p>Complicated names of files and directories can make your life painful when working on the command line. Here we provide a few useful tips for the names of your files and directories.</p> <pre><code>Don\u2019t use spaces.\n</code></pre> <p>Spaces can make a name more meaningful, but since spaces are used to separate arguments on the command line it is better to avoid them in names of files and directories. You can use - or _ instead (e.g. fastq-data-files/ rather than fastq data files/). To test this out, try typing mkdir fastq data files and see what directory (or directories!) are made when you check with ls -F.</p> <ol> <li> <p>Don\u2019t begin the name with - (dash).</p> </li> <li> <p>Don\u2019t begin the name with numbers.</p> </li> <li> <p>Stick with letters in the beginning and then use numbers, . (period), - (dash), or an _ (underscore) in the middle of the file or directory name. </p> </li> </ol> <p>You may have noticed by now that all the files we are using are named \u2018something dot something\u2019. </p> <ul> <li>This is just a convention; we can call a file mythesis or almost anything else we want. However, most people use two-part names most of the time to help them (and their programs) tell different kinds of files apart. </li> </ul> <ul> <li>The second part of such a name is called the filename extension and indicates what type of data the file holds: .txt signals a plain text file, .pdf indicates a PDF document, .png is a PNG image, and so on.</li> <li>This is just a convention, albeit an important one. Files merely contain bytes; it\u2019s up to us and our programs to interpret those bytes according to the rules for plain text files, PDF documents, configuration files, images, and so on.</li> <li>However, naming a PNG image of a whale as whale.mp3 doesn\u2019t somehow magically turn it into a recording of whale song, though it might cause the operating system to associate the file with a music player program. In this case, if someone double-clicked whale.mp3 in a file explorer program,the music player will automatically (and erroneously) attempt to open the whale.mp3 file.</li> </ul>"},{"location":"ch02/03_week2_mmg3320/#writing-files","title":"Writing files","text":"<p>We've been able to do a lot of work with files that already exist, but what if we want to create our own files? </p> <p>In order to create or edit files we will need to use a text editor. When we say, \"text editor,\" we really do mean \"text\". These editors can only work with plain character data, not tables, images, or any other media. Text editors can generally be grouped into two categories:  command-line editors and graphical user interface editors. </p>"},{"location":"ch02/03_week2_mmg3320/#command-line-editors","title":"Command-line editors","text":"<p>Some popular editors include:</p> <ul> <li>Emacs</li> <li>Vim</li> <li>Gedit </li> </ul> <p>These are editors which are generally available for use on high-performance compute clusters. There are also simpler editors available for use on the cluster (e.g. Nano), but tend to have limited functionality. We will use Nano in this lesson. </p>"},{"location":"ch02/03_week2_mmg3320/#nano","title":"Nano","text":"<p>Nano is a simple text editor for UNIX/Linux operating systems. Nano is easy-to-use but has its' limitations. </p>"},{"location":"ch02/03_week2_mmg3320/#creating-or-editing-a-file-with-nano","title":"Creating or editing a file with Nano","text":"<p>To create a new file or edit an existing one type: </p> <pre><code>nano filename\n</code></pre> <p>Type the following in your terminal: </p> <pre><code>nano colors.txt\n</code></pre> <p>After pressing the Enter key, the nano editor appears. Notice the following elements: </p> <ul> <li> <p>the top line displays the version of nano in the left corner and the name of the file being edited </p> </li> <li> <p>the 3rd line from the bottom indicates the status of the file you're editing; it shows that color.txt is a \"New File\" </p> </li> <li> <p>the last two lines of the screen present a menu of useful shortcuts for nano. They all will require you to use the control button on your laptop. </p> </li> </ul> <p>At this point we can begin typing: </p> <pre><code>red\nblue \nyellow\n</code></pre> <p>Notice that after your first keystroke, the word \"Modified appears in the upper-right corner. This shows that you have changed the contents of your file but it has not been saved yet. </p> <ul> <li> <p>Saving your work: To save your edited file to disk, press Ctrl-o. Nano displays the current filename. (To save the file under a different name, delete the filename that Nano displays and type a new one.) Press Enter. </p> </li> <li> <p>Exiting Nano:  To exit Nano, press control + x. If you made any changes since the last save, Nano will ask whether or not to save them. Type <code>y</code> for yes or <code>n</code> for no. Press Enter. </p> </li> </ul> <p>Class Exercise</p> <p>You will have ~5 minutes to complete</p> <ol> <li>Make a copy of <code>get-pip.py</code> from this location <code>/gpfs1/cl/mmg3320/course_materials/tutorials</code> and save it in the <code>~/unit1_unix/other</code> directory. </li> <li>Open the <code>get-pip.py</code> to view its contents. </li> <li>Determine the number of lines in <code>get-pip.py</code>. </li> <li>Copy and paste the 3rd line as the answer for the final question in the quiz. </li> <li>Exit and return to terminal </li> </ol>"},{"location":"ch02/03_week2_mmg3320/#summary-basic-nano-commands","title":"Summary Basic nano commands","text":"key action control + X exit from the editor control + A Let's you jump from the beginning of the line control + E Let's you jump from the end of the line control + V Scroll page down control + Y Scroll page up control + O Save the file control + K It cuts the entire selected line"},{"location":"ch02/03_week2_mmg3320/#summary-commands-options-and-keystrokes-covered","title":"Summary: Commands, options, and keystrokes covered","text":"<p>The wildcard *</p> <ul> <li>can represent zero or more other characters</li> <li>can be placed anywhere in your pattern</li> </ul> <pre><code>~           # home dir\n.           # current dir\n..          # parent dir\n*           # wildcard\nctrl + c    # cancel current command\nctrl + a    # start of line\nctrl + e    # end of line\nctrl + l    # clear your terminal screen\nhistory\ncat         # prints out the all the contents of file \nless        # allows you to view and move through file content \nhead        # allows you to view beginning of file \ntail        # allows you to view end of file \nwc -l       # word count showing number of lines in a file        \n</code></pre>"},{"location":"ch02/03_week2_mmg3320/#citation","title":"Citation","text":"<p>This lesson has been developed by members of the teaching team at the Harvard Chan Bioinformatics Core (HBC). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p> <ul> <li>The materials used in this lesson were derived from work that is Copyright \u00a9 Data Carpentry (http://datacarpentry.org/).  All Data Carpentry instructional material is made available under the Creative Commons Attribution license (CC BY 4.0).</li> <li>Adapted from the lesson by Tracy Teal. Original contributors: Paul Wilson, Milad Fatenejad, Sasha Wood and Radhika Khetani for Software Carpentry (http://software-carpentry.org/) </li> <li>Other Authors include: Sheldon  McKay, Mary Piper, Radhika Khetani, Meeta Mistry, Jihe Liu, Mary Piper, Meeta Mistry, Jihe Liu, &amp; Will Gammerdinger</li> </ul>"},{"location":"ch03/","title":"Overview","text":""},{"location":"ch03/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this week, students will be able to:  </p> <ol> <li>Use the <code>grep</code> command to search for specific characters or patterns in a text file.  </li> <li>Redirect command output to a file using output redirection (<code>&gt;</code> and <code>&gt;&gt;</code>) to write or append content.  </li> <li>Apply the pipe (<code>|</code>) operator to create pipelines, connecting the output of one command as the input to another.  </li> <li>Utilize the <code>wc</code> command to count lines, words, and characters in an input.  </li> <li>Employ the <code>sort</code> command to organize input data in a specified order.  </li> <li>Differentiate between <code>&gt;</code> (overwrites content) and <code>&gt;&gt;</code> (appends content) for file redirection.  </li> <li>Construct pipelines (<code>first | second</code>) to efficiently combine multiple commands in sequence.</li> </ol>"},{"location":"ch03/#ongoing-list-of-commands","title":"Ongoing List of Commands","text":"<pre><code>cp [old] [new] copies a file\n\nmkdir [directory-name] creates a new directory\n\nls lists all the files and directories in a specific location \n\nmv [old] [new] moves(renames) a file or directory \n\nrm removes(deletes) a file\n\n`*` matches zero or more characters in a filename, so *.txt matches ALL files ending in .txt \n\n`?` matches a single character in a filename, so ?.txt matches a.txt or b.txt but not ab.txt \n</code></pre> <pre><code>+ Use of the &lt;button&gt;control&lt;/button&gt; key may be described in many ways, including Ctrl-X, Control-X, or ^X. \n\n+ The shell does not have a trash bin, once something is deleted it is really gone. \n\n+ Most file names are `something.extension`. The extension is not required, but does help bioinformatic programs find required files (ex. FASTQC)\n\n+ Depending on the type of work you do, you may need a more powerful text editor than Nano. \n</code></pre>"},{"location":"ch03/04_week3_mmg3320/","title":"Searching and Redirection (L4)","text":""},{"location":"ch03/04_week3_mmg3320/#learning-objectives","title":"Learning objectives","text":"<ul> <li>Search for characters or patterns in a text file using the <code>grep</code> command</li> <li>Write to and append a file using output redirection </li> <li>Use of pipe (<code>|</code>) character  <ul> <li>How can I combine existing commands to do new things? </li> </ul> </li> </ul>"},{"location":"ch03/04_week3_mmg3320/#recap-from-last-week","title":"Recap from last week","text":"<ul> <li><code>*</code> matches zero or more characters in a filename</li> <li><code>?</code> matches a single character in a filename </li> <li>Use of the control key may be described in many ways, including Ctrl-X, Control-X, or ^X. </li> <li>The shell does not have a trash bin, once something is deleted it is really gone. </li> <li>Most file names are something.extension. The extension is not required, but does help bioinformatic programs find required files for downstream processing. </li> <li>Depending on the type of work you do, you may need a more powerful text editor than Nano. </li> </ul> <p>Class Exercise</p> <p>Before moving on, please complete the following class activity below. You will have ~5 minutes to answer all questions. Please submit the quiz without answering the last question. </p> <p>Class-activity</p>"},{"location":"ch03/04_week3_mmg3320/#searching-files-with-grep-command","title":"Searching files with <code>grep</code> command","text":"<p>In the same way that many of us now use \u2018Google\u2019 as a verb meaning \u2018to find\u2019, UNIX programmers often use the word <code>grep</code>. </p> <ul> <li> <p><code>grep</code> is a contraction of \u2018global/regular expression/print\u2019, a common sequence of operations in early UNIX text editors. It is also the name of a very useful command-line program.</p> </li> <li> <p><code>grep</code> allows you to search plain-text files without opening them. </p> </li> </ul> <p>The syntax for <code>grep</code> is as follows: </p> <pre><code>grep  search-term  filename\n</code></pre> <p>The pattern that we want to search is specified in the <code>search-term</code> slot, and the file we want to search within is specified in the <code>filename</code> slot. Let's generate a file to try this on first!  </p> <p>Class Exercise</p> <p>You will have ~5 minutes to complete. NOTE: It is never my intention to rush you. If you do find you need more time to participate, please let me know.</p> <ol> <li> <p>Copy the following text into a text file. Name the text file haiku.txt. </p> <p>It might be good to make a folder called class-exercises or something like that for activities like this moving forward!  </p> <pre><code>The Tao that is seen\nIs not the true Tao, until\nYou bring fresh toner.\n\nWith searching comes loss\nand the presence of absence:\n\"My Thesis\" not found.\n\nYesterday it worked\nToday it is not working\nSoftware is like that.\n</code></pre> </li> <li> <p>Now that you have haiku.txt created save and exit the text editor. </p> </li> <li> <p>On your terminal try the following command: </p> <pre><code>grep not haiku.txt\n</code></pre> </li> <li> <p>Copy-and-paste the output from this command as an answer in the class-activity quiz. </p> </li> <li> <p>Now try the following command: </p> <pre><code>grep Not haiku.txt\n</code></pre> </li> <li> <p>Finally, try the following command: </p> <pre><code>grep The haiku.txt\n</code></pre> </li> </ol>"},{"location":"ch03/04_week3_mmg3320/#class-exercise-discussion","title":"Class Exercise Discussion","text":"<p>In the above exercise, not was the first pattern we were searching for. The <code>grep</code> command searches through the file, and then looks for matches to the pattern specified. To use it you typed <code>grep</code>, then the pattern you were searching for and finally the name of the file (or files) you were searching in. </p> <ul> <li> <p>You also would have noticed that by default <code>grep</code> searches for a pattern in a case-sensitive way. </p> </li> <li> <p>Finally, you would have saw upon completing #6, the search pattern we have selected does not have to form a complete word or phrase as the following where two results: </p> <ul> <li> <p>The Tao</p> </li> <li> <p>My Thesis</p> </li> </ul> </li> </ul>"},{"location":"ch03/04_week3_mmg3320/#other-useful-options-for-grep","title":"Other useful options for grep","text":"<p>To restrict matches to lines containing the word The on its own, we can give <code>grep</code> the <code>-w</code> option. This will limit matches to word boundaries.</p> <pre><code>grep -w\n</code></pre> <p>Sometimes we don\u2019t want to search for a single word, but for a phrase. We can also do this by putting the phrase in quotes: </p> <pre><code>grep \"is not\" haiku.txt\n</code></pre> <p>Another useful option is <code>-n</code> which numbers the lines that match:</p> <p></p><pre><code>grep -n \"it\" haiku.txt\n</code></pre> Here, we can see that lines 5, 9, and 10 contain the letters \u2018it\u2019.  <p><code>grep</code> has lots of other options. To find out what they are, we can type: </p> <pre><code>grep --help\n</code></pre>"},{"location":"ch03/04_week3_mmg3320/#searching-within-fastq-files-with-grep","title":"Searching within FASTQ files with <code>grep</code>","text":"<p>The Next-Generation Sequencing (NGS) technologies all rely on a complex interplay of chemistry, hardware and optical sensors. Adding to this complexity is software to analyze the sensor data to predict the individual bases. This last step in the process is referred to as base-calling. Below is figure from Cancer Biology &amp; Medicine which shows the importance of base-calling in variant analysis. Base-calling algorithms process the raw signal to decode the sequence of bases within strands of DNA or RNA into data stored in BAM or FASTQ files. </p> Summary of technical validity and clinical utility assessment for cancer NGS. (A) NGS basecalling, wherein a DNA sequence and corresponding confidence score is generated from a nuclear genomic DNA template. (B) The next step, which compares all available data to the reference and each other. Variant calling is then performed (underlined bases in panel B), comparing base calls across many reads; many false positive variant calls (x'ed out bases) can be filtered, while true positives (circled bases) should generate a strong signal. (C) Multiple quality metrics are generated during variant calling, which can be compared to cutoffs established during assay validation (dashed lines). (D) Detailed review of available databases and literature (left side) and comparison to clinical history and tumor pathology (right side) to assess clinical utility. VAF, variant allele frequency; QUAL, variant call quality; COSMIC, Catalogue of Somatic Mutations in Cancer; TKIs, tyrosine kinase inhibitor therapies.  <p>FASTQ files contain the sequencing reads (nucleotide sequences) output from a high throughput sequencer. Each sequencing read in a FASTQ file is associated with four lines, with the first line (header line) always starting with an <code>@</code> symbol. A whole FASTQ record for a single read should appear similar to the following:</p> <pre><code>@HWI-ST330:304:H045HADXX:1:1101:1111:61397\nCACTTGTAAGGGCAGGCCCCCTTCACCCTCCCGCTCCTGGGGGANNNNNNNNNNANNNCGAGGCCCTGGGGTAGAGGGNNNNNNNNNNNNNNGATCTTGG\n+\nB?@DDDDDDHHH?GH:?FCBGGB@C?DBEGIIIIAEF;FCGGI#########################################################\n</code></pre> <p>More information about the FASTQ file format</p> Line Description 1 Read name preceded by '@' 2 The actual DNA sequence 3 Read name (same as line 1) preceded by a '+' or just a '+' sign 4 String of characters which represent the quality score of each nucleotide in line 2; must have same number of characters as line 2 <p>So what happens if the sequencer is unable to decide which base (A, G, C, T) is the correct one? In this case, the sequencer will designate an N. </p> <p>Class Exercise</p> <p>Suppose we want to find the exact line(s) in the file Mov10_oe_1.subset.fq that contain 10 consecutive Ns (NNNNNNNNNN).</p> <p>Perform the appropriate command and be ready to report the line number of the first match.</p> <p>Notice, that this output will only return the lines that contain the <code>NNNNNNNNNN</code>. However, each read in the FASTQ file is made up of 4 lines (as discussed above).   </p> <p>So what if we wanted to see the whole FASTQ record for each of the reads? We would need to modify the default behavior of <code>grep</code> and specify additional argument/options. </p> <p>The <code>-B</code> and <code>-A</code> arguments are useful to return the matched line plus one before (<code>-B 1</code>) and two lines after (<code>-A 2</code>). Therefore, using these arguments will return the whole read record. </p> <pre><code>grep -B 1 -A 2 NNNNNNNNNN Mov10_oe_1.subset.fq\n</code></pre> <pre><code>@HWI-ST330:304:H045HADXX:1:1101:1111:61397\nCACTTGTAAGGGCAGGCCCCCTTCACCCTCCCGCTCCTGGGGGANNNNNNNNNNANNNCGAGGCCCTGGGGTAGAGGGNNNNNNNNNNNNNNGATCTTGG\n+\n@?@DDDDDDHHH?GH:?FCBGGB@C?DBEGIIIIAEF;FCGGI#########################################################\n--\n@HWI-ST330:304:H045HADXX:1:1101:1106:89824\nCACAAATCGGCTCAGGAGGCTTGTAGAAAAGCTCAGCTTGACANNNNNNNNNNNNNNNNNGNGNACGAAACNNNNGNNNNNNNNNNNNNNNNNNNGTTGG\n+\n?@@DDDDDB1@?:E?;3A:1?9?E9?&lt;?DGCDGBBDBF@;8DF#########################################################\n</code></pre>"},{"location":"ch03/04_week3_mmg3320/#group-separators-and-how-to-remove-them","title":"Group separators (<code>--</code>), and how to remove them","text":"<p>You will notice that when we use the <code>-B</code> and/or <code>-A</code> arguments with the <code>grep</code> command, the output has some additional lines with dashes (<code>--</code>), these dashes work to separate your returned \"groups\" of lines and are referred to as \"group separators\". This might be problematic if you are trying to maintain the FASTQ file structure or if you simply do not want them in your output. Using the argument <code>--no-group-separator</code> with <code>grep</code> will disable this behavior:</p> <pre><code>grep -B 1 -A 2 --no-group-separator NNNNNNNNNN Mov10_oe_1.subset.fq\n</code></pre> <p>Now your output should be returned as:</p> <pre><code>@HWI-ST330:304:H045HADXX:1:1101:1111:61397\nCACTTGTAAGGGCAGGCCCCCTTCACCCTCCCGCTCCTGGGGGANNNNNNNNNNANNNCGAGGCCCTGGGGTAGAGGGNNNNNNNNNNNNNNGATCTTGG\n+\n@?@DDDDDDHHH?GH:?FCBGGB@C?DBEGIIIIAEF;FCGGI#########################################################\n@HWI-ST330:304:H045HADXX:1:1101:1106:89824\nCACAAATCGGCTCAGGAGGCTTGTAGAAAAGCTCAGCTTGACANNNNNNNNNNNNNNNNNGNGNACGAAACNNNNGNNNNNNNNNNNNNNNNNNNGTTGG\n+\n?@@DDDDDB1@?:E?;3A:1?9?E9?&lt;?DGCDGBBDBF@;8DF#########################################################\n</code></pre>"},{"location":"ch03/04_week3_mmg3320/#which-line-number-has-a-match","title":"Which line number has a match?","text":"<p>A useful option is the <code>-n</code> option which will print out the line number from the file for the match. Add this option to the previous command:</p> <pre><code>grep -B 1 -A 2 --no-group-separator -n NNNNNNNNNN Mov10_oe_1.subset.fq\n</code></pre> <p>This would return the output:</p> <pre><code>861213-@HWI-ST330:304:H045HADXX:1:1101:1111:61397\n861214:CACTTGTAAGGGCAGGCCCCCTTCACCCTCCCGCTCCTGGGGGANNNNNNNNNNANNNCGAGGCCCTGGGGTAGAGGGNNNNNNNNNNNNNNGATCTTGG\n861215-+\n861216-@?@DDDDDDHHH?GH:?FCBGGB@C?DBEGIIIIAEF;FCGGI#########################################################\n861953-@HWI-ST330:304:H045HADXX:1:1101:1106:89824\n861954:CACAAATCGGCTCAGGAGGCTTGTAGAAAAGCTCAGCTTGACANNNNNNNNNNNNNNNNNGNGNACGAAACNNNNGNNNNNNNNNNNNNNNNNNNGTTGG\n861955-+\n861956-?@@DDDDDB1@?:E?;3A:1?9?E9?&lt;?DGCDGBBDBF@;8DF#########################################################\n</code></pre> <p>A small thing you should note is that when using the <code>-n</code> option, lines that have a <code>:</code> after the line number correspond to the lines with the match (e.g <code>861214:CACTTGTAAGGGCAGGCCCCCTTCACCCTCCCGCTCCTGGGGGANNNNNNNNNNANNN...</code>), while lines with a <code>-</code> after the line number are the surrounding lines retrieved when using the <code>-A</code> and/or <code>-B</code> options (e.g. <code>861213-@HWI-ST330:304:H045HADXX:1:1101:1111:61397</code>).</p> <p>Class Exercise</p> <ol> <li> <p>Use <code>grep</code> to search for the sequence CTCAATGAGCCA in <code>Mov10_oe_1.subset.fq</code>. Write the command so that only the read name and sequence are shown.</p> </li> <li> <p>If you wanted to search for the above sequence in all Mov10 replicate fastq files, what command would you use? Which FASTQ file contains the most reads with the 'CTCAATGAGCCA' barcode?</p> </li> </ol>"},{"location":"ch03/04_week3_mmg3320/#redirection","title":"Redirection","text":"<p>When we use <code>grep</code>, the matching lines are displayed in the Terminal (also called Standard Output or stdout). If the result of the <code>grep</code> search contains only a few lines, we can easily read them. However, if the output is very long, the lines will keep scrolling, and we\u2019ll only be able to see the last few lines on the screen. You might have experienced this when searching for the pattern <code>NNNNNNNNNN</code>.</p> <p>So, how can we capture this output and save it for later review instead of letting it scroll past in the Terminal?</p> <p>We can achieve this using redirection. Redirection allows us to send the output from the Terminal to another destination. In this case, we can save the output to a file, which lets us examine it at our convenience.</p>"},{"location":"ch03/04_week3_mmg3320/#redirecting-with-aka-greater-than-sign","title":"Redirecting with <code>&gt;</code> AKA \"Greater-than sign\"","text":"<p>The redirection command for writing something into a file is <code>&gt;</code>.</p> <p>Let's put all the sequences that contain <code>NNNNNNNNNN</code> from the <code>Mov10_oe_1.subset.fq</code> into another file called <code>bad_reads.txt</code>.</p> <pre><code>grep -B 1 -A 2 NNNNNNNNNN Mov10_oe_1.subset.fq &gt; bad_reads.txt\n</code></pre> <p>Now you should have a new file called <code>bad_reads.txt</code> in the raw_fastq directory. </p> <pre><code> ls -l \n</code></pre> <p>Take a look at the file and see if it contains what you think it should. </p> <p>NOTE: If we already had a file named <code>bad_reads.txt</code> in our directory, it would have overwritten it without any warning!</p>"},{"location":"ch03/04_week3_mmg3320/#redirecting-and-appending-with","title":"Redirecting (and appending) with <code>&gt;&gt;</code>","text":"<p>The redirection command for appending something to an existing file is <code>&gt;&gt;</code>.</p> <p>If we use <code>&gt;&gt;</code> it will append to the existing content in a file rather than overwrite it. This can be useful for saving more than one search. For example, the following command will append the bad reads from Mov10_oe_2 to the <code>bad_reads.txt</code> file that we just generated.</p> <pre><code> grep -B 1 -A 2 NNNNNNNNNN Mov10_oe_2.subset.fq &gt;&gt; bad_reads.txt\n ls -l \n</code></pre> <p>Did the size of the <code>bad_reads.txt</code> file change? How do we explain this? </p>"},{"location":"ch03/04_week3_mmg3320/#passing-output-to-another-command-with-or-pipe","title":"Passing output to another command with <code>|</code> (or pipe)","text":"<p>The vertical bar or pipe key | is very likely not something you use very often. It is on the same key as the back slash, right above the (Enter/Return)key. </p> <p>What <code>|</code> does is take the output from one command and runs it through the command specified after it. </p> <p>First, to really see the benefit let's type the following command again: </p> <pre><code>grep -B 1 -A 2 NNNNNNNNNN Mov10_oe_1.subset.fq \n</code></pre> <p>Notice that we are at the end of the document automatically. It just whizzed on by! And if we wanted to find the first line, it get's messy and \"jumpy\" - there are even gaps! </p> <p>Now let's pipe the output of <code>grep</code> command to <code>less</code>. This will allow us to slowly scroll through the entire document using the up and down arrows! </p> <pre><code>grep -B 1 -A 2 NNNNNNNNNN Mov10_oe_1.subset.fq | less\n</code></pre> <p>Remember to use <code>q</code> to get out of less</p> <p>Or let's say we are interested in the first few lines, we could do the following:</p> <pre><code>grep -B 1 -A 2 NNNNNNNNNN Mov10_oe_1.subset.fq | head -n 5\n</code></pre> <p>Another thing we can also do is count the number of lines output by <code>grep</code>. </p> <p>We will introduce another command called <code>wc</code>. The <code>wc</code> command stands for w*ord c*ount. This command counts the number of lines, words and characters in the text input given to it. The <code>-l</code> argument will only count the number of lines instead of counting everything.</p> <p></p><pre><code>grep -B 1 -A 2 NNNNNNNNNN Mov10_oe_1.subset.fq | wc -l\n</code></pre> There are a total of 148 lines that are shown as the output.  <p>How many lines contain <code>NNNNNNNNNN</code> in the document Mov10_oe_1.subset.fq?  </p>"},{"location":"ch03/04_week3_mmg3320/#the-power-of-pipes","title":"The Power of Pipes","text":"<ul> <li>The pipe is a very important/powerful concept in Shell</li> <li>You can string along as many commands together as you like</li> </ul> <p>The philosophy behind the three redirection operators (<code>&gt;</code>, <code>&gt;&gt;</code>, <code>|</code>) you have learned so far is that none of them by themselves do a lot. BUT when you start chaining them together, you can do some really powerful things really efficiently. </p> <p>To be able to use the shell effectively, becoming proficient in the use of the pipe and redirection operators is essential.</p>"},{"location":"ch03/04_week3_mmg3320/#practice-with-searching-and-pipingredirection","title":"Practice with searching and piping/redirection","text":"<p>Let's use the new commands in our toolkit and a few new ones to examine the gene annotation file chr1-hg19_genes.gtf. We will be using this file to find the genomic coordinates of all known exons on chromosome 1. This file is located in the following directory: </p> <pre><code>~/unix_lesson/reference_data/\n</code></pre>"},{"location":"ch03/04_week3_mmg3320/#introducing-the-gtf-file-format","title":"Introducing the GTF file format","text":"<p>Let's explore our <code>chr1-hg19_genes.gtf</code> file a bit. What information does it contain?</p> <p></p><pre><code>head -10 chr1-hg19_genes.gtf | less -S \n</code></pre> <pre><code>chr1    unknown exon    14362   14829   .       -       .       gene_id \"WASH7P\"\nchr1    unknown exon    14970   15038   .       -       .       gene_id \"WASH7P\"\nchr1    unknown exon    15796   15947   .       -       .       gene_id \"WASH7P\"\nchr1    unknown exon    16607   16765   .       -       .       gene_id \"WASH7P\"\nchr1    unknown exon    16858   17055   .       -       .       gene_id \"WASH7P\"\nchr1    unknown exon    17233   17368   .       -       .       gene_id \"WASH7P\"\nchr1    unknown exon    17606   17742   .       -       .       gene_id \"WASH7P\"\nchr1    unknown exon    17915   18061   .       -       .       gene_id \"WASH7P\"\nchr1    unknown exon    18268   18366   .       -       .       gene_id \"WASH7P\"\nchr1    unknown exon    24738   24891   .       -       .       gene_id \"WASH7P\n</code></pre> <p>The GTF (Gene Transfer Format) file is a tab-delimited file with information arranged in a very specific manner, usually for NGS analysis. That information is specifically about all the various entities or features found in a genome; in this case, on chromosome 1. </p> <p>The columns in the GTF file contain the genomic coordinates (location) of gene features (exon, start_codon, stop_codon, CDS) along with the associated gene_names, transcript_ids and protein_ids (p_id). </p> Line Description 1 chromosome number 2 source, name of program that generated the feature - its \"unknown\" above 3 feature type name 4 start position of feature 5 end position of feature 6 score 7 strand, defined at + (forward) or - (reverse) 8 frame 9 attribute, provides additional information about each feature <p>For more information on this file format, check out the Ensembl site. </p> <p>Given our understanding of splice isoforms, we know that a given exon can be part of 2 or more different transcripts generated from the same gene. In a GTF file, this exon will be represented multiple times, once for each transcript (or splice isoform). For example, </p> <pre><code>grep PLEKHN1 chr1-hg19_genes.gtf | less -S\n</code></pre> <p>This search returns two different transcripts of the same gene, NM_001160184 and NM_032129, that contain the same exon.</p> <p>Class Exercise: Cut</p> <p><code>cut</code> is a command that extracts columns from files. Use <code>cut</code> with the <code>-f</code> argument to specify which specific fields or columns from the dataset you want to extract. </p> <p>Identify all entries in <code>chr1-hg19_genes.gtf</code> that correspond to the gene PLEKHN1.</p> <pre><code>From those entries, extract:\n+ Column 1 -&gt; chromosome\n+ Column 4 -&gt; start coordinate\n+ Column 5 -&gt; end coordinate\n</code></pre> <p>Save the output to a new file named: coordinates_PLEKHN1.txt</p> <p>Class Exercise: Sort</p> <p>You have generated a file called: coordinates_PLEKHN1.txt. This file contains genomic coordinates for the gene PLEKHN1, extracted from a GTF annotation file.</p> <ul> <li>First, determine how many total coordinate entries exist for PLEKHN1. </li> <li>Then, determine how many unique coordinate entries exist using the sort command. </li> </ul>"},{"location":"ch03/04_week3_mmg3320/#summary","title":"Summary","text":"<pre><code>grep          # Allows for searching within files without opening them\n               +   grep search_term filename \n\n&gt;             # Redirect output to another file  \n\n&gt;&gt;            # append to an existing file rather than overwrite it  \n\n|             # Pipe key \n                + takes the output and runs it through the command specified after it \n\ncut           # used to extract specific columns from a tab-delimited file \n\nsort          # used to sort a specific column within a tab-delimited file\n</code></pre>"},{"location":"ch03/04_week3_mmg3320/#citation","title":"Citation","text":"<p>This lesson has been developed by members of the teaching team at the Harvard Chan Bioinformatics Core (HBC). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p> <ul> <li> <p>The materials used in this lesson were derived from work that is Copyright \u00a9 Data Carpentry (http://datacarpentry.org/).  All Data Carpentry instructional material is made available under the Creative Commons Attribution license (CC BY 4.0).</p> </li> <li> <p>Adapted from the lesson by Tracy Teal. Contributors: Paul Wilson, Milad Fatenejad, Sasha Wood, and Radhika Khetani for Software Carpentry (http://software-carpentry.org/)</p> </li> <li> <p>Original Authors: Sheldon  McKay, Bob Freeman, Mary Piper, Radhika Khetani, Meeta Mistry, Jihe Liu, Will Gammerdinger</p> </li> </ul>"},{"location":"ch03/05_week3_mmg3320/","title":"Intro to Shell Scripting (L5)","text":""},{"location":"ch03/05_week3_mmg3320/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Capture commands into a shell script</li> <li>Implement variables in a shell script</li> </ul>"},{"location":"ch03/05_week3_mmg3320/#recap","title":"Recap","text":"<ul> <li><code>wc</code> counts lines, words, and characters in its inputs </li> <li>command <code>&gt;</code> [file] redirects a command's output to a file (overwriting any existing content)</li> <li>command <code>&gt;&gt;</code> [file] appends a command's output to a file </li> <li>[first] <code>|</code> [second] is a pipeline: the output of the first command is used as the input to the second</li> </ul>"},{"location":"ch03/05_week3_mmg3320/#cut","title":"Cut","text":"<ul> <li><code>cut</code> is a command that extracts columns from files. </li> </ul> <p>We can use <code>cut</code> with the <code>-f</code> argument to specify which specific fields or columns from the dataset we want to extract. Let's say we want to get the 1st and 3rd column of a file, we can use:</p> <pre><code>cut -f 1,3 file.csv\n</code></pre> <p>Please note: the argument <code>-f 1,3</code> cannot be written with spaces between the field numbers. If you write it as <code>-f 1, 3</code> (with a space), the cut command will treat 1, as one argument and 3 as another, leading to an error.</p> <pre><code>cut -f 1, 3 file.csv\n# Error: \"cut: invalid byte, character, or field list\"\n</code></pre> <p>Class Exercise: Participation Grade</p> <p>Before moving on, please complete the following class activity below. You will have ~10 minutes to answer all questions.</p> <p>Class-activity</p>"},{"location":"ch03/05_week3_mmg3320/#sort","title":"Sort","text":"<ul> <li><code>sort</code> is a command used to sort lines in text files in a particular order. The sort command comes with many options, some of them are further explained below: </li> </ul> Option Description Example Usage <code>-u</code> (unique) Outputs unique lines <code>sort -u [FILE]</code> <code>-k</code> (key) Allows sorting based on a specific key field/column in each line <code>sort -k 2 [FILE]</code> <code>-n</code> (numerics) Performs a numeric sort <code>sort -n [FILE]</code>"},{"location":"ch03/05_week3_mmg3320/#bed-file-structure","title":"BED File Structure","text":"<p>A BED file is a widely used format in genomics to store genomics regions and associated annotations. It is primarily used for specifying intervals or features on a reference genome such as genes, exons, or regulatory elements. A BED file consists of at least three required columns but can include additional optional columns. </p> <p>Required Columns:</p> <ol> <li>Chromosome: The name of the chromosome (<code>chr1</code>)</li> <li>Start: The start position of a feature </li> <li>End: The end position of a feature</li> </ol> <p>Class Exercise #2</p> <p>Dr.Patel: \"Emma I noticed that the file <code>coordinates_PLEKHN1.txt</code> contained a number of duplicates. Can you please modify this file and save it as a BED file so I can upload it to UCSC Genome Browser. The file extension is <code>.bed</code>. Thanks.\"</p>"},{"location":"ch03/05_week3_mmg3320/#shell-scripts","title":"Shell scripts","text":"<p>Over the past few weeks, you have been introduced to a number of commands to explore data files. To demonstrate the function of each command we have run them one at a time at the command prompt. The command prompt is useful for testing out commands and also performing simple tasks like exploring and organizing the file system. However, when we are running bioinformatic analyses which require a series of tasks to be run, there is a more efficient way to do this. </p> <p>Shell scripts are text files that contain commands we know we want to run. </p>"},{"location":"ch03/05_week3_mmg3320/#what-can-shell-scripts-be-used-for","title":"What can shell scripts be used for?","text":"<ul> <li>for creating, maintaining and implementing system-wide scripts</li> <li>automating tedious repetitive tasks</li> <li>scheduling and executing system tasks</li> <li>for automating the installation process for new software or for new software updates across the organization</li> <li>for scheduling data backup process</li> </ul>"},{"location":"ch03/05_week3_mmg3320/#writing-a-simple-shell-script","title":"Writing a simple shell script","text":"<p>We are ready to see what makes the shell such a powerful programming environment. To create our first script, we are going to take some of the commands we have run previously and save them into a file so that we can re-run all those operations again later, by typing just one single command. For historical reasons, a bunch of commands saved in a file is referred to as shell script, but make no mistake, this is actually a small program! </p> <p>Interestingly, when working on the command line you can give files any extension (.txt, .tsv, .csv, etc.). Similarly, for a shell script you don't need a specific extension. However, it is best practice to give shell scripts the extension <code>.sh</code> (bash shell script file). </p> <p>Class Exercise #3</p> <p>You will have ~5 minutes to complete. NOTE: It is never my intention to rush you. If you do find you need more time to participate, please let me know.</p> <p>Objective: Create a script that contains 3 lines: </p> <ol> <li> <p>Navigate into the <code>raw_fastq</code> directory and create a new file, call it <code>practice.sh</code>. </p> </li> <li> <p>Write the First line. This line should: </p> <ul> <li>Grab all reads from <code>Mov10_oe_1.subset.fq</code> that contain 10 consecutive N's </li> <li>Be sure to use the appropriate arguments to output all (4) lines contained within a FASTQ read </li> <li>Then redirect the output to a new file called <code>redirect.txt</code> </li> </ul> </li> <li> <p>Copy and paste the <code>echo</code> line below: </p> <pre><code>echo \"The number of lines in redirect.txt is:\" \n</code></pre> <ul> <li>You are adding some verbosity to your script by using the <code>echo</code> command. The <code>echo</code> command is used to display a line of text that is passed in as an argument. </li> </ul> </li> <li> <p>Write the Third line. This line should:</p> <ul> <li>Count the number of lines in redirect.txt </li> </ul> </li> </ol> <p>To run the shell script you can use the <code>sh</code> command, followed by the name of your script:</p> <pre><code>```bash\nsh practice.sh\n```\n</code></pre> <p>Final Question: Dr.Patel: \"Emma thanks for carrying out that exercise. How many FASTQ reads in <code>Mov10_oe_1.subset.fq</code> contained 10 consecutive N's or more?</p> <p>Submit your answer in Class Participation survey.</p> <p>  One FASTQ Read  </p>"},{"location":"ch03/05_week3_mmg3320/#executing-scripts","title":"Executing Scripts","text":"<p>There are two main ways to execute a script: </p> <p>1. Direct Invocation by the Shell: When you use <code>sh script-name.sh</code>, the shell (i.e. way to interact with kernel) reads and executes the script file. The executable permissions of the script file is not checked because we are directly passing it as an argument to the shell interpreter. </p> <ul> <li><code>sh script.sh</code>:  The script does not need executable permissions as <code>sh</code> is invoked manually </li> <li>Best for testing scripts</li> </ul> <p>2. Executable Permissions: Making a script executable allows you to run it directly using <code>./script-name.sh</code>. Here, the kernel (core of the OS, manages hardware and system resources) will check if the script file has executable (<code>x</code>) permissions and invokes the interpreter specified in the scripts shebang line. </p> <ul> <li><code>./script.sh</code>: Requires the script file to have executable permissions and the kernel uses the interpreter specified in the shebang line. </li> <li>Best for automation and reusable scripts </li> </ul>"},{"location":"ch03/05_week3_mmg3320/#bash-variables","title":"Bash variables","text":"<p>A variable is a common concept shared by many programming languages. Think of variables as a temporary store or bucket for a piece of information. This bucket will have a name associated with it therefore when referring to the information inside the bucket, we can use the name of the bucket instead! </p> <p>First, to create a variable in bash, you will provide the name of the variable, followed by the equals sign and finish with the value we want to assign to the variable. </p> <pre><code>name_of_variable=value_of_variable\n</code></pre> <ul> <li> <p>Note that the variable name cannot contain spaces, nor can there be spaces on either side of the equals sign.</p> </li> <li> <p>The variable name can have only letters (a to z or A to Z), numbers (0 to 9), or the underscore character (_). The wrong character usage in the variable name will cause a syntax error.</p> </li> <li> <p>By convention, the variable names in UNIX are in UPPERCASE.</p> </li> </ul> <p>Let's start by creating a variable called <code>NUM</code> that has the number 25 stored inside it:</p> <p></p><pre><code>NUM=25\n</code></pre> If we are using our bucket analogy - You can think of the variable <code>NUM</code> like this:   <p>Once you press return, you will find yourself back at the command prompt. But nothing happened... so how do we know that we actually created a variable? </p> <p>One way to see the variable created is by using the <code>echo</code> command. </p>"},{"location":"ch03/05_week3_mmg3320/#echo-command","title":"<code>echo</code> command","text":"<p>The <code>echo</code> command is used to display text or the value of variables to the terminal/standard output. </p> <pre><code>echo \"Hello, World!\"\n</code></pre> <pre><code>## Hello, World!\"\n</code></pre> <p>In the case of variable values: </p> <pre><code>NAME=ALICE\necho \"Hello, $NAME!\"\n</code></pre> <pre><code>## Hello, Alice!\n</code></pre> <p>To display the contents of the variable we need to explicitly use a <code>$</code> in front of the variable name:</p> <pre><code>echo $NUM\n</code></pre> <p>You should see the number 25 returned to you. Notice that when we created the variable, we did not use the <code>$</code>. </p>"},{"location":"ch03/05_week3_mmg3320/#what-is-the","title":"What is the <code>$</code>","text":"<p>The <code>$</code> is a standard shell notation for defining and using variables. The <code>$</code> tells the shell interpreter to treat the variable as a variable name and substitute its value in its place, rather than treat it as text or an external command.</p> <p>Therefore, when defining a variable (i.e. setting the value) you can just type it as is, but when retrieving the value of a variable you must use the <code>$</code>! </p> <p>NOTE: Variables are not physical entities like files. When you create files you can use <code>ls</code> to list contents and see if the file exists. To list all variables in your environment you can use the command <code>declare</code> with the <code>-p</code> option. You will notice that while you only have created one variable so far, the output of <code>declare -p</code> will be more than just one variable. These other variables are called environment variables. To remove a variable you can use <code>unset</code>. </p>"},{"location":"ch03/05_week3_mmg3320/#use-variables-as-input-to-commands","title":"Use variables as input to commands","text":"<p>One important aspect of the variable is that the value stored inside can be used as input to commands. </p> <p>Let's solidify this important concept. </p> <p>Class Exercise #4</p> <ol> <li> <p>Create a new variable called <code>FILE</code>. Use the name of one of the fastq files in the <code>raw_fastq</code> directory as the value of the variable.</p> </li> <li> <p>Recall the variable with <code>echo</code> </p> </li> <li> <p>Check the number of lines in the <code>FILE</code> variable. </p> </li> </ol> <p>NOTE: The variables we create in a session are system-wide, and independent of where you are in the filesystem. This is why we can reference it from any directory. However, it is only available for your current session. If you exit the cluster and login again at a later time, the variables you have created will no longer exist.</p>"},{"location":"ch03/05_week3_mmg3320/#basename","title":"<code>basename</code>","text":"<p>When creating shell scripts variables are used to store information that can be used later in the script (once or many times over). The value stored can be hard-coded in as we have done above, assigning the variable a numeric or character value. Alternatively, the value stored can be the output of another command. We will demonstrate this using a new command called <code>basename</code>.</p> <p>The <code>basename</code> command is used to extract the file name or directory name from a given file path. This is accomplished using string splitting. </p> <p>String splitting is a way to break a larger string into smaller parts based on a specified delimiter. The delimiter is a character or a sequence of characters that indicates where the string should be split. For example, if you have the string \"apple,banana,orange\" and use a comma as the delimiter, you can split it into three separate strings: \"apple\", \"banana\", and \"orange\". It's a handy technique often used in programming for data manipulation and analysis.</p> <p>Other \"common\" strings used include: </p> <ul> <li>Space: Useful for splitting words in a sentence.</li> <li>Tab (\\t): Often used in tab-delimited data files.</li> <li>Semicolon (;): Another popular choice for separating values in data.</li> <li>Colon (:): Commonly used in key-value pairs.</li> <li>Pipe (|): Used in various data formats, such as CSV files.</li> <li>Hyphen (-): Can be used to split ranges or parts of a string.</li> <li>Underscore (_): Frequently used in variable or function names.</li> <li>Forward dash (/): Useful for splitting file paths. </li> </ul> <p>Put more simply, a basename refers to the file or directory name without its path information. It essentially provides the core name of the file or directory by removing the directory path and any leading prefixes or extensions. </p>"},{"location":"ch03/05_week3_mmg3320/#basic-usage-of-basename","title":"Basic Usage of <code>basename</code>:","text":"<p>1. Extract file name</p> <pre><code>basename /path/to/file.txt\n</code></pre> <pre><code>## file.txt\n</code></pre> <p>For example, if you have the file path <code>/home/user/documents/report.txt</code> the basename would be <code>report.txt</code>. </p> <p>Let's try an example together: </p> <pre><code>basename ~/unit1_unix/raw_fastq/Mov10_oe_1.subset.fq\n</code></pre> <p>The command returns only the file name. </p> <p>2. Remove file extension</p> <pre><code>basename /path/to/file.txt .txt\n</code></pre> <pre><code>## file\n</code></pre> <p>Suppose we wanted to also trim off the file extension (i.e. remove <code>.fq</code> leaving only the file base name). We can do this by adding a parameter to the command to specify what string of characters we want trimmed.</p> <pre><code>basename ~/unit1_unix/raw_fastq/Mov10_oe_1.subset.fq .fq\n</code></pre> <p>You should now see that only <code>Mov10_oe_1.subset</code> is returned. </p> <p>Class Exercise #5</p> <p>Use <code>basename</code> with the file <code>Irrel_kd_1.subset.fq</code> as input. Return only <code>Irrel_kd_1</code> to the terminal.</p>"},{"location":"ch03/05_week3_mmg3320/#storing-the-basename-output-in-a-variable","title":"Storing the <code>basename</code> output in a variable","text":"<p>The <code>basename</code> command returns a character string and this too can be stored inside a variable. To do this without error, we need to add another special syntax because when we run the command we will generate spaces. If you remember earlier, one of the rules of creating variables is that there cannot be any spaces. </p> <p>NOTE: The special syntax involves a key that is probably not used much on your keyboard, it is the backtick key `. On most keyboards this character is located just underneath the esc key. If you have trouble finding it you can also just copy and paste it from the materials.</p> <pre><code>VARIABLE=`basename /path/to/file`\n</code></pre> <p>Let's try an example:</p> <pre><code>samplename=`basename ~/unit1_unix/raw_fastq/Mov10_oe_1.subset.fq .fq`\n</code></pre> <p>Once you press return you should be back at the command prompt. Check to see what got stored in the <code>samplename</code> variable:</p> <pre><code>echo $samplename\n</code></pre>"},{"location":"ch03/05_week3_mmg3320/#the-basename-command","title":"The <code>basename</code> command","text":"<p>It is hard to see the utility of this command by just running it at command-line, but it is very useful command when creating scripts for analysis. Within a script it is common to create an output file and the <code>basename</code> allows us to easily create a prefix to use for naming the output files. This utility will be demonstrated in more detail next. </p>"},{"location":"ch03/05_week3_mmg3320/#shell-scripting-with-bash-variables-using-jupyter-notebook","title":"Shell scripting with bash variables using Jupyter Notebook","text":"<p>Now it's time to put all of these concepts together to create a more advanced version of the script. This script will allow the user to get information on any given directory. These are the steps you will code into a shell script using Jupyter Notebook:</p> <ol> <li>Assign the path of the directory to a variable</li> <li>Create a variable that stores only the directory name (and no path information)</li> <li>Move from the current location in the filesystem into the directory we selected in 1.</li> <li>List the contents of the directory</li> <li>List the total number of files in the directory</li> </ol> <p>It seems like a lot, but you are equipped with all the necessary concepts and commands to do this quite easily!</p>"},{"location":"ch03/05_week3_mmg3320/#what-is-jupyter-notebook","title":"What is Jupyter Notebook","text":"<p>Jupyter Notebook is an open-source web application that allows you to create and share documents containing live code, equations, visualizations, and narrative text. It's widely used in data science, scientific research, and education. The term \"Jupyter\" is derived from the combination of three core programming languages it supports: Julia, Python, and R.</p> <p>Slurm Account: mmg3320  Partition: general Everything else leave as default  Press Launch</p> <p>Class Exercise #6 and your Homework Assignment Part B</p> <p>This is a self-paced assignment. This is also the final assignment for today. If you would like to do this from home feel free! You will need to submit (2) screenshots with your homework for this week. </p> <ol> <li> <p>To get started move into the <code>other</code> directory. Jupyter Notebook is user-friendly. You should be able to click from <code>unit1_unix</code> into <code>other</code> easily. </p> </li> <li> <p>Press New (Right-side) -&gt; New file -&gt; and create a script called <code>directory_info.sh</code>. </p> <ul> <li>In this script, we will be adding comments by using the hashtag symbol <code>#</code>. Lines in your script that begin with <code>#</code> will not be interpreted as code by command-line. Comments are crucial for proper documentation of your scripts. This will allow your future self to know what each line of code is doing!</li> </ul> </li> <li> <p>Copy and paste the line below as line 1. </p> <pre><code>## USAGE: Provide a full path to the directory you want information on\n</code></pre> </li> <li> <p>Have your script create a variable called <code>dirPath</code>. Assign this variable as the full path to <code>raw_fastq\\</code>. This will be line 2. </p> </li> <li> <p>Skip a line and then copy and paste the line below to line 4. Yes, it is okay to skip a line to increase read-ability! </p> <pre><code># Get only the directory name\n</code></pre> </li> <li> <p>Next have your script create another variable called <code>dirName</code>. Use this variable to store the directory name extracting it from <code>dirPath</code>. Make use of the <code>$</code> to retrieve the value stored inside the variable! This will be line 5. </p> <p>The next few tasks will require simple commands for changing directories and listing contents of the <code>raw_fastq</code> directory.</p> </li> <li> <p>Copy and paste the line below to line 7. </p> <pre><code>echo \"Reporting on the directory\" $dirName \"...\"\n</code></pre> </li> <li> <p>Write a command to change directories into <code>dirPath</code>. This will be line 8. </p> </li> <li> <p>Copy and paste the lines below to line 10-11. Modify to make sure the sizes listed are human-readable. </p> <pre><code>echo \"These are the contents of\" $dirName \nls -l\n</code></pre> </li> <li> <p>Copy and paste the lines below starting at line 13. </p> <pre><code>echo \"The total number of files contained in\" $dirName\nls | wc -l\n\necho \"Report complete!\"\n</code></pre> </li> <li> <p>You are all set with script! Take a Screenshot of your final script before hitting Save. Submit this screenshot with this week's homework. </p> </li> <li> <p>Run the script <code>directory_info.sh</code>. Take a screenshot of the results. Submit this screenshot with this week's homework. </p> </li> </ol>"},{"location":"ch03/05_week3_mmg3320/#summary","title":"Summary","text":"<p>In today's lesson, we described shell scripts and introduced a few related concepts that are helpful when you are starting out. It is important to understand each of the indvidual concepts, but also to see how they all come together to add flexibility and efficency to your script. Later on we will further illustrate the power of scripts and how they can make our lives (when coding) much easier. Any type of data you will want to analyze will inevitably involve not just one step, but many steps and perhaps many different tools/software programs. Compiling these into a shell script is the first step in creating your analysis workflow!</p>"},{"location":"ch03/05_week3_mmg3320/#citation","title":"Citation","text":"<p>This lesson has been developed by members of the teaching team at the Harvard Chan Bioinformatics Core (HBC). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p> <ul> <li>The materials used in this lesson were derived from work that is Copyright \u00a9 Data Carpentry (http://datacarpentry.org/).  All Data Carpentry instructional material is made available under the Creative Commons Attribution license (CC BY 4.0).</li> <li> <p>Adapted from the lesson by Tracy Teal. Original contributors: Paul Wilson, Milad Fatenejad, Sasha Wood and Radhika Khetani for Software Carpentry (http://software-carpentry.org/)</p> </li> <li> <p>Other Authors: Meeta Mistry, Bob Freeman, Mary Piper, Radhika Khetani, Jihe Liu, Will Gammerdinger</p> </li> </ul>"},{"location":"ch04/","title":"Overview","text":""},{"location":"ch04/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this week, students will be able to:  </p> <ul> <li>Describe the concept of 'looping' to iterate commands over multiple items</li> <li>Automate a task by using a loop inside of a shell script</li> <li>Understand applications of RNA sequencing</li> <li>Introduce the overall differential expression workflow</li> <li>Understand experimental design concepts such as replicates and batch effects </li> <li>Understand different types of library preps, their requirements and uses. </li> <li>Understand the types of data that is accessible from Gene Expression Omnibus (GEO)</li> <li>Use the command-line interface to copy over data from GEO</li> <li>How to submit a job using SLURM</li> </ul>"},{"location":"ch04/06_week4_mmg3320/","title":"Loops (L6)","text":""},{"location":"ch04/06_week4_mmg3320/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Describe the concept of 'looping' to iterate commands over multiple items</li> <li>Automate a task by using a loop inside of a shell script</li> </ul>"},{"location":"ch04/06_week4_mmg3320/#recap","title":"Recap","text":"<ul> <li>You can save commands in files (usually called shell scripts) for re-use.</li> <li><code>sh</code> [filename] will run the command saved within the shell script</li> <li>shell script end in <code>.sh</code></li> <li>You should place variables in backticks (`) if the values might have spaces in them</li> </ul> <p>Class Exercise</p> <p>Before moving on, please complete the following class activity below. You will have ~5 minutes to answer both questions.</p> <p>Class-activity</p>"},{"location":"ch04/06_week4_mmg3320/#loops","title":"Loops","text":"<p>Typically, when you are running analyses on the cluster, you are running multiple commands which correspond to individual steps in your workflow. We learned earlier that we can compile these commands into a single shell script to make this process more efficient. What if we could further increase our efficiency so that the same series of commands could be easily repeated for each sample in our dataset? We can do this with the use of loops in Shell!</p> <p>Looping is a concept shared by several programming languages, and its implementation in bash is very similar to other languages. </p> <p>The structure or the syntax of (for) loops in bash is as follows:</p> <pre><code>for (variable_name) in (list)\ndo\n (command1 $variable_name)\n (command2 $variable_name)\ndone\n</code></pre> <p>The text that is bold, are parts of the loop structure that remain constant. That is, for every loop your create you will need to have the words: <code>for</code>, <code>in</code>, <code>do</code> and <code>done</code>. This syntax/structure is virtually set in stone. The text that goes in between those words will change depending on what it is you want your loop to do.</p>"},{"location":"ch04/06_week4_mmg3320/#how-do-loops-work","title":"How do loops work?","text":"<p>Let's use the example below to go through step-by-step how a loop is actually working.</p> <p>Together we will create a script called <code>loop.sh</code> in Nano: </p> <pre><code>cd raw_fastq/\n\nfor x in Mov10_oe_1.subset.fq Mov10_oe_2.subset.fq Mov10_oe_3.subset.fq\ndo\n echo $x\n wc -l $x\ndone\n</code></pre> <pre><code>Mov10_oe_1.subset.fq\n 1223600 Mov10_oe_1.subset.fq\nMov10_oe_2.subset.fq\n 1110016 Mov10_oe_2.subset.fq\nMov10_oe_3.subset.fq\n  690816 Mov10_oe_3.subset.fq\n</code></pre> Loop component Value variable_name <code>x</code> list <code>Mov10_oe</code> FASTQ files body (commands to be executed) <code>echo</code> and <code>wc -l</code>"},{"location":"ch04/06_week4_mmg3320/#loopsh-explained","title":"Loop.sh explained","text":"<ol> <li> <p>When we start the loop, the temporary variable is initialized by taking the value of the first item in the list. </p> <p>We don't explicitly see this, but the variable has been defined as <code>x=Mov10_oe_1.subset.fq</code>.</p> </li> <li> <p>Next, all of the commands in the body of the loop (between the <code>do</code> and <code>done</code>) are executed. Usually, the commands placed here will be using the temporary variable as input. Remember, if you are using the value stored in the variable you need to use $ to reference it! In the example, we are running two commands:</p> <ul> <li><code>echo $x</code>: print out the value stored in <code>x</code></li> <li><code>wc -l $x</code>: count/report the number of lines in <code>x</code></li> </ul> </li> <li> <p>Once those two commands are complete, the temporary variable is assigned a new value. It now takes the value of the second item in the list.</p> <p>The variable is reassigned a value <code>x=Mov10_oe_2.subset.fq</code>.</p> </li> <li> <p>Once again, all of the commands in between the <code>do</code> and <code>done</code> are executed. This time they are using the new value stored in <code>x</code> as input.</p> </li> <li> <p>The temporary variable then takes on the value of the third item in the list.</p> <p>The variable is reassigned a value <code>x=Mov10_oe_3.subset.fq</code>.</p> </li> <li> <p>Once again, all of the commands in between the <code>do</code> and <code>done</code> are executed using the new value stored in <code>x</code>. </p> </li> <li> <p>Now that we have gone through every item in the list, the loop is <code>done</code> and it exits. </p> </li> </ol> <p>Essentially, the number of items in the list = number of times the code will loop through. So in our case, we had three files listed and so the series of commands in the body of the loop were repeated three times. If we had provided all six files, the series of commands would be repeated six times.</p>"},{"location":"ch04/06_week4_mmg3320/#running-loops-at-the-command-prompt","title":"Running loops at the command prompt","text":"<p>In our materials, the for loop is written out using multiple lines rather than the single line commands we have been running so far. When running this at the command prompt begin by typing out the <code>for</code> statement, then press the return key. You will notice that you are not back at your command prompt. Rather than a <code>$</code>, you should see a <code>&gt;</code>. The shell has acknowledged that you have started a for loop and is waiting for you to complete it. Continue to type code line by line. Once you type in <code>done</code> and press return the shell will know you are done and will run the loop. </p>"},{"location":"ch04/06_week4_mmg3320/#creating-loops-using-best-practices","title":"Creating loops using best practices","text":""},{"location":"ch04/06_week4_mmg3320/#meaningful-variable-names","title":"Meaningful variable names","text":"<p>It doesn't matter what variable name we use, but it is advisable to make it something more intuitive. In the long run, it's best to use a name that will help point out a variable's functionality, so your future self will understand what you are thinking now.</p>"},{"location":"ch04/06_week4_mmg3320/#using-the-wildcard-to-define-the-list","title":"Using the wildcard to define the list","text":"<p>In the example above, we typed out each item in the list leaving a space in between each item. This is usually fine for one or two items, but with larger lists this can become tedious and error-prone. If the list you are iterating over share some similarities in the naming we recommend using the wildcard shortcut to specify the list. </p> <p>Class Exercise</p> <p>Change the for loop above so that:</p> <pre><code>1) `x` variable is given a more meaningful name\n2) the script runs on all six FASTQ files \n3) and it prints out the first two lines of all six files\n</code></pre>"},{"location":"ch04/06_week4_mmg3320/#automating-more-with-scripts","title":"Automating more with Scripts","text":"<p>Imagine, if you will, a script that would do the following for us each time we get a new data set:</p> <ul> <li>Use for loop to iterate over each FASTQ file</li> <li>Generate a prefix to use for naming our output files</li> <li>Dump out bad reads into a new file</li> <li>Get the count of the number of bad reads and report it to a running log file</li> </ul> <p>It might seem daunting, but everything outlined above is something that you know how to do. Let's get started...</p> <p>Class Exercise and Homework Assignment #4</p> <ol> <li> <p>Create a directory called <code>badreads</code> in <code>unit1_unix</code></p> </li> <li> <p>Use Jupyter Notebooks/Nano to create a new script called <code>generate_bad_reads_summary.sh</code> in <code>badreads/</code></p> </li> <li> <p>At the beginning of your script add a shebang line. </p> <pre><code>#!/bin/bash\n</code></pre> <p>This line is the absolute path to the Bash interpreter. The shebang line ensures that the bash shell interprets the script even if it is executed using a different shell.</p> </li> <li> <p>After the shebang line, skip a line and copy-and-paste the following in Line 3: </p> <pre><code># enter directory with raw FASTQs\n</code></pre> </li> <li> <p>In line 4 write a command to change directories into the <code>raw_fastq</code> directory. </p> </li> <li> <p>Add the following comment as Line 6. </p> <pre><code># loop over each FASTQ file\n</code></pre> </li> <li> <p>Now you are ready to begin writing the for loop. To create the first line of your for loop use the following: </p> Loop component Value variable_name <code>filename</code> list all FASTQ files </li> <li> <p>Type <code>do</code> in Line 8. </p> </li> <li> <p>Skip a line. On line 10 copy-and-paste the following comment. Yes it is okay to skip a line! </p> <pre><code># create a prefix for all output files\n</code></pre> </li> <li> <p>Now you are ready to move on to create a prefix for all (6) fastq files. These prefixes will be stored in a second variable called <code>samplename</code> in line 11. To write this line of code successfully, remember the following:</p> <pre><code>variable_name=value_of_variable\n</code></pre> <ul> <li>The value_of_variable should be equal to the <code>basename</code> of <code>filename</code>. </li> <li>Be sure to trim off the file extension <code>.subset.fq</code></li> </ul> <p>Why are we doing this? Storing the prefixes in <code>samplename</code> will allow us to uniquely label our output files later on! </p> </li> <li> <p>Copy-and-paste the following into lines 12 and 13. The <code>echo</code> statement will keep the user informed on which file is being processed in real-time. </p> <pre><code># tell us what file we're working on\necho $filename\n</code></pre> </li> <li> <p>Complete the command below to extract and save all \"bad reads\" into an output file. A read is considered \"bad\" if it contains 10 consecutive N's. Below, you are given the right side of the command, which specifies the output file location. Your task is to complete the left side of the command using <code>grep</code>. These will be lines 15 and 16 of your script.</p> <p></p><pre><code># Extract all bad read records and save them to a new file  \nWRITE-THE-COMMAND-HERE &gt; ~/unit1_unix/badreads/${samplename}_badreads.fq  \n</code></pre> + Ensure that all four lines of each matching sequence read are included in the output.  </li> <li> <p>You are almost finished! Copy and paste the lines below in lines 18-20. </p> <pre><code># grab the number of bad reads and write it to a summary file\ngrep -cH NNNNNNNNNN $filename &gt;&gt; ~/unit1_unix/badreads/badreads.count.summary\ndone\n</code></pre> </li> <li> <p>Save and exit, and voila! You now have a script you can use to assess the quality of all your new datasets. </p> </li> </ol> <p>To run this script simply enter the following command:</p> <pre><code>sh generate_bad_reads_summary.sh\n</code></pre> <p>This lesson has been developed by members of the teaching team at the Harvard Chan Bioinformatics Core (HBC). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p> <ul> <li>The materials used in this lesson were derived from work that is Copyright \u00a9 Data Carpentry (http://datacarpentry.org/).  All Data Carpentry instructional material is made available under the Creative Commons Attribution license (CC BY 4.0).</li> <li>Adapted from the lesson by Tracy Teal. Original contributors: Paul Wilson, Milad Fatenejad, Sasha Wood and Radhika Khetani for Software Carpentry (http://software-carpentry.org/)</li> </ul>"},{"location":"ch04/06_week4_mmg3320/#why-do-you-need-a-shebang-line","title":"Why do you need a shebang line?","text":"<p>Having a shebang line is best practice. While your script will run fine without it in environments where bash is the default shell, it won't if the user of this script is using a different shell. To avoid any issues, we explicitly state that this script needs to executed using the bash shell.</p>"},{"location":"ch04/06_week4_mmg3320/#explanation-of-command-above","title":"Explanation of command above","text":"<p>You are using <code>grep</code> to find all the bad reads (in this case, bad reads are defined as those with 10 consecutive N's), and then extracting the four lines associated with each sequence read and writing them to a file. The output file is named using the <code>samplename</code> variable you created earlier in the loop. You will also notice we are adding a path to redirect the output into the <code>badreads</code> directory.</p>"},{"location":"ch04/06_week4_mmg3320/#why-are-we-using-curly-brackets-with-the-variable-name","title":"Why are we using curly brackets with the variable name?","text":"<p>When we append a variable to some other free text, we need shell to know where our variable name ends. By encapsulating the variable name in curly brackets we are letting shell know that everything inside it is the variable name. This way when we reference it, shell knows to print the variable <code>$base</code> and not to look for a variable called <code>$base_badreads.fq</code>.</p>"},{"location":"ch04/06_week4_mmg3320/#explanation-of-command-above_1","title":"Explanation of command above","text":"<p>Above, you are counting the number of identified bad reads using the count flag of <code>grep</code>, <code>-c</code>, which will return the number of matches rather than the actual matching lines. Here, you are using the <code>-H</code> flag; this will report the filename along with the count value. This is useful because you are writing this information to a running summary file. So rather than just reporting a count value you will also know which file it is associated with. You then closed the loop with <code>done</code>. </p>"},{"location":"ch04/06_week4_mmg3320/#how-do-we-know-if-your-script-worked","title":"How do we know if your script worked?","text":"<p>Take a look inside the <code>badreads</code> directory. You should see that for every one of the original FASTQ files, one bad read file was created. You should also have a summary file documenting the total number of bad reads from each file.</p> <pre><code>badreads.count.summary  Irrel_kd_2_badreads.fq  Mov10_oe_1_badreads.fq  Mov10_oe_3_badreads.fq\nIrrel_kd_1_badreads.fq  Irrel_kd_3_badreads.fq  Mov10_oe_2_badreads.fq\n</code></pre>"},{"location":"ch04/07_week4_intro-to-RNAseq/","title":"RNA-Seq Overview (L7)","text":""},{"location":"ch04/07_week4_intro-to-RNAseq/#learning-objectives","title":"Learning Objectives:","text":"<ul> <li>Understand applications of RNA sequencing</li> <li>Introduce the overall differential expression workflow</li> <li>Understand experimental design concepts such as replicates and batch effects </li> <li>Understand different types of library preps, their requirements and uses. </li> </ul>"},{"location":"ch04/07_week4_intro-to-RNAseq/#overview-of-rna-seq","title":"Overview of RNA-seq","text":"<p>RNA-seq is an exciting experimental technique that is utilized to explore and/or quantify gene expression within or between conditions. </p> <p>As we know, genes provide instructions to make proteins, which perform some function within the cell. Although all cells contain the same DNA sequence, muscle cells are different from nerve cells and other types of cells because of the different genes that are turned on in these cells and the different RNAs and proteins produced. </p> <p>Different biological processes, as well as mutations, can affect which genes are turned on and which are turned off, in addition to, how much specific genes are turned on/off.</p> <p>To make proteins, the DNA is transcribed into messenger RNA, or mRNA, which is translated by the ribosome into protein. However, some genes encode RNA that does not get translated into protein; these RNAs are called non-coding RNAs, or ncRNAs. Often these RNAs have a function in and of themselves and include rRNAs, tRNAs, and siRNAs, among others. All RNAs transcribed from genes are called transcripts.</p> <p>To be translated into proteins, the RNA must undergo processing to generate the mRNA. In the figure below, the top strand in the image represents a gene in the DNA, comprised of the untranslated regions (UTRs) and the open read frame. Genes are transcribed into pre-mRNA, which still contains the intronic sequences.  After post-transciptional processing, a 5' cap and polyA tail are added and the introns are spliced out to yield mature mRNA transcripts, which can be translated into proteins.</p> <p>While mRNA transcripts have a polyA tail, many of the non-coding RNA transcripts do not.</p>"},{"location":"ch04/07_week4_intro-to-RNAseq/#transcriptomics","title":"Transcriptomics","text":"<p>The transcriptome is defined as a collection of all the transcript readouts present in a cell. RNA-seq data can be used to explore and/or quantify the transcriptome of an organism, which can be utilized for the following types of experiments:</p> <ul> <li>Differential Gene Expression: quantitative evaluation and comparison of transcript levels between conditions</li> <li>Transcriptome assembly: building the profile of transcribed regions of the genome, a qualitative evaluation</li> <li>Refinement of gene models: building better gene models and verifying them using transcriptome assembly</li> <li>Metatranscriptomics: community transcriptome analysis</li> </ul>"},{"location":"ch04/07_week4_intro-to-RNAseq/#illumina-sequencing","title":"Illumina Sequencing","text":""},{"location":"ch04/07_week4_intro-to-RNAseq/#illumina-library-preparation","title":"Illumina Library preparation","text":"<p>The general workflow for library preparation is detailed in the step-by-step images below.</p> <p>Briefly, the RNA is isolated from the sample and contaminating DNA is removed with DNase.</p> <p>The RNA sample then undergoes either selection of the mRNA (polyA selection) or depletion of the rRNA. The resulting RNA is fragmented. </p> <p>Generally, ribosomal RNA represents the majority of the RNAs present in a cell, while messenger RNAs represent a small percentage of total RNA, ~2% in humans. Therefore, if we want to study the protein-coding genes, we need to enrich for mRNA or deplete the rRNA. </p> <p>The size of the target fragments in the final library is a key parameter for library construction. DNA fragmentation is typically done by physical methods (i.e., acoustic shearing and sonication) or enzymatic methods (i.e., non-specific endonuclease cocktails and transposase tagmentation reactions.</p> <p>The RNA is then reverse transcribed into double-stranded cDNA and sequence adapters are then added to the ends of the fragments.</p> <p>The cDNA libraries can be generated in a way to retain information about which strand of DNA the RNA was transcribed from. Libraries that retain this information are called stranded libraries, which are now standard with Illumina\u2019s TruSeq stranded RNA-Seq kits. Stranded libraries should not be any more expensive than unstranded, so there is not really any reason not to acquire this additional information. </p> <p>There are 3 types of cDNA libraries available:</p> <ul> <li>Forward (secondstrand) \u2013 reads resemble the gene sequence or the secondstrand cDNA sequence</li> <li>Reverse (firststrand) \u2013 reads resemble the complement of the gene sequence or firststrand cDNA sequence (TruSeq)</li> <li>Unstranded</li> </ul> <p>Finally, the fragments are PCR amplified if needed, and the fragments are size selected (usually ~300-500bp) to finish the library.</p> <p>Image credit: Martin J.A. and Wang Z., Nat. Rev. Genet. (2011) 12:671\u2013682</p>"},{"location":"ch04/07_week4_intro-to-RNAseq/#strandedness","title":"Strandedness","text":"<p>The implication of stranded libraries is that one could distinguish whether the reads are derived from the forward or reverse-encoded transcripts. </p> <ul> <li>Red = positive strand </li> <li>Blue = negative strand </li> </ul>"},{"location":"ch04/07_week4_intro-to-RNAseq/#single-end-versus-paired-end","title":"Single-end versus Paired-end","text":"<p>After preparation of the libraries, sequencing can be performed to generate the nucleotide sequences of the ends of the fragments, which are called reads. You will have the choice of sequencing a single end of the cDNA fragments (single-end reads) or both ends of the fragments (paired-end reads).</p> <ul> <li>SE - Single end dataset =&gt; Only Read1</li> <li>PE - Paired-end dataset =&gt; Read1 + Read2<ul> <li>often are 2 separate FastQ files! </li> </ul> </li> </ul> <p>Generally single-end sequencing is sufficient unless it is expected that the reads will match multiple locations on the genome (e.g. organisms with many paralogous genes), assemblies are being performed, or for splice isoform differentiation. </p>"},{"location":"ch04/07_week4_intro-to-RNAseq/#different-sequencing-platforms","title":"Different sequencing platforms","text":"<p>There are a variety of Illumina platforms to choose from to sequence the cDNA libraries.</p> <p>Image credit: Adapted from Illumina</p> <p>Differences in platform can alter the length of reads generated, the quality of reads, as well as the total number of reads sequenced per run and the amount of time required to sequence the libraries. The different platforms each use a different flow cell, which is a glass surface coated with an arrangement of paired oligos that are complementary to the adapters added to your template molecules. The flow cell is where the sequencing reactions take place.</p> <p>Image credit: Adapted from Illumina</p>"},{"location":"ch04/07_week4_intro-to-RNAseq/#multiplexing","title":"Multiplexing","text":"<p>Depending on the Illumina platform (MiSeq, HiSeq, NextSeq), the number of lanes per flow cell, and the number of reads that can be obtained per lane varies widely. The researcher will need to decide on how many reads they would like per sample (i.e. the sequencning depth) and then based on the platform you choose calculate how many total lanes you will require for your set of samples. </p> <p>Typically, charges for sequencing are per lane of the flow cell and you will be able to run multiple samples per lane. Illumina has therefore devised a nice multiplexing method which allows libraries from several samples to be pooled and sequenced simultaneously in the same lane of a flow cell. This method requires the addition of indices (within the Illumina adapter) or special barcodes (outside the Illumina adapter) as described in the schematic below.</p> <ul> <li> <p>General gene-level differential expression:</p> </li> <li> <p>ENCODE guidelines suggest 30 million SE reads per sample (stranded).</p> </li> <li> <p>15 million reads per sample is often sufficient, if there are a good number of replicates (&gt;3). </p> </li> <li> <p>Use of an HiSeq or NextSeq, or NovaSeq for sequencing </p> </li> </ul>"},{"location":"ch04/07_week4_intro-to-RNAseq/#differential-gene-expression","title":"Differential gene expression","text":"<p>Differential gene expression analysis allows us to explore the gene expression changes that occur in disease or between different conditions, by measuring the quantity of RNA expressed by all genes in each of the different conditions. </p> <p>Using this analysis we can answer questions such as:</p> <ul> <li> <p>What genes are differentially expressed between conditions?</p> </li> <li> <p>Are there any trends in gene expression over time or across conditions?</p> </li> </ul> <p>Citation: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0138236 </p> <ul> <li> <p>Which groups of genes change similarly over time or across conditions?</p> </li> <li> <p>What processes or pathways are important for my condition of interest?</p> </li> </ul> <p>Citation: https://elifesciences.org/articles/63003 </p> <p>To perform differential gene expression analysis, we perform the following steps:</p>"},{"location":"ch04/07_week4_intro-to-RNAseq/#experimental-design","title":"Experimental Design","text":"<p>Understanding the steps in the experimental process of RNA extraction and preparation of RNA-Seq libraries is helpful for designing an RNA-Seq experiment and important to consider when selecting a dataset to analyze. There are special considerations that should be highlighted which can greatly affect the quality of a differential expression analysis. </p> <p>These important considerations include:</p> <ol> <li>Number and type of replicates</li> <li>Avoiding confounding variables </li> <li>Addressing batch effects</li> </ol>"},{"location":"ch04/07_week4_intro-to-RNAseq/#replicates","title":"Replicates","text":"<p>Experimental replicates can be performed as technical replicates or biological replicates. </p> <p>Image credit: Klaus B., EMBO J (2015) 34: 2727-2730</p> <ul> <li> <p>Technical replicates: use the same biological sample to repeat the technical or experimental steps in order to accurately measure technical variation and remove it during analysis. </p> </li> <li> <p>Biological replicates use different biological samples of the same condition to measure the biological variation between samples. </p> </li> </ul> <p>Biological replicates are absolutely essential for differential expression analysis. In fact, the more biological replicates, the better for estimates of biological variation and the more precise our estimates of the mean expression levels. This leads to more accurate modeling of our data and identification of more differentially expressed genes.</p> <p>Image credit: Liu, Y., et al., Bioinformatics (2014) 30(3): 301\u2013304</p> <p>As the figure above illustrates, biological replicates are of greater importance than sequencing depth. The figure shows the relationship between sequencing depth and number of replicates on the number of differentially expressed genes identified [1]. Note that an increase in the number of replicates tends to return more DE genes than increasing the sequencing depth. Therefore, generally more replicates are better than higher sequencing depth, with the caveat that higher depth is required for detection of lowly expressed DE genes and for performing isoform-level differential expression. </p> <p>Replicates are almost always preferred to greater sequencing depth for bulk RNA-Seq. </p>"},{"location":"ch04/07_week4_intro-to-RNAseq/#confounding","title":"Confounding","text":"<p>A confounded RNA-Seq experiment is one where you cannot distinguish the separate effects of two different sources of variation in the data. </p> <p>For example, we know that sex has large effects on gene expression, and if all of our control mice were female and all of the treatment mice were male, then our treatment effect would be confounded by sex. We could not differentiate the effect of treatment from the effect of sex.</p> <p>To AVOID confounding:</p> <ul> <li> <p>Ensure animals in each condition are all the same sex, age, litter, and batch, if possible.</p> </li> <li> <p>If not possible, then ensure to split the animals equally between conditions</p> </li> </ul>"},{"location":"ch04/07_week4_intro-to-RNAseq/#batch-effects","title":"Batch effects","text":"<p>Batch effects are a significant issue for RNA-Seq analyses, since you can see significant differences in expression due solely to the batch effect.</p> <p>Image credit: Hicks SC, et al., bioRxiv (2015)</p>"},{"location":"ch04/07_week4_intro-to-RNAseq/#how-to-know-whether-you-have-batches","title":"How to know whether you have batches?","text":"<ul> <li> <p>Were all RNA isolations performed on the same day and with the same kit?</p> </li> <li> <p>Were all library preparations performed on the same day?</p> </li> <li> <p>Did the same person perform the RNA isolation/library preparation for all samples?</p> </li> <li> <p>Did you use the same reagents for all samples?</p> </li> <li> <p>Did you perform the RNA isolation/library preparation in the same location?</p> </li> </ul> <p>If any of the answers is \u2018No\u2019, then you have batches.</p>"},{"location":"ch04/07_week4_intro-to-RNAseq/#best-practices-regarding-batches","title":"Best practices regarding batches:","text":"<ul> <li> <p>Design the experiment in a way to avoid batches, if possible.</p> </li> <li> <p>If unable to avoid batches:</p> </li> <li> <p>Do NOT confound your experiment by batch:</p> <p> </p> <p>Image credit: Hicks SC, et al., bioRxiv (2015)</p> </li> <li> <p>DO split replicates of the different sample groups across batches. The more replicates the better!</p> <p> </p> <p>Image credit: Hicks SC, et al., bioRxiv (2015)</p> </li> <li> <p>DO include batch information in your experimental metadata. During the analysis, we can regress out the variation due to batch so it doesn\u2019t affect our results if we have that information.</p> <p> </p> </li> </ul>"},{"location":"ch04/07_week4_intro-to-RNAseq/#citation","title":"Citation","text":"<p>This lesson has been developed by members of the teaching team at the Harvard Chan Bioinformatics Core (HBC). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Authors: Mary Piper, Meeta Mistry, Radhika Khetani Other sources - https://umich-brcf-bioinf.github.io/rnaseq_demystified_workshop/site/Module3a_Design_Prep_Seq#2_Experimental_Design_and_Practicalities </p>"},{"location":"ch04/08_accessing_public_experimental_data/","title":"Accessing Public Data (L8)","text":""},{"location":"ch04/08_accessing_public_experimental_data/#learning-objectives","title":"Learning Objectives","text":"<p>We spent the last few weeks introducing UNIX, navigating the file system, and working on a high performance cluster. Now we will proceed with:</p> <ul> <li>Understand the types of data that are accessible from Gene Expression Omnibus (GEO)</li> <li>Learning how to use SRA-toolkit to retrieve data from the Sequence Reads Archive <ul> <li>Download data from the SRA with <code>fastq-dump</code></li> <li>split files into forward and reverse reads </li> <li>Download part, not all, the data </li> </ul> </li> </ul>"},{"location":"ch04/08_accessing_public_experimental_data/#where-are-we-heading","title":"Where are we heading?","text":""},{"location":"ch04/08_accessing_public_experimental_data/#sequence-file-formats","title":"Sequence file formats","text":"<p>Below is a cartoon displaying the (3) file types required to perform an RNA-Seq analysis. </p> <ul> <li>FASTQ files will contain the raw sequence reads</li> <li>The reference genome will be in the form of a FASTA file</li> <li>Gene annotations will be in the form of a GTF file </li> </ul>"},{"location":"ch04/08_accessing_public_experimental_data/#fasta","title":"FASTA","text":"<p>During an NGS experiment, the nucleotide sequences stored inside the raw FASTQ files, or \"sequence reads\", need to be mapped or aligned to the reference genome to determine from where these sequences originated. Therefore, we need a reference genome (in FASTA format) in which to align our sequences.</p>"},{"location":"ch04/08_accessing_public_experimental_data/#gtf","title":"GTF","text":"<p>In addition, many NGS methods require knowing where known genes or exons are located on the genome in order to quantify the number of reads aligning to different genome features, such as exons, introns, transcription start sites, etc. These analyses require reference data containing specific information about genomic coordinates of various genomic \u201cfeatures\u201d, such as gene annotation files (in GTF, GFF, etc.). </p>"},{"location":"ch04/08_accessing_public_experimental_data/#fastq","title":"FASTQ","text":"<p>These are the extension of FASTA files which contain quality scores and are output from the NGS technologies. </p>"},{"location":"ch04/08_accessing_public_experimental_data/#downloading-file-formats","title":"Downloading file formats","text":"<p>To find and download NGS experimental data and associated reference data we will explore a few key repositories. For finding reference data, we will navigate the Ensembl database. For accessing experimental data, we will explore the Gene Expression Omnibus and the Sequence Read Archive repositories. </p> <ul> <li>General biological databases: Ensembl, NCBI, and UCSC</li> <li>Organism-specific biological databases: Wormbase, Flybase, Cryptodb, etc. (often updated more frequently, so may be more comprehensive)</li> </ul> <p>*Note that these reference data sources are relevant to most types of genomic analyses not just NGS analyses.</p>"},{"location":"ch04/08_accessing_public_experimental_data/#general-biological-databases","title":"General biological databases","text":"<p>Biological databases for gene expression data store genome assemblies and provide annotations regarding where the genes, transcripts, and other genomic features are located on the genome. </p> <p>Genome assemblies give us the nucleotide sequence of the reference genome. Although the Human Genome Project was \"completed\" in 2003, small gaps in the sequence remained (estimated 1% of gene-containing portions). As technology improves and more genomes are sequenced, these gaps are filled, mistakes are corrected and alternate alleles are provided. Therefore, every several years a new genome build is released that contains these improvements. </p> <p>The current genome build is GRCh38/hg38 for the human, which was released in 2013 and is maintained by the Genome Reference Consortium (GRC). </p> <p>Usually the biological databases will include the updated versions as soon as they are stably released, in addition to access to archived versions.</p> <p>Genome databases incorporate these genomes and generate the gene annotations with the following similarities/differences:</p> <ul> <li> <p>Ensembl, NCBI, and UCSC all use the same genome assemblies or builds provided by the GRC</p> <ul> <li>GRCh38 = hg38; GRCh37 = hg19</li> </ul> </li> <li> <p>Each biological database independently determines the gene annotations; therefore, gene annotations between these databases can differ, even though the genome assembly is the same. Naming conventions are also different (chr1=1) between databases.</p> </li> <li> <p>Always use the same biological database for all reference data!</p> </li> </ul>"},{"location":"ch04/08_accessing_public_experimental_data/#ensembl","title":"Ensembl","text":"<p>Ensembl provides a website that acts as a single point of access to annotated genomes for vertebrate species. For all other organisms there are additional Ensembl databases available through Ensembl Genomes; however, they do not include viruses (NCBI does).</p> <ul> <li> <p>Genome assemblies/builds (reference genomes)</p> <ul> <li>New genome builds are released every several years or more depending on the species</li> <li>Genome assemblies are updated every two years to include patches, or less often depending on the species</li> </ul> </li> <li> <p>Gene annotations</p> <ul> <li>Gene annotations are created or updated using a variety of sources (ENA, UniProtKB, NCBI RefSeq, RFAM, miRBase, and tRNAscan-SE databases)</li> <li>Automatic annotation is performed for all species using identified proteins and transcripts</li> <li>Manual curation by the HAVANA group is performed for human, mouse, zebrafish, and rat species, providing better confidence of transcript annotations</li> <li>Directly imports annotations from FlyBase, WormBase and SGD</li> </ul> </li> </ul>"},{"location":"ch04/08_accessing_public_experimental_data/#using-the-ensembl-genomic-database-and-genome-browser","title":"Using the Ensembl genomic database and genome browser","text":"<p>Navigate to the Ensembl website to view the interface. The homepage for Ensembl has a lot to offer, with the a lot of information and access to a range of functionality and tools.</p> <ul> <li> <p>Searching Ensembl:  Look for a gene, location, variant and more using the search box on the homepage or the box that is provided in the top right corner of any Ensembl page.</p> <ul> <li>a gene name (for example, BRCA2) - best to use the official gene symbols (HGNC)</li> <li>a UniProt accession number (for example, P51587)</li> <li>a disease name (for example, coronary heart disease)</li> <li>a variation (for example, rs1223)</li> <li>a location - a genomic region (for example, rat X:100000..200000)</li> <li>a PDB ID or a Gene Ontology (GO) term</li> </ul> <p>Most search results will take you to the appropriate Ensembl view through a results page. These linked pages will allow you to download information/sequences for specific genes/transcripts/exons/variants. If you search using a location you will be directed straight to the location tab (this tab provides a view of a region of a genome). </p> </li> <li> <p>Ensembl identifiers: When using Ensembl, note that it uses the following format for biological identifiers:</p> <ul> <li>ENSG###########:  Ensembl Gene ID</li> <li>ENST###########:  Ensembl Transcript ID</li> <li>ENSP###########:  Ensembl Peptide ID</li> <li>ENSE###########:  Ensembl Exon ID</li> </ul> <p>For non-human species a suffix is added:</p> <ul> <li>ENSMUSG###: MUS (Mus musculus) for mouse </li> <li>ENSDARG###: DAR (Danio rerio) for zebrafish</li> </ul> </li> <li> <p>Downloading reference data from Ensembl: Go to Downloads, then click FTP Download on the left side bar. </p> </li> </ul> <p>Class Exercise</p> <p>Amanda is an graduate student studying optimal breeding practices for cattle. They are interested in investigating transcriptional differences in cattle raised in tropical versus temperate conditions. To do this, Amanda needs to download the Bos taurus FASTA file to set up their pipeline on the VACC. Amanda comes to you for help. How would you download the Bos taurus FASTA file from Ensembl to be used on the VACC?</p>"},{"location":"ch04/08_accessing_public_experimental_data/#gene-expression-omnibus-geo","title":"Gene Expression Omnibus (GEO)","text":"<p>GEO is a database for curated functional genomics data, including gene expression datasets from microarrays, RNA-Seq, and other transcriptomic studies. It stores processed and analyzed data, such as gene expression matrices and differential expression results. This database provides access to data for tens of thousands of studies as it is a requirement for publication. For datasets containing sequencing data, GEO often links to the Sequence Read Archive (SRA) (also maintained by NCBI). Users can access the SRA database to download raw sequencing data files in the FASTQ format. </p> <p>To download FASTQ from GEO, you need the following: </p> <p>1) A list of accession numbers (SRRXXXXXX format) for the files to download using Run Selector 2) Knowledge of how to access and use <code>fastq-dump</code>  3) An understanding of how to submit a script using SLURM batch system </p>"},{"location":"ch04/08_accessing_public_experimental_data/#finding-geo-data-for-a-particular-publication","title":"Finding GEO data for a particular publication","text":"<p>The publication will provide the GEO accession number. Let's find the data associated with the paper, \"MOV10 and FRMP regulate AGO2 association with microRNA recognition elements\". First, we can navigate to the article.</p> <p>We can search for the term \"GEO\"; different papers have different requirements for where this information is located. In this article, it is available in a separate section called Accession Numbers.</p> <p>By clicking on the GEO accession number for the experiment of interest, the GEO page for this experiment will open.</p> <p>Please Note: Many paper have multiple GEO accession numbers. Each will correspond to a specific dataset </p> <p>The GEO page contains information about the experiment, including:</p> <ul> <li>an experimental summary: gives you an understanding of how the experiment was performed. </li> <li>literature citation</li> <li>contact information</li> <li>links to the Sample GEO pages: each sample will have its own page with additional information regarding how the sample was generated and analyzed </li> <li>link to the SRA project containing the raw FASTQ files</li> </ul> <p>In addition, if we were interested in downloading the raw counts matrix (<code>GSE50499_GEO_Ceman_counts.txt.gz</code>), which gives the number of reads/sequences aligning to each gene we could scroll down to supplementary data at the bottom of the page. </p>"},{"location":"ch04/08_accessing_public_experimental_data/#downloading-data-from-sra","title":"Downloading data from SRA","text":"<p>The Sequence Read Archive (SRA) is an archive for high throughput sequencing data, publicly accessible, for the purpose of enhancing reproducibility in the scientific community.</p> <p>There are four hierarchical levels of SRA entities and their accessions:  </p> <ol> <li>STUDY with accessions in the form of SRP, ERP, or DRP  </li> <li>SAMPLE with accessions in the form of SRS, ERS, or DRS  </li> <li>EXPERIMENT with accessions in the form of SRX, ERX, or DRX  </li> <li>RUN with accessions in the form of SRR, ERR, or DRR</li> </ol> <p>The minimum publishable unit in the SRA, is an EXPERIMENT (SRX)</p> <p>Since most studies include multiple samples and a high number of replicates, it is useful to know how to download all the sequencing runs from all samples in a study, without having to hunt down and type in individual SRR numbers one by one. Using the study accession number as previously shown, we can navigate to the summary page for the study. </p> <p>Towards the bottom of the page you will find a link for SRA under the heading Relations.</p> <p>Clicking on this link takes you to a page listing all the biological samples for the study, each with links to their specific sequencing runs and files. If we were only interested in one sample, we could follow the relevant link and find its associated sequencing runs. However, since we typically need files for multiple samples and their replicates, we will use Run Selector to obtain a comprehensive list. </p> <p>To do this, scrolls to the bottom of the page and click Send to, select the radio button for Run Selector, and then press Go.</p>"},{"location":"ch04/08_accessing_public_experimental_data/#run-selector","title":"Run selector","text":"<p>You'll notice that the run selector has aggregated all the information for the study samples, including a table of metadata at the top, giving information on: </p> <ul> <li>LibraryLayout - whether the reads were sequenced using single or paired end sequencing</li> <li>Platform - which sequencing technology was used </li> <li>Organism </li> <li>Instrument </li> <li>Cell type/ tissue type   ... and other useful information that should be noted for downstream analysis.</li> </ul> <p>Below this there is also a summary line detailing the total number of runs in the study. Let's pause here. Notice that every sample (ex. GSM1220262, GSM1220263, etc.) has two Run accession numbers associated with it. </p> <p>To fully understand what this means, we need to go back to the sample page for a sample. Notice that this sample was submitted for sequencing either twice or on two separate lanes.</p> <p>Therefore, for a single sample, there will be double the amount of sequencing files to process. </p> <p>It is on this page that we can download the Metadata and Accession List in text format. </p> <ul> <li>The Metadata is a very useful text summary of all metadata for all runs in the study</li> <li>The Accession List is a list of all the SRR accession numbers for the study. We will need this list to download the FASTQ files using the script below. </li> </ul>"},{"location":"ch04/08_accessing_public_experimental_data/#sra-toolkit","title":"SRA-Toolkit","text":"<p>The SRA Toolkit is a set of utilities developed by the National Center for Biotechnology Information (NCBI) for accessing data in the Sequence Read Archive (SRA), a database that stores raw sequencing data from various high-throughput sequencing platforms. The toolkit provides command-line tools for downloading, manipulating, and converting sequencing data stored in the SRA format, making it easier for researchers to work with large-scale genomic data. It's widely used in bioinformatics and genomics research for tasks such as sequence alignment, quality control, and data analysis.</p>"},{"location":"ch04/08_accessing_public_experimental_data/#fastq-dump","title":"<code>fastq-dump</code>","text":"<p><code>fastq-dump</code> is a command-line tool included in the SRA Toolkit developed by the National Center for Biotechnology Information (NCBI). It's used to extract data from the Sequence Read Archive (SRA) and convert it into the FASTQ format, which is a standard file format used to store biological sequences and their corresponding quality scores from high-throughput sequencing experiments.</p> <p>When you download sequencing data from the SRA using <code>fastq-dump</code>, it retrieves the raw sequencing reads along with quality information and saves them into one or more FASTQ files, making it easier for researchers to perform downstream analyses such as alignment, assembly, and variant calling. <code>fastq-dump</code> is a crucial tool in bioinformatics pipelines for processing sequencing data stored in the SRA.</p>"},{"location":"ch04/08_accessing_public_experimental_data/#using-fastq-dump-with-the-environmental-module-system","title":"Using <code>fastq-dump</code> with the Environmental Module System","text":"<p>We would like to run the program <code>fastq-dump</code> to download the fastq files. Let's type the following command: </p> <p></p><pre><code>fastq-dump --help\n</code></pre> If this does not work it means that this program <code>fastq-dump</code> is not available in your current environment. However, a great work-around to downloading and configuring programs yourself is to first check if they are available as library packages through the Environmental Module System found within the VACC.  <p>Environmental Modules provide a convenient way for VACC users to load and unload packages. These packages are maintained and updated by the VACC.  The following commands are necessary to work with modules: </p> Module commands description <code>module avail</code> List all available software modules <code>module load</code> Loads the named software module <code>module list</code> Lists all the currently loaded modules <code>module unload</code> Unload a specific module <code>module purge</code> Unload all loaded modules <code>module help</code> Displays general help/information about modules <p>Note:Before using software, we have to load the software. You will have to load the software every time you would like to use it. </p> <p>Let's begin: </p> <pre><code>module load gcc/13.3.0-xp3epyt\nmodule load sratoolkit/3.0.0-y2rspiu\n</code></pre> <p>Once a module for a tool is loaded, you have essentially made it directly available to you like any other basic shell command. We can check using the following command: </p> <pre><code>module list\n</code></pre> <pre><code>Currently Loaded Modules:\n  1) gmp/6.2.1-ip3t4a7    3) mpc/1.3.1-dv3gprk       5) zstd/1.5.6-apl64xw   7) sratoolkit/3.0.0-y2rspiu\n  2) mpfr/4.2.1-344sqki   4) zlib-ng/2.1.6-ibq6yfi   6) gcc/13.3.0-xp3epyt\n</code></pre> <p>The SRAToolKit contains the program called <code>fastq-dump</code>. We would like to use this program to download FASTQ files. </p> <pre><code>fastq-dump  --help\n</code></pre> <pre><code>Usage:\n  fastq-dump [options] &lt;path&gt; [&lt;path&gt;...]\n  fastq-dump [options] &lt;accession&gt;\n\nINPUT\n  -A|--accession &lt;accession&gt;       Replaces accession derived from &lt;path&gt; in \n                                   filename(s) and deflines (only for single \n                                   table dump) \n  --table &lt;table-name&gt;             Table name within cSRA object, default is \n                                   \"SEQUENCE\"\n</code></pre>"},{"location":"ch04/08_accessing_public_experimental_data/#submit-a-job-using-slurm","title":"Submit a job using SLURM","text":"<p>Unfortunately, SRA-toolkit doesn't have its own methods for downloading multiple SRR files at once in parallel. To download multiple SRR FASTQ files sometimes takes hours. Lucky for us, we have a script we can run. Lets discuss how to submit a script to be run using the SLURM batch system. </p> <p>Submitting a job to an HPC machine is done using a workload manager called SLURM (Simple Linux Utility for Resource Management). SLURM handles job scheduling, resource allocation (nodes, processors, memory, GPUs), and job monitoring. Jobs can be put in queue and then run as resources become available.</p> <p>The basic steps you will follow include: </p> <ol> <li>Log into VACC</li> <li>Write job script </li> <li>Submit job </li> <li>Monitor job and wait for it to run</li> <li>Retrieve your output </li> </ol> <p>A job script can be created using any text editor - such as Nano or Vim - or any GUI editor you may want to download. </p> <p>To use the SLURM job scheduler, it requires SLURM directives. </p>"},{"location":"ch04/08_accessing_public_experimental_data/#slurm-directives","title":"SLURM Directives","text":"<p>At the top of the job script will always be several lines that start with #SBATCH. The SLURM directives provide the job setup information used by SLURM, including resources to request. This information is then followed by the commands to be executed in the script. </p> <p>Create a script using nano: </p> <pre><code>nano test_job.sh\n</code></pre> <p>We need to have a shebang line at the beginning of the script to specify the file is a shell script. </p> <pre><code>#!/bin/sh\n</code></pre> <p>Next lets add the SLURM directives which must precede the executable section in your script.</p> <pre><code># Run on the general partition \n#SBATCH --partition=general\n\n# Request one node\n#SBATCH --nodes=1\n\n# Request one task\n#SBATCH --ntasks=1\n\n# Request 4GB of RAM\n#SBATCH --mem=4G\n\n# Run for a maximum of 30 minutes\n#SBATCH --time=30:00\n\n# Name of the job\n#SBATCH --job-name=fastq\n\n# Name the output file \n#SBATCH --output=%x_%j.out\n\n# Request email to be sent at both begin and end, and if job fails\n#SBATCH --mail-type=END\n</code></pre>"},{"location":"ch04/08_accessing_public_experimental_data/#partition","title":"Partition","text":"<p>First, we will need to specify a partition. A partition refers to a group of nodes which are characterized by their hardware. Specifying a partition is optional and if not specified the default partition is general. As practice we will specify general anyways using the following line: </p> <pre><code>#SBATCH --partition=general\n</code></pre> <p>Other Partitions: </p> Partition Intended Use Max Runtime general General computing \u2013 default partition 30 hours short General computing with short runtime 3 hours week General computing with longer runtime 7 days nvgpu NVIDIA GPU partition 48 hours <p>You can check partition usage using the following command: </p> <pre><code>sinfo -p partition_name\nsinfo -p general \n</code></pre>"},{"location":"ch04/08_accessing_public_experimental_data/#walltime","title":"Walltime","text":"<p>Walltime is the maximum amount of time your job will run. </p> <p>Your job may run for less time than you request, but it will not run for more time than you request.</p> <p>Walltime is requested with #SBATCH --time=, where \u201cdd\u201d refers to day(s), \u201chh\u201d to hour(s), \u201cmm\u201d to minute(s), and \u201css\u201d to second(s). You will replace each of these units with a two-digit numeral. Acceptable formats are: mm, mm:ss, hh:mm:ss, dd-hh, dd-hh:mm, dd-hh:mm:ss.</p> <pre><code># requesting 30 hours of walltime (hh:mm:ss)\n#SBATCH --time=30:00:00\n</code></pre>"},{"location":"ch04/08_accessing_public_experimental_data/#nodes-tasks-and-cores-cpus","title":"Nodes, Tasks, and Cores (CPUs)","text":"<p>The nodes, tasks, and core (CPU) resources you request depend on the type of job you are running. </p> <p>Node: A \u201cnode\u201d is a server in the cluster. Each node is configured with a certain number of cores (CPUs).</p> <p>Task: A \u201ctask\u201d is a process sent to a core. By default, 1 core is assigned per 1 task.</p> <p>Core/CPU: The terms \u201ccore\u201d and \u201ccpu\u201d are used interchangeably in high-performance computing.</p> <p>VACC recommend's that you begin with 1 node and 2 processes. As we move forward, we will change the number of nodes required for \"bigger\" jobs.</p> <pre><code># requesting 1 compute node\n#SBATCH --nodes=1\n# requesting 2 processes\n#SBATCH --ntasks=2\n</code></pre>"},{"location":"ch04/08_accessing_public_experimental_data/#mail-type","title":"Mail Type","text":"<p>In order to receive emails, you must set what types of emails you would like to receive, using the flag --mail-type. The options include: BEGIN (when your job begins), END (when your job ends), FAIL (if your job fails), ALL. For example:</p> <pre><code>#SBATCH --mail-type=ALL\n</code></pre>"},{"location":"ch04/08_accessing_public_experimental_data/#job-name","title":"JOB NAME","text":"<p>Job name is used as part of the name of the job log files. It also appears in lists of queued and running jobs.</p> <p>Specifying a job name is not required. If you don\u2019t supply a job name, the job ID (supplied by Slurm) is used.</p> <p>However, if you do wish to specify a job name, use the --job-name flag. For example, where your job name is \u201cmyjob\u201d:</p> <pre><code># replace \"myjob\" with YOUR chosen job name\n#SBATCH --job-name=myjob\n</code></pre>"},{"location":"ch04/08_accessing_public_experimental_data/#job-submission","title":"Job Submission","text":"<p>Once your job script is written, you can submit it. To submit your job, use the sbatch command with your filename. For example, where the filename is \u201cmyfilename\u201d:</p> <pre><code># replace \"myfilename\" with YOUR filename\nsbatch myfilename\n</code></pre> <p>When you submit your job, Slurm will respond with the job ID. For example, where the job ID Slurm assigns is \u201c123456,\u201d Slurm will respond:</p> <pre><code>Submitted batch job 123456\n</code></pre> <p>Note your job ID! </p>"},{"location":"ch04/08_accessing_public_experimental_data/#slurm-commands","title":"SLURM Commands","text":"Command What It Does sbatch  Submits a job, e.g., sbatch myjob scontrol show job  Detailed information about a particular job, e.g., scontrol show job 123456 squeue Checks status of all jobs in scheduling queue squeue -u  Checks status of all jobs belonging to the named user, e.g., squeue -u usr1234 squeue --start -j  Estimates earliest start time of a particular job, e.g., squeue --start -j 123456 squeue --start -u  Estimates earliest start time of all jobs belonging to the named user, e.g., squeue --start -u usr1234 scancel -u  Deletes/cancels all jobs belonging to the named user, e.g., scancel -u usr1234 scancel  Deletes/cancels a particular job, e.g., scancel 123456 <p>The first script is a loop that will go through your list of SRR's, and calls a second script at each iteration, passing it for each SRR number on the list. </p> <p>The script directly below is called <code>sra_fqdump.sh</code></p> <pre><code>#!/bin/sh\n#SBATCH --partition=general\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --mem=4G\n#SBATCH --time=30:00\n#SBATCH --job-name=fastq\n#SBATCH --output=%x_%j.out\n#SBATCH --mail-type=END\n\n#while there are lines in the list of SRRs file\nwhile read p\ndo\n#call the bash script that does the fastq dump, passing it the SRR number next $\nsbatch inner_script.sh $p\ndone &lt;list_of_SRRs.txt\n</code></pre> <p>The script directly below is called <code>inner_script.sh</code></p> <pre><code>#!/bin/sh\n#SBATCH --partition=general\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --mem=4G\n#SBATCH --time=30:00\n#SBATCH --job-name=fastq\n#SBATCH --output=%x_%j.out\n#SBATCH --mail-type=END\n\n#for single end reads only\nfastq-dump --gzip $1\n</code></pre> <p>Estimated Memory Requirements </p> Data Type Memory Required (Approx.) Single-end small dataset (~1GB SRA file) 4-8 GB Single-end large dataset (~10GB SRA file) 8-16 GB Paired-end small dataset (~10-20GB SRA file) 8-16 GB Paired-end large dataset (~50GB SRA file) 16-32 GB+ <p>To run the main script:</p> <pre><code>sbatch sra_fqdump.sh\n</code></pre>"},{"location":"ch04/08_accessing_public_experimental_data/#paired-end-files","title":"Paired end files","text":"<p>Unlike the standard format for paired end data, where we normally find two fastq files labelled as sample1_001.fastq and sample1_002.fastq, SRR files can be very misleading in that even paired end reads are found in one single file, with sequence pairs concatenated alongside each other. Because of this format, paired files need to be split at the download step. SRA toolkit has an option for this called \"--split-files\". By using this, one single SRR file will download as SRRxxx_1.fastq and SRRxxx_2.fastq.</p> <p>Furthermore, there is a very helpful improvement on this function called \"--split-3\" which splits your SRR into 3 files: one for read 1, one for read 2, and one for any orphan reads (ie: reads that aren't present in both files). This is important for downstream analysis, as some aligners require your paired reads to be in sync (ie: present in each file at the same line number) and orphan reads can throw this order off. Change the inner_script.sh as follows if your reads are paired end:</p> <pre><code>#!/bin/sh\n#SBATCH --partition=general\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --mem=4G\n#SBATCH --time=30:00\n#SBATCH --job-name=fastq\n#SBATCH --output=%x_%j.out\n#SBATCH --mail-type=END\n\n#splits paired read sra files into two normal fastq files plus a third for any orphaned reads, to keep paired files in sync\nfastq-dump --split-3 --gzip $1\n</code></pre> <p>Class Exercise</p> <ol> <li>To check that you are able to download fastq files using the scripts above, please download the <code>SRR_download</code> folder: </li> </ol> <pre><code>/gpfs1/cl/mmg3320/course_materials/SRR_download\n</code></pre> <ol> <li> <p>Run the <code>sra_fqdump.sh</code> script</p> </li> <li> <p>If this ran successfully, you should see two new fastq files and an email in your inbox. </p> </li> </ol> <pre><code>SRR25462396.fastq.gz\nSRR25462427.fastq.gz\n</code></pre> <ol> <li> <p>Check their sizes to see that SRR25462396 is 3.2MB and SRR25462427 is 4.6MB. </p> </li> <li> <p>When you are finished show me your laptop. </p> </li> </ol>"},{"location":"ch04/08_accessing_public_experimental_data/#bypassing-storage-issues-with-scratch","title":"Bypassing storage issues with /scratch","text":"<p>When downloading large datasets to the server, it\u2019s important to consider storage limits. If you download files to your home directory, the maximum storage allowed is 100GB. This can become an issue when handling tens or hundreds of FASTQ files because SRA-Toolkit does not download FASTQ files directly. Instead, it first writes an intermediate cache file of equal size, which is not automatically removed. As a result, you may quickly run into storage errors, causing incomplete downloads and error messages in your log file.</p> <p>To avoid this, use the scratch space on the VACC (/scratch). This location has a much larger storage limit (12TB) and is better suited for handling large downloads.</p>"},{"location":"ch04/08_accessing_public_experimental_data/#summary-of-header-lines","title":"Summary of Header Lines","text":"Header Line What It Does Example #SBATCH --partition= Specifies partition #SBATCH --partition=short or #SBATCH --partition=dggpu #SBATCH --nodes= Requests number of nodes (n) #SBATCH --nodes=1 #SBATCH --ntasks= Requests number of processes to run #SBATCH --ntasks=4 #SBATCH --gres=gpu: Requests GPUs #SBATCH --gres=gpu:1 #SBATCH --mem= Requests memory for the entire job #SBATCH --mem=24G #SBATCH --mem-per-cpu= Requests memory for the entire job #SBATCH --mem-per-cpu=1G #SBATCH --time= Requests amount of time needed for job. Acceptable formats are: mm, mm:ss, hh:mm:ss, dd-hh, dd-hh:mm, dd-hh:mm:ss #SBATCH --time=01:00:00 #SBATCH --job-name= Sets job name #SBATCH --job-name=myjob #SBATCH --mail-user=youremail@uvm.edu Sets email address where status emails are sent #SBATCH --mail-user=usr1234@uvm.edu #SBATCH --mail-type= Requests that a status email be sent. Options include: NONE, BEGIN, END, FAIL, REQUEUE, ALL. #SBATCH --mail-type=ALL #SBATCH --output=%x_%j.out This command sets a custom output file name by using Slurm-assigned variables: %x =  and %j =  Output filed named: myjob_123456.out"},{"location":"ch04/08_accessing_public_experimental_data/#citation","title":"Citation","text":"<p>This lesson has been developed by members of the teaching team at the Harvard Chan Bioinformatics Core (HBC). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p> <p>This lesson was developed using materials from the Vermont Advanced Computing Center. These materials are freely available.</p>"},{"location":"ch05/","title":"Overview","text":""},{"location":"ch05/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this week, students will be able to:  </p> <ul> <li>Describe the contents and format of a FASTQ file</li> <li>Create a quality report using FASTQC</li> <li>Run the multiQC tool to gather QC metrics from multiple tools for all samples</li> <li>Assess and compare QC metrics among samples</li> </ul>"},{"location":"ch05/09_assessing_fastqc_output/","title":"QC with FASTQC (L9)","text":""},{"location":"ch05/09_assessing_fastqc_output/#learning-objectives","title":"Learning Objectives:","text":"<ul> <li>Describe the contents and format of a FASTQ file</li> <li>Create a quality report using FASTQC</li> <li>Evaluate the quality of your NGS data using FastQC</li> </ul>"},{"location":"ch05/09_assessing_fastqc_output/#recap-advantages-of-batch-job-submissions","title":"Recap: Advantages of Batch Job Submissions","text":"<p>Batch job submission on an HPC (High-Performance Computing) system offers several advantages, particularly for computationally intensive tasks like bioinformatics, genomics, and large-scale data analysis.</p> <ol> <li> <p>Efficient Resource Management:</p> <ul> <li>Jobs are queued and scheduled based on resource availability, ensuring optimal utilization of CPUs, memory, and GPUs.</li> <li>Users can specify resource requirements (e.g., nodes, cores, memory) to avoid wasting computational power.</li> </ul> </li> <li> <p>Scalability:</p> <ul> <li>HPC clusters handle jobs of varying sizes, from single-threaded processes to massively parallel workloads.</li> <li>Batch processing supports running multiple jobs concurrently, improving overall throughput.</li> </ul> </li> <li> <p>Parallel Execution:</p> <ul> <li>Batch submission allows running thousands of jobs in parallel (e.g., processing multiple sequencing samples).</li> </ul> </li> <li> <p>Job Monitoring:</p> <ul> <li>Provides insights into job status, resource usage, and debugging. </li> </ul> </li> </ol>"},{"location":"ch05/09_assessing_fastqc_output/#looking-inside-of-sra_fqdumpsh","title":"Looking inside of <code>sra_fqdump.sh</code>","text":"<p>Purpose: Is to download FASTQ files from the SRA. FASTQ files to be downloaded are listed in a text file with accession numbers provided by you! </p> <p>To submit a script use the command: </p> <pre><code>sbatch your-script.sh\n</code></pre> <p>After submitting this script you will see <code>.out</code> files: </p>"},{"location":"ch05/09_assessing_fastqc_output/#stdout-records-the-output-of-programs","title":"STDOUT records the output of programs","text":"<p>Three data streams exist for all Linux programs: </p> <ul> <li>STDIN (Standard Input - a way to send data into the program)</li> <li>STDOUT (Standard Output - a way to send expected data out of the program)</li> <li>STDERR    (Standard Error - a way to send errors or warnings out of the program)</li> </ul>"},{"location":"ch05/09_assessing_fastqc_output/#quality-control-of-fastq-files","title":"Quality Control of FASTQ files","text":"<p>When working with high-throughput sequencing data, the raw reads you get off the sequencer will need to pass through a number of different tools in order to generate the final output. The first step in the RNA-Seq pipeline is to assess the quality of the sequence reads retrieved from the sequencing facility. </p>"},{"location":"ch05/09_assessing_fastqc_output/#starting-with-fastq-files","title":"Starting with FASTQ files","text":"<p>The FASTQ file format is the defacto file format for sequence reads generated from next-generation sequencing technologies. This file format evolved from FASTA in that it contains sequence data, but also contains quality information. Similar to FASTA, the FASTQ file begins with a header line. The difference is that the FASTQ header is denoted by a <code>@</code> character. For a single record (sequence read), there are four lines, each of which are described below:</p> Line Description 1 Always begins with '@', followed by information about the read 2 The actual DNA sequence 3 Always begins with a '+', and sometimes the same info as in line 1 4 Has a string of characters representing the quality scores; must have same number of characters as line 2 <p>Let's use the following read as an example:</p> <pre><code>@HWI-ST330:304:H045HADXX:1:1101:1111:61397\nCACTTGTAAGGGCAGGCCCCCTTCACCCTCCCGCTCCTGGGGGANNNNNNNNNNANNNCGAGGCCCTGGGGTAGAGGGNNNNNNNNNNNNNNGATCTTGG\n+\n@?@DDDDDDHHH?GH:?FCBGGB@C?DBEGIIIIAEF;FCGGI#########################################################\n</code></pre> <p>The line 4 has characters encoding the quality of each nucleotide in the read. The legend below provides the mapping of quality scores (Phred-33) to the quality encoding characters. Different quality encoding scales exist (differing by offset in the ASCII table), but note the most commonly used one is fastqsanger, which is the scale output by Illumina since mid-2011. <code>Quality encoding: !\"#$%&amp;'()*+,-./0123456789:;&lt;=&gt;?@ABCDEFGHI                    |         |         |         |         |     Quality score: 0........10........20........30........40</code></p> <p>Using the quality encoding character legend, the first nucelotide in the read (C) is called with a quality score of 31 (corresponding to encoding character <code>@</code>), and our Ns are called with a score of 2 (corresponding to encoding character <code>#</code>). As you can tell by now, this is a bad read. </p> <p>Each quality score represents the probability that the corresponding nucleotide call is incorrect. This quality score is logarithmically based and is calculated as:</p> <pre><code>Q = -10 x log10(P), where P is the probability that a base call is erroneous\n</code></pre> <p>These probabaility values are the results from the base calling algorithm and dependent on how much signal was captured for the base incorporation. The score values can be interpreted as follows:</p> Phred Quality Score Probability of incorrect base call Base call accuracy 10 1 in 10 90% 20 1 in 100 99% 30 1 in 1000 99.9% 40 1 in 10,000 99.99% <p>Therefore, for the first nucleotide in the read (C), there is less than a 1 in 1000 chance that the base was called incorrectly. Whereas, for the the end of the read there is greater than 50% probabaility that the base is called incorrectly.</p>"},{"location":"ch05/09_assessing_fastqc_output/#assessing-quality-with-fastqc","title":"Assessing quality with FastQC","text":"<p>Now that we understand what information is stored in a FASTQ file, the next step is to examine quality metrics for our data.</p> <p>FastQC provides a simple way to do some quality checks on raw sequence data coming from high throughput sequencing pipelines. It provides a modular set of analyses, which you can use to obtain an impression of whether your data has any problems that you should be aware of before moving on to the next analysis.</p> <p>FastQC does the following: * accepts FASTQ files (or BAM files) as input * generates summary graphs and tables to help assess your data * generates an easy-to-view HTML-based report with the graphs and tables</p>"},{"location":"ch05/09_assessing_fastqc_output/#running-fastqc","title":"Running FastQC","text":"<p>We would like to run the FastQC tool on fastq files in the raw_fastq directory. However, if we were to run the following <code>fastqc</code> command now we would retrieve the following error:</p> <pre><code>fastqc --help\n\n-bash: fastqc: command not found\n</code></pre> <p>Let's load fastqc with the Environment Module System: </p> <pre><code>module load gcc/13.3.0-xp3epyt\nmodule load fastqc/0.12.1-qxseug5\n</code></pre> <p>Once a module for a tool is loaded, you have essentially made it directly available to you like any other basic shell command. Check to see it is loaded with: </p> <pre><code>module list\n</code></pre> <p>Now, let's create a directory to store the output of FastQC:</p> <pre><code>mkdir fastqc\n</code></pre> <p>How do we know which argument to type to properly use <code>fastqc</code>?</p> <pre><code>fastqc --help\n</code></pre> <pre><code>SYNOPSIS\n\n    fastqc seqfile1 seqfile2 .. seqfileN\n\n    fastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] \n           [-c contaminant file] seqfile1 .. seqfileN\n</code></pre> <p>Let's run fastqc on <code>Mov10_oe_1.subset.fq</code></p> <pre><code>fastqc Mov10_oe_1.subset.fq\n</code></pre>"},{"location":"ch05/09_assessing_fastqc_output/#fastqc-outputs","title":"FASTQC Outputs","text":"<p>For each individual FASTQ file that is input to FastQC, there are two output files that are generated. </p> <ol> <li> <p>The first is an HTML file which is a self-contained document with various graphs embedded into it. Each of the graphs evaluate different quality aspects of our data, we will discuss in more detail in this lesson. </p> </li> <li> <p>Alongside the HTML file is a zip file (with the same name as the HTML file, but with .zip added to the end). This file contains the different plots  from the report as separate image files but also contains data files which are designed to be easily parsed to allow for a more detailed and automated evaluation of the raw data on which the QC report is built.</p> </li> </ol> <p>Class Exercise</p> <p>Run FASTQC on all FASTQ files in <code>raw_fastq</code>. FASTQC allows you to redirect your output into a specified location with the <code>-o</code> parameter. Be sure to use this parameter in your final code. </p> <p>If successful, you will see the following outputs inside of the <code>fastqc</code> folder: </p> <pre><code>Irrel_kd_1.subset_fastqc.html  Irrel_kd_3.subset_fastqc.html  Mov10_oe_2.subset_fastqc.html\nIrrel_kd_1.subset_fastqc.zip   Irrel_kd_3.subset_fastqc.zip   Mov10_oe_2.subset_fastqc.zip\nIrrel_kd_2.subset_fastqc.html  Mov10_oe_1.subset_fastqc.html  Mov10_oe_3.subset_fastqc.html\nIrrel_kd_2.subset_fastqc.zip   Mov10_oe_1.subset_fastqc.zip   Mov10_oe_3.subset_fastqc.zip\n</code></pre> <p>Note: We are running FASTQC interactively. This is running on the login node relatively quickly. This is because this alignment for these FASTQ files was only performed for a small portion of the chromosome 1. Later on, this will take a lot longer. Therefore, you will need to generate a script. </p> <p>Running Parameters for FASTQC: </p> <ul> <li>10G of memory is required</li> <li>1 node, 2 tasks </li> </ul>"},{"location":"ch05/09_assessing_fastqc_output/#assessing-the-results-from-fastqc","title":"Assessing the results from FastQC","text":"<p>Class Exercise</p> <p>Grab the following folder from the location below. </p> <pre><code>/gpfs1/cl/mmg3320/course_materials/FASTQC_example\n</code></pre>"},{"location":"ch05/09_assessing_fastqc_output/#viewing-the-html-report-output-from-fastqc","title":"Viewing the HTML report output from FASTQC","text":"<p>All of the following are solutions that allow students to transfer files between remote (i.e. VACC) and local (i.e. your laptop) servers. </p> <p>An FTP application such as Filezilla</p> <p>RStudio (via VACC-OOD)</p> <p>You can export it or simply view it using RStudio</p> <p>In File Explorer on OpenOnDemand, use the \"Download\" button </p>"},{"location":"ch05/09_assessing_fastqc_output/#interpreting-the-html-report","title":"Interpreting the HTML report","text":"<p>Now we can take a look at the metrics and assess the quality of our sequencing data!</p> <p>FastQC has a really well documented manual page with detailed explanations about every plot in the report. </p> <p>Within our report, a summary of all of the modules is given on the left-hand side of the report. Don't take the yellow \"WARNING\"s and red \"FAIL\"s too seriously; they should be interpreted as flags for modules to check out. </p> <p>The first module gives the basic statistics for the sample. Generally it is a good idea to keep track of the total number of reads sequenced for each sample and to make sure the read length and %GC content is as expected.</p>"},{"location":"ch05/09_assessing_fastqc_output/#per-base-sequence-quality","title":"Per base sequence quality","text":"<p>One of the most important analysis modules is the \"Per base sequence quality\" plot. This plot provides the distribution of quality scores at each position in the read across all reads. The y-axis gives the quality scores, while the x-axis represents the position in the read. The color coding of the plot denotes what are considered high, medium and low quality scores. </p> <p>This plot can alert us to whether there were any problems occuring during sequencing and whether we might need to contact the sequencing facility.</p> <p>For example, the box plot at nucleotide 1 shows the distribution of quality scores for the first nucleotide of all reads in the <code>Mov10_oe_1</code> sample. The yellow box represents the 25th and 75th percentiles, with the red line as the median. The whiskers are the 10th and 90th percentiles. The blue line represents the average quality score for the nucleotide. Based on these metrics, the quality scores for the first nucleotide are quite high, with nearly all reads having scores above 28.</p> <p>The quality scores appear to drop going from the beginning toward the end of the reads. For reads generated by Illumina sequencing, this is not alarming and there are known causes for this drop in quality. </p> <p>For Illumina sequencing, the quality of the nucleotide base calls are related to the signal intensity and purity of the fluorescent signal. Low intensity fluorescence or the presence of multiple different fluorescent signals can lead to a drop in the quality score assigned to the nucleotide. Due to the nature of sequencing-by-synthesis there are some drops in quality that can be expected, but other quality issues can be indicative of a problem at the sequencing facility.</p>"},{"location":"ch05/09_assessing_fastqc_output/#per-sequence-quality-scores","title":"Per sequence quality scores","text":"<p>The \"Per sequence quality scores\" plot gives you the average quality score on the x-axis and the number of sequences with that average on the y-axis. We hope the majority of our reads have a high average quality score with no large bumps at the lower quality values.</p> <p>This data has a small bump at a mean quality of 12. Since it doesn't represent a large proportion of the data, it isn't extremely worrisome, but it might be worth a quick check of the reads resulting in the poor quality scores.</p>"},{"location":"ch05/09_assessing_fastqc_output/#per-base-sequence-content","title":"Per base sequence content","text":"<p>The next plot gives the \"Per base sequence content\", which always gives a FAIL for RNA-seq data. This is because the first 10-12 bases result from the 'random' hexamer priming that occurs during RNA-seq library preparation. This priming is not as random as we might hope giving an enrichment in particular bases for these intial nucleotides. </p>"},{"location":"ch05/09_assessing_fastqc_output/#per-sequence-gc-content","title":"Per sequence GC content","text":"<p>The \"Per sequence GC content\" plot gives the GC distribution over all sequences. Generally is a good idea to note whether the GC content of the central peak corresponds to the expected % GC for the organism. Also, the distribution should be normal unless over-represented sequences (sharp peaks on a normal distribution) or contamination with another organism (broad peak).</p> <p>This plot would indicate some type of over-represented sequence with the sharp peaks, indicating either contamination or a highly over-expressed gene.</p>"},{"location":"ch05/09_assessing_fastqc_output/#sequence-duplication","title":"Sequence duplication","text":"<p>The next module explores numbers of duplicated sequences in the library. This plot can help identify a low complexity library, which could result from too many cycles of PCR amplification or too little starting material. For RNA-seq we don't normally do anything to address this in the analysis, but if this were a pilot experiment, we might adjust the number of PCR cycles, amount of input, or amount of sequencing for future libraries. In this analysis we seem to have a large number of duplicated sequences, but this is expected due to the subset of data we are working with containing the over-expression of MOV10.</p>"},{"location":"ch05/09_assessing_fastqc_output/#over-represented-sequences","title":"Over-represented sequences","text":"<p>The \"Overrepresented sequences\" table is another important module as it displays the sequences (at least 20 bp) that occur in more than 0.1% of the total number of sequences. This table aids in identifying contamination, such as vector or adapter sequences. If the %GC content was off in the above module, this table can help identify the source. If not listed as a known adapter or vector, it can help to BLAST the sequence to determine the identity.</p> <p>Since our data is just a subset of the original data and it contains the over-expressed MOV10 gene, if we BLAST the sequences we will find they belong to MOV10. For this experiment, these over-represented sequences are not concerning.</p>"},{"location":"ch05/09_assessing_fastqc_output/#summary","title":"Summary","text":"<p>As our report only represents a subset of reads (chromosome 1) for <code>Mov10_oe_1.subset.fq</code>, which can skew the QC results. If the quality of the raw data is acceptable, we can move on to the next step and quantify gene expression.</p> <p>This lesson has been developed by members of the teaching team at the Harvard Chan Bioinformatics Core (HBC). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>"},{"location":"ch05/10_trimming/","title":"Trimming and Filtering (L10)","text":""},{"location":"ch05/10_trimming/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Learn to clean FASTQ reads using Trimmomatic if required</li> </ul>"},{"location":"ch05/10_trimming/#introduction","title":"Introduction","text":"<p>During the last class, we took a high-level look at the quality of each of our samples using FastQC. We visualized per-base quality graphs showing the distribution of read quality at each base across all reads in a sample and extracted information about which samples fail which quality checks. Some of our samples failed quite a few quality metrics used by FastQC. This does not mean, though, that our samples should be thrown out! It is very common to have some quality metrics fail, and this may or may not be a problem for your downstream application. </p> <p>Now, we want to make sure that as many reads as possible map or align accurately to the genome. To ensure accuracy, only a small number of mismatches between the read sequence and the genome sequence are allowed, and any read with more than a few mismatches will be marked as being unaligned.</p> <p>Therefore, to make sure that all the reads in the dataset have a chance to map/align to the genome, unwanted information can be trimmed off from every read, one read at a time. The types of unwanted information can include one or more of the following:</p> <ul> <li>leftover adapter sequences</li> <li>known contaminants (strings of As/Ts, other sequences)</li> <li>poor quality bases </li> </ul>"},{"location":"ch05/10_trimming/#where-does-this-unwanted-information-come-from","title":"Where does this \"unwanted information\" come from?","text":"<ul> <li> <p>During the sequencing process, adapter sequences are added to the ends of the reads. These sequences are used to attach the RNA molecules to the sequencing platform but they do not map to regions of the genome. These adapter sequences can interfere with downstream analysis and lead to inaccurate results. </p> </li> <li> <p>RNA-seq data may contain low-quality reads that are caused by sequencing errors, PCR bias, or other factors such as poor sample quality. By removing these low-quality reads researcher can ensure that the data is of high quality. </p> </li> </ul>"},{"location":"ch05/10_trimming/#steps-of-trimming-data","title":"Steps of trimming data","text":"<p>Step 1: Quality Control</p> <p>The first step in trimming RNA-seq data is to assess the quality of the raw reads. This can be done using software such as FastQC, which generates a report that includes information on read length, GC content, base quality, and sequencing adapter contamination. If the data is of poor quality, it may need to be re-sequenced or excluded from further analysis.</p> <p>Step 2: Adapter Trimming The next step is to remove adapter sequences from the reads. These sequences are added during library preparation and can interfere with downstream analysis. Many tools (some listed below) can be used to trim adapters from the reads by searching for the adapter sequence and removing it.</p> <p>Step 2b: Quality Trimming During adapter trimming, we could also remove low-quality reads by setting a minimum quality threshold and removing any bases that fall below this threshold. Trimming tools often have built-in quality trimming options that can be used for this purpose.</p> <p>Step 2c: Short read filtering</p> <p>Reads that are too short after trimming can also be removed. This step is important to ensure that only high-quality reads are used for downstream analysis. Trimming tools also have this built-in option as well. </p>"},{"location":"ch05/10_trimming/#tools-for-trimming","title":"Tools for Trimming","text":"<p>There are a number of tools that can be used for read trimming, some include:</p> <ul> <li>Cutadapt</li> <li>Trimmomatic</li> <li>fastp</li> <li>Trim Galore </li> </ul> <p>They have a varying range of clipping and trimming features, but for the most part they all work similarly. </p>"},{"location":"ch05/10_trimming/#trimming-is-not-always-required","title":"Trimming is not always required","text":"<ul> <li>There are some aligners that are available which will \"soft-clip\" low-quality bases or adapter sequences during alignment. If you are working with short reads (&lt;50 bp), trimming can actually prevent the aligner from discarding poor-quality reads. </li> </ul> <p>We will compare some aligners next week </p> <p>In the class exercises below, we will be using Trimmomatic and Trim Galore. </p>"},{"location":"ch05/10_trimming/#option-1-trimmomatic","title":"Option 1: Trimmomatic","text":"<pre><code>module load gcc/13.3.0-xp3epyt\n</code></pre> <p>Search for the module with: </p> <pre><code>module avail\n</code></pre> <p>Use the following to check that the program was loaded</p> <pre><code>module list\n</code></pre> <p>Trimmomatic has a variety of options to trim your reads. If we run the following command, we can see some of our options.</p> <pre><code>trimmomatic\n</code></pre> <pre><code>Usage: \n       PE [-version] [-threads &lt;threads&gt;] [-phred33|-phred64] [-trimlog &lt;trimLogFile&gt;] [-summary &lt;statsSummaryFile&gt;] [-quiet] [-validatePairs] [-basein &lt;inputBase&gt; | &lt;inputFile1&gt; &lt;inputFile2&gt;] [-baseout &lt;outputBase&gt; | &lt;outputFile1P&gt; &lt;outputFile1U&gt; &lt;outputFile2P&gt; &lt;outputFile2U&gt;] &lt;trimmer1&gt;...\n   or: \n       SE [-version] [-threads &lt;threads&gt;] [-phred33|-phred64] [-trimlog &lt;trimLogFile&gt;] [-summary &lt;statsSummaryFile&gt;] [-quiet] &lt;inputFile&gt; &lt;outputFile&gt; &lt;trimmer1&gt;...\n</code></pre> <p>This output shows us that we must first specify whether we have paired end (PE) or single end (SE) reads. Next, we specify what flag we would like to run. For example, you can specify threads to indicate the number of processors on your computer that you want Trimmomatic to use. In most cases using multiple threads (processors) can help to run the trimming faster. These flags are not necessary, but they can give you more control over the command. The flags are followed by positional arguments, meaning the order in which you specify them is important. In paired end mode, Trimmomatic expects the two input files, and then the names of the output files. These files are described below. While, in single end mode, Trimmomatic will expect 1 file as input, after which you can enter the optional settings and lastly the name of the output file.</p> option meaning inputFile1 Input reads to be trimmed. Typically the file name will contain an _1 or _R1 in the name. inputFile2 Input reads to be trimmed. Typically the file name will contain an _2 or _R2 in the name. outputFile1P Output file that contains surviving pairs from the _1 file. outputFile1U Output file that contains orphaned reads from the _1 file. outputFile2P Output file that contains surviving pairs from the _2 file. outputFile2U Output file that contains orphaned reads from the _2 file. <p>In addition, trimmomatic expects to see is the trimming parameters:</p> step meaning ILLUMINACLIP Perform adapter removal. SLIDINGWINDOW Perform sliding window trimming, cutting once the average quality within the window falls below a threshold. LEADING Cut bases off the start of a read, if below a threshold quality. TRAILING Cut bases off the end of a read, if below a threshold quality. CROP Cut the read to a specified length. HEADCROP Cut the specified number of bases from the start of the read. MINLEN Drop an entire read if it is below a specified length. TOPHRED33 Convert quality scores to Phred-33. TOPHRED64 Convert quality scores to Phred-64. <p>We will use only a few of these options and trimming steps in our analysis. It is important to understand the steps you are using to clean your data. For more information about the Trimmomatic arguments and options, see the Trimmomatic manual.</p> <p>However, a complete command for Trimmomatic will look something like the command below. This command is an example and will not work, as we do not have the files it refers to:</p> <pre><code>trimmomatic PE SRR_1056_1.fastq SRR_1056_2.fastq  \\\n        SRR_1056_1.trimmed.fastq SRR_1056_1un.trimmed.fastq \\\n        SRR_1056_2.trimmed.fastq SRR_1056_2un.trimmed.fastq \\\n        SLIDINGWINDOW:4:20 ILLUMINACLIP:SRR_adapters.fa \n</code></pre> <p>In this example, we have told Trimmomatic:</p> code meaning PE that it will be taking a paired end file as input SRR_1056_1.fastq the first input file name SRR_1056_2.fastq the second input file name SRR_1056_1.trimmed.fastq the output file for surviving pairs from the _1 file SRR_1056_1un.trimmed.fastq the output file for orphaned reads from the _1 file SRR_1056_2.trimmed.fastq the output file for surviving pairs from the _2 file SRR_1056_2un.trimmed.fastq the output file for orphaned reads from the _2 file SLIDINGWINDOW:4:20 to use a sliding window of size 4 that will remove bases if their phred score is below 20 ILLUMINACLIP:SRR_adapters.fa to clip the Illumina adapters from the input file using the adapter sequences listed in SRR_adapters.fa <p>Note</p> <p>Some of the commands we ran in this lesson are long! When typing a long command into your terminal, you can use the \\ character to separate code chunks onto separate lines. This can make your code more readable.</p>"},{"location":"ch05/10_trimming/#running-trimmomatic","title":"Running Trimmomatic","text":"<p>Class Exercise #1</p> <p>Estimated Time: ~20 minutes</p> <p>Collaboration: Please work with your neighbor if you have questions. I will begin answering questions at the 5 minute mark. </p> <p>Learning goal: By the end of this exercise, you should be able to:</p> <ul> <li>Understand what each Trimmmomatic argument does </li> <li>Successfully trim FASTQ files</li> <li>Verify trimming success using FastQC results</li> </ul> <p>Part A: Make a copy of the exercise materials into your current working directory. The folder can be found in the location below. Confirm that the folder is copied into your VACC working directory before continuing. </p> <pre><code>/gpfs1/cl/mmg3320/course_materials/trimmomatic_exercise\n</code></pre> <p>Part B: Modify the <code>trim.sh</code> script using Jupyter Notebook, Nano, or your preferred text editor. Your goal is to run <code>trimmomatic</code> on the provided FASTQ files. </p> <p>Important reminder: Trimmomatic arguments must be provided in the correct order starting with input files, output files, and remaining arguments. </p> argument to add meaning SE or PE single end or paired end file as input outputFile1P Output file that contains surviving pairs from the _1 file. outputFile1U Output file that contains orphaned reads from the _1 file. outputFile2P Output file that contains surviving pairs from the _2 file. outputFile2U Output file that contains orphaned reads from the _2 file. SLIDINGWINDOW:4:20 Uses a sliding window of size 4; trims when average Phred score &lt;20&gt; <p>You will need to identify two additional arguments to:  </p> argument to add meaning filter short reads drop an entire read if it is below 25 remove adapter Identify which adapter to remove using FastQC output <p>Note: We will be adding additional parameters the adapter file specified. Once you reach this point, please ask me for help. These parameters control how aggressively adapters are detected and removed.</p> <ul> <li><code>2:40:15</code> <ul> <li><code>2</code> : Seed mismatch threshold (maximum number of mismatches allowed in the adapter sequence match).</li> <li><code>40</code> : Palindrome mode threshold (minimum match length for identifying adapter sequences when the paired-end reads overlap).</li> <li><code>15</code> : Simple adapter trimming threshold (minimum length of a match required to trigger adapter removal).</li> </ul> </li> </ul> <p>Once your script is complete, run the script using:</p> <pre><code>sh trim.sh\n</code></pre> <p>Expected Output: You will know that your script is working correctly if you see output similar to the following,</p> <pre><code>TrimmomaticPE: Started with arguments:\nSRR2589044_1.fastq.gz SRR2589044_2.fastq.gz SRR2589044_1.trim.fastq.gz SRR2589044_1un.trim.fastq.gz SRR2589044_2.trim.fastq.gz SRR2589044_2un.trim.fastq.gz SLIDINGWINDOW:4:20 MINLEN:25 ILLUMINACLIP:trimmomatic_adapters/NexteraPE-PE.fa:2:40:15\nUsing PrefixPair: 'AGATGTGTATAAGAGACAG' and 'AGATGTGTATAAGAGACAG'\nUsing Long Clipping Sequence: 'GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAG'\nUsing Long Clipping Sequence: 'TCGTCGGCAGCGTCAGATGTGTATAAGAGACAG'\nUsing Long Clipping Sequence: 'CTGTCTCTTATACACATCTCCGAGCCCACGAGAC'\nUsing Long Clipping Sequence: 'CTGTCTCTTATACACATCTGACGCTGCCGACGA'\nILLUMINACLIP: Using 1 prefix pairs, 4 forward/reverse sequences, 0 forward only sequences, 0 reverse only sequences\nQuality encoding detected as phred33\n\nInput Read Pairs: 1107090 Both Surviving: 885220 (79.96%) Forward Only Surviving: 216472 (19.55%) Reverse Only Surviving: 2850 (0.26%) Dropped: 2548 (0.23%)\nTrimmomaticPE: Completed successfully\n</code></pre> <p>Part C: Check output file sizes</p> <pre><code>total 453M\n-rw-r--r-- 1 pdrodrig pi-jdragon 124M Feb 11 14:30 SRR2589044_1.fastq.gz\n-rw-r--r-- 1 pdrodrig pi-jdragon  94M Feb 11 18:32 SRR2589044_1.trim.fastq.gz\n-rw-r--r-- 1 pdrodrig pi-jdragon  18M Feb 11 18:32 SRR2589044_1un.trim.fastq.gz\n-rw-r--r-- 1 pdrodrig pi-jdragon 128M Feb 11 14:30 SRR2589044_2.fastq.gz\n-rw-r--r-- 1 pdrodrig pi-jdragon  91M Feb 11 18:32 SRR2589044_2.trim.fastq.gz\n-rw-r--r-- 1 pdrodrig pi-jdragon 271K Feb 11 18:32 SRR2589044_2un.trim.fastq.gz\n</code></pre> <p>Part D: Quality control with FASTQC Run FASTQC on all the <code>.gz</code> files (raw and trimmed), then open the HTML reports. When reviewing the FASTQC results, focus on: </p> <ul> <li>Per base sequence quality</li> <li>Adapter content </li> </ul> <p>The following questions will be included in HW#6. Please be sure you are able to answer them. </p> <p>1) How many reads were trimmed in <code>SRR2589044_1</code> and <code>SRR2589044_2</code>, respectively. State which files were used to answer this question. </p> <p>2) Did the per base sequence quality improve after trimming? If so, at which positions was the biggest improvement? </p> <p>3) Was adapter content reduced or eliminated after trimming? In your answer, please state the name of the adapter that was altered. </p>"},{"location":"ch05/10_trimming/#option-2-trim-galore","title":"Option 2: Trim Galore","text":"<p>In the next example we will be using Trim Galore. Trim Galor is a wrapper tool that combines Cutadapt (adapter and quality trimming) and FastQC.  </p> <p>BASIC USAGE: <code>trim_galore [options] &lt;filename(s)&gt;</code></p> <p>Trim Galore automatically detects adapters by default, but for this exercise we will explicitly specify Illumina adapters.</p>"},{"location":"ch05/10_trimming/#some-general-options","title":"Some general options:","text":"<p>More options can be found here</p> <ul> <li> <p><code>-h/--help</code></p> <ul> <li>Print this help message and exits.</li> </ul> </li> <li> <p><code>-q/--quality &lt;INT&gt;</code></p> <ul> <li>Trim low-quality ends from reads in addition to adapter removal. For RRBS samples, quality trimming will be performed first, and adapter trimming is carried in a second round. Other files are quality and adapter trimmed in a single pass. The algorithm is the same as the one used by BWA (Subtract INT from all qualities; compute partial sums from all indices to the end of the sequence; cut sequence at the index at which the sum is minimal).</li> <li>Default Phred score: <code>20</code></li> </ul> </li> <li> <p><code>--phred33</code></p> <ul> <li>Instructs Cutadapt to use <code>ASCII+33</code> quality scores as Phred scores (Sanger/Illumina 1.9+ encoding) for quality trimming.</li> <li>Default: <code>ON</code></li> </ul> </li> <li> <p><code>--fastqc</code></p> <ul> <li>Run FastQC in the default mode on the FastQ file once trimming is complete.</li> </ul> </li> <li> <p><code>--fastqc_args \"&lt;ARGS&gt;\"</code></p> <ul> <li>Passes extra arguments to FastQC. If more than one argument is to be passed to FastQC they must be in the form <code>arg1 arg2 [..]</code>.</li> <li>An example would be: <code>--fastqc_args \"--nogroup --outdir /home/\"</code>.</li> <li>Passing extra arguments will automatically invoke FastQC, so <code>--fastqc</code> does not have to be specified separately.</li> </ul> </li> <li> <p><code>--illumina</code></p> <ul> <li>Adapter sequence to be trimmed is the first 13bp of the Illumina universal adapter <code>AGATCGGAAGAGC</code> instead of the default auto-detection of adapter sequence.</li> </ul> </li> <li> <p><code>--gzip</code></p> <ul> <li>Compress the output file with <code>gzip</code>.</li> <li>If the input files are gzip-compressed the output files will be automatically gzip compressed as well.</li> </ul> </li> <li> <p><code>--dont_gzip</code></p> <ul> <li>Output files won't be compressed with gzip. This overrides <code>--gzip</code>.</li> </ul> </li> <li> <p><code>-o/--output_dir &lt;DIR&gt;</code></p> <ul> <li>If specified all output will be written to this directory instead of the current directory. If the directory doesn't exist it will be created for you.</li> </ul> </li> </ul>"},{"location":"ch05/10_trimming/#running-trim-galore","title":"Running Trim Galore","text":"<p>Class Exercise #2</p> <p>Estimated Time: ~10 minutes</p> <p>Collaboration: Please work with your neighbor if you have questions.</p> <p>Learning goal: By the end of this exercise, you should be able to:</p> <ul> <li>Use Trim Galore to perform adapter and quality trimming</li> <li>Verify successful trimming using post-trimming FastQC results</li> </ul> <p>Part A: Make a copy of the exercise materials into your current working directory. The folder can be found in the location below. Confirm that the folder is copied into your VACC working directory before continuing. </p> <pre><code>/gpfs1/cl/mmg3320/course_materials/trim_galore_exercise\n</code></pre> <p>Part B: Run Trim Galore on <code>Test_adapter_contamination.fq.gz</code>. Before running Trim Galore, make sure you:</p> <ul> <li>Load the Trim Galore module</li> <li>Load the Cutadapt module</li> <li>Load the FastQC module</li> </ul> <p>Trimming Instructions Your trim-galore command should:  </p> <ul> <li>Use the <code>--illumina</code> option</li> <li>Run FASTQC automatically on the trimmed FASTQ file</li> </ul> <p>Run the command from the directory containing the FASTQ file.</p> <p>The following questions will be included in HW#6. Please be sure you are able to answer them. </p> <p>1) Comparing the FASTQC reports before and after trimming, report how the sequence length changes. </p> <p>2) Which FastQC modules showed the most improvement after trimming? Be sure to report which adapter was reduced/eliminated after trimming. </p> <p>3) If you examine the Overrepresented Sequences table, do any contaminants remain? </p> <p>You may notice that some overrepresented sequences persist even after trimming. In practice, this can be addressed by modifying or extending the adapter FASTA file to include additional contaminant sequences.</p>"},{"location":"ch06/11_ClassExercise_Align-Submission/","title":"11 ClassExercise Align Submission","text":""},{"location":"ch06/11_ClassExercise_Align-Submission/#class-exercise-1-script-submission-for-aligning-with-hisat2","title":"Class Exercise 1: Script submission for aligning with HISAT2","text":"<p>Before we begin, we will submit the following script, which will map and align FASTQ files to the provided reference genome using the HISAT2 aligner. By the end of the lecture, it will be clear why this aligner was chosen. The script will run for approximately 40 minutes to run. We will most likely review the results together during the next class. </p> <ul> <li>HISAT2 main page</li> <li>HISAT2 user manual</li> </ul>"},{"location":"ch06/11_ClassExercise_Align-Submission/#instructions","title":"Instructions:","text":"<ol> <li> <p>Make a copy of the following folder located here: </p> <pre><code>/gpfs1/cl/mmg3320/course_materials/HISAT2_example \n</code></pre> </li> <li> <p>Check to see that your file sizes are correct. </p> <pre><code>total 2.1G\n-rwxr-xr-x 1 pdrodrig pi-jdragon  1.4K Feb 19 08:56 hisat2_align.sh\n-rwxr-xr-x 1 pdrodrig pi-jdragon 1010M Feb 19 08:56 SRR13423162.fastq.gz\n-rwxr-xr-x 1 pdrodrig pi-jdragon  1.1G Feb 19 08:56 SRR13423165.fastq.gz\n</code></pre> </li> <li> <p>Submit the <code>hisat2_align.sh</code> using the SLURM job submission system. </p> </li> <li> <p>Check to see that your job is running with the <code>squeue -u your-net-id</code> command. You will see the following:</p> <pre><code>JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n60223   general align_CD pdrodrig  R       0:08      1 node200\n</code></pre> </li> </ol>"},{"location":"ch06/12_ClassExercise2_Align-Modify/","title":"12 ClassExercise2 Align Modify","text":""},{"location":"ch06/12_ClassExercise2_Align-Modify/#class-exercise-3-modifying-the-hisat2-alignment-script-for-pe-samples","title":"Class Exercise 3: Modifying the HISAT2 alignment script for PE samples","text":""},{"location":"ch06/12_ClassExercise2_Align-Modify/#overview","title":"Overview","text":"<p>Everyone will download at least six FASTQ files from the GEO repository for their final project. To ensure that the alignment and downstream processing steps are performed consistently for each sample, it is best to apply the same line of code to all samples simultaneously. To achieve this, we can use loops \u2014 a feature of programming languages that allows us to execute a series of commands on multiple files at once.</p> <p>In addition to saving time, loops make your code more readable and concise. There are various types of loops, but we have focused on the for loop. At the end, you will be \"creating\" a script that could be used to align paired end samples using HISAT2. </p> <p>The final outputs will be in a form of a BAM file. BAM is the binary version of a SAM (Sequence Alignment Map) file. A SAM file is a tab-delimited text file that contains information about each individual read and its alignment to the genome. However, SAM files are not a desirable output due to their large size. Therefore, we will immediately convert them to BAM files using SAMtools.</p>"},{"location":"ch06/12_ClassExercise2_Align-Modify/#for-loops","title":"For loops","text":"<p>For loops are constructed with 4 basic words: </p> Words What it does for set the loop variable name in specify whatever it is we are looping over do specify what we want to do with each item done tell the computer we are done <p>putting these together, for loop syntax is as follows: </p> <pre><code>for VARIABLE in file1 file2 file3\ndo\n  command1 on $VARIABLE\n  command2 \ndone\n</code></pre> <p>A basic loop looks something like this when it\u2019s written within a job script:</p> <pre><code>for i in A B C\ndo\n  echo $i\ndone\n</code></pre> <p>The VARIABLE could be any letter or word that is meaningful, but is commonly represented by a single letter like \"i\" for ease.  Often the <code>in</code> portion will be a directory instead of single files (A, B, C) - i.e. a directory that contains the FASTQ files you want to work with. </p>"},{"location":"ch06/12_ClassExercise2_Align-Modify/#variables","title":"Variables","text":"<p>The variable portion of a for loop is the item that we are going to iterate over and will change with every \u201cnew\u201d loop. The action is typically being performed on each individual file. In their simplest form variables can look like this:</p> <pre><code>DIRECTORY=PATH/TO/DIRECTORY\n\nDBDIR=/users/p/d/pdrodrig/genome_index\n</code></pre> <p>In the above example, this is showing a location to a specific directory </p> <p>As a reminder, if we want to make sure this assignment worked, use the <code>echo</code> command. This is a command that prints out whatever is provided to it, which can be really useful to test commands or report information back to yourself during a loop.</p> <pre><code>echo $DBDIR \n</code></pre>"},{"location":"ch06/12_ClassExercise2_Align-Modify/#instructions","title":"Instructions","text":"<p>Let's start assembling our script. You will be copying and pasting the script in sections. </p> <pre><code>+ The purpose of this exercise is the understand how the alignment script was composed. Therefore, take the time to read any of the descriptions provided.\n\n+ Make a copy of the following folder located here:\n\n```bash\n/gpfs1/cl/mmg3320/course_materials/HISAT2-modify\n```\n\n+ You will see two FASTQ files inside called `JC1A_R1.fastq.gz` and  `JC1A_R2.fastq.gz`.\n\n+ Open Jupyter Notebooks to \"write\" your alignment script using the step by step instructions below. Call the script `hisat2-modify-PE.sh`. Keep your terminal open as well.\n</code></pre> <ol> <li> <p>In lines 1-9, provide the job submission parameters. Copy-and-paste the following section: </p> <pre><code>#!/bin/bash\n#SBATCH --partition=general\n#SBATCH --nodes=1\n#SBATCH --ntasks=2\n#SBATCH --mem=10G\n#SBATCH --time=3:00:00\n#SBATCH --job-name=align_CD8\n# %x=job-name %j=jobid\n#SBATCH --output=%x_%j.out\n</code></pre> </li> <li> <p>In lines 11-16, copy-and-paste the next section. The importance of this section is that it will allow you to process all FASTQ files while maintaining the file name for each file output.  </p> <pre><code># Iterate through each fastq.gz file in the current directory\nfor fastq_file in *fastq.gz; do\n\n    # Extract sample name from the file name\n    SAMPLE=$(echo ${fastq_file} | sed \"s/.fastq.gz//\")\n    echo ${SAMPLE}.fastq.gz \n</code></pre> <p>How does this maintain sample naming?</p> <ul> <li>We define the variable <code>SAMPLE</code> as the sample name by using the <code>sed</code> command to remove <code>.fastq.gz</code> from each filename.<ul> <li>This allows us to create a variable that effectively stores a list of all sample names.</li> <li>For example, if the filename was <code>WT-REP1.fastq.gz</code>:<ul> <li>The <code>sed</code> command removes <code>.fastq.gz</code>, leaving just <code>WT-REP1</code>.</li> <li>When we <code>echo $SAMPLE</code>, it returns <code>WT-REP1</code> instead of the full filename.</li> </ul> </li> </ul> </li> <li>Using <code>echo</code>, we can check that each sample name is correctly extracted.</li> </ul> </li> <li> <p>Next, copy-and-paste the following section into lines 18-21. Make sure to use the <code>tab</code> key to indent. </p> <pre><code># Set database directory, genome, and processor count\nDBDIR=\"/gpfs1/cl/mmg3320/course_materials/genome_index/hisat2_index_mm10\"\nGENOME=\"GRCm39\"\np=2\n</code></pre> <p>All of these are examples of variables. Each variable is assigned a specific value:</p> <ul> <li><code>DBDIR</code> stores a file path to the genome index directory.</li> <li><code>GENOME</code> stores a character string representing the genome version (GRCm39).</li> <li><code>p</code> is set to a numeric value (2), which represents the number of processors to use.</li> </ul> <p>Variables help make the script more flexible and easier to modify by centralizing important parameters.</p> </li> <li> <p>Next, copy-and-paste the following section into lines 23-26. The script will not run unless the required modules are loaded. </p> <pre><code># Load required modules\nmodule load gcc/13.3.0-xp3epyt\nmodule load hisat2/2.2.1-x7h4grf\nmodule load samtools/1.19.2-pfmpoam\n</code></pre> </li> <li> <p>Next, copy-and-paste the following section into lines 28-33. </p> <pre><code># Align reads to the reference genome\nhisat2 \\\n    -p ${p} \\\n    -x ${DBDIR}/${GENOME} \\\n    -U ${SAMPLE}.fastq.gz \\\n    -S ${SAMPLE}.sam &amp;&gt; ${SAMPLE}.log\n</code></pre> </li> <li> <p>Move into your terminal tab. Do not close your script, we will return to it shortly. </p> </li> <li> <p>Run the following command in your terminal. </p> <pre><code>hisat2 --help\n</code></pre> <ul> <li>What does <code>-U</code> mean? </li> <li>What does <code>-S</code> mean? </li> <li>What does <code>-x</code> mean? </li> </ul> <p>Its really important that you take some time to understand what these parameters do!</p> </li> <li> <p>When you are ready, modify the script you copied-and-pasted to accommodate paired end reads </p> <p>The paired-end files in this directory are: </p> <pre><code>JC1A_R1.fastq.gz  JC1A_R2.fastq.gz\n</code></pre> <p>Hints:</p> <ul> <li>The loop should iterate only over <code>_1.fastq.gz</code> files to ensure proper pairing. If the script loops over all <code>*.fastq.gz</code> files, it will process both <code>_1.fastq.gz</code> and <code>_2.fastq.gz</code> files individually.</li> <li>If the <code>SAMPLE</code> variable is set to <code>.fastq.gz</code> only, the samples will not distinguish between <code>_1.fastq.gz</code> and <code>_2.fastq.gz</code></li> <li>Use <code>_1.fastq.gz</code> for forward reads and <code>_2.fastq.gz</code> for reverse reads.</li> </ul> </li> <li> <p>Copy-and paste the last section starting in line 36. When you are done, submit the script. </p> <pre><code>    # Convert SAM to BAM\n    samtools view ${SAMPLE}.sam \\\n        --threads 2 \\\n        -b \\\n        -o ${SAMPLE}.bam\n\n    # Remove SAM file\n    rm ${SAMPLE}.sam\n\n    # Generate alignment statistics\n    samtools flagstat ${SAMPLE}.bam &gt; ${SAMPLE}.txt\n\n    # Sort the BAM file by coordinates\n    samtools sort ${SAMPLE}.bam -o ${SAMPLE}_sorted.bam\n\n    # Index the sorted BAM file\n    samtools index ${SAMPLE}_sorted.bam\n\n    echo \"Sample ${SAMPLE} processing complete.\"\ndone\n</code></pre> <p>The final files output should include the following: </p> <pre><code>align_CD8_60836.out  JC1A.log          JC1A_R2.fastq.gz  JC1A_sorted.bam.bai  \nJC1A.bam             JC1A_R1.fastq.gz  JC1A_sorted.bam   JC1A.txt\n</code></pre> <p>We will review these outputs during next class. </p> </li> </ol>"},{"location":"ch06/13_ClassExercise3_Index_Genome/","title":"13 ClassExercise3 Index Genome","text":""},{"location":"ch06/13_ClassExercise3_Index_Genome/#class-exercise-2-indexing-a-genomic-reference-fasta","title":"Class Exercise #2: Indexing a Genomic Reference (FASTA)","text":"<p>For this tutorial, we will be using an Organism-specific biological database called Cryptodb. These types of databases are updated more frequently than broader biological databases like Ensembl, especially for non-model organisms. As a result, they tend to be more comprehensive.</p> <p>If you are working with a non-model organism, it is important to check whether a dedicated database exists for your species, as it may provide more up-to-date and detailed information. </p> <p>The steps we will perform include: </p> <ol> <li>Making a directory called <code>indexed_genomes_example</code></li> <li>Grab the correct FASTA file from the Cryptodb website </li> <li>Load the <code>hisat2</code> program using <code>module load</code> </li> <li>Build the genome index using <code>hisat2-build</code></li> </ol>"},{"location":"ch06/13_ClassExercise3_Index_Genome/#what-is-the-purpose-of-building-a-genome-index","title":"What is the purpose of building a genome index?","text":"<p>Prior to mapping, most aligners require you to construct and index the genome, so that the aligner can quickly and efficiently retrieve reference sequence information. </p> <p>Indexing in general is widely used in bioinformatics to improve performance. This step can take a long time depending on how large the genome is (i.e. hours) so it is a good idea to generate a script. </p> <p>For this example, we will be indexing the genome of the parasite Cryptosporidium parvum, which is very small in size. </p>"},{"location":"ch06/13_ClassExercise3_Index_Genome/#instructions","title":"Instructions","text":"<ol> <li> <p>Make a directory called <code>indexed_genomes_example</code></p> </li> <li> <p>Grab the correct FASTA file from the Cryptodb website and place inside of <code>indexed_genomes_example</code>.  </p> <pre><code>https://cryptodb.org/common/downloads/Current_Release/CparvumIOWA-ATCC/fasta/data/\n</code></pre> <ul> <li>Which file will you select? Think about the TYPE of aligner we are using. </li> <li>Once you make a decision, right-click, select copy-link </li> </ul> <p>Use the <code>wget</code> command </p> <pre><code>wget path-you-copied\n</code></pre> <p>You should see the following: </p> <pre><code>Connecting to cryptodb.org (cryptodb.org)|128.192.21.13|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 9275167 (8.8M) [application/x-fasta]\nSaving to: \u2018CryptoDB-67_CparvumIOWA-ATCC_Genome.fasta\u2019\n\n100%[=================================================================&gt;] 9,275,167   5.93MB/s   in 1.5s   \n</code></pre> <p>Check the file with <code>head</code> once the download is complete. </p> <pre><code>&gt;CP044422 | organism=Cryptosporidium_parvum_IOWA-ATCC | version=2022-10-21 | length=920510 | SO=chromosome\nAAACCCCTAAACCTAAACCTAAACCTAAACCTAAACCCTAAACCTAAACCTAAACCTAAA\nCCTAAAACCTAAACCTAAAACCTAAACCTAAAACCTAAAACCTAAACCTAAACCTAAACC\nCCTAAACCTAAACCTAAACCTAAAAACCTAAACCCTAAACCTAAACCTAAAAAACCTAAA\nCCTAAACCTAAACCTAAACCTAAACCTAAACCTAAACCTAAAAAACCTAAAACCTAAACC\nTAAACCTAAACCTAAACCTAAAACCTAAAAACCTAAACCTAAAAACCTAAAAAACCTAAA\nCCTAAAAAACCTAAACCTAAACCTAAAACCTAAAACCTAAACCTAAAACCTAAAACCTAA\nACCTAAACTAAACCTAAAAAACCTAAAAACCTAAACCCCTAAACCTAAACCTAAAACCTA\nAACCTAAACCTAAACCTAAACCTAAAACCTAAACCTAAACCTAAAACCTAAACCTAAACC        \nTAAACCTAAACCTCCTAAAAACCTAAACCTAAACCTAAACCTAAACCTAAAAAACCTAAA\n</code></pre> </li> <li> <p>Load the <code>hisat2</code> program using <code>module load</code> </p> <pre><code>module load gcc/13.3.0-xp3epyt \nmodule load hisat2/2.2.1-x7h4grf \n</code></pre> </li> <li> <p>Build the genome index using the <code>hisat2-build</code> program. <code>hisat2-build</code> can index reference genomes of any size. </p> <pre><code>hisat2-build\n</code></pre> <p>Output: </p> <pre><code>Usage: hisat2-build [options]* &lt;reference_in&gt; &lt;ht2_index_base&gt;\n    reference_in            comma-separated list of files with ref sequences\n    hisat2_index_base       write ht2 data to files with this dir/basename\nOptions:\n    -c                      reference sequences given on cmd line (as\n                            &lt;reference_in&gt;)\n</code></pre> <p>To index a reference genome using hisat2-build, the basic command syntax is:</p> <pre><code>hisat2-build input-fasta-file basename\n</code></pre> <ul> <li><code>input-fasta-file</code>: The reference genome FASTA file you want to index.</li> <li><code>basename</code>: The prefix for the output index files. This is the \"name\" that will be used for all generated index files.</li> </ul> <p>For this case, use <code>CparvumIOWA_ATCC_v68</code> as the basename</p> </li> </ol>"},{"location":"ch06/13_ClassExercise3_Index_Genome/#final-ouput","title":"Final Ouput","text":"<p>Small indexes are stored in files with the <code>.ht2</code> extension. These files together constitute the index: they are all that is needed to align reads to that reference. The original sequence FASTA files is no longer used by <code>HISAT2</code> once the index is built.</p> <p>The final output should look similar to below once complete: </p> <pre><code>CparvumIOWA_ATCC_v68.1.ht2  CparvumIOWA_ATCC_v68.6.ht2\nCparvumIOWA_ATCC_v68.2.ht2  CparvumIOWA_ATCC_v68.7.ht2\nCparvumIOWA_ATCC_v68.3.ht2  CparvumIOWA_ATCC_v68.8.ht2\nCparvumIOWA_ATCC_v68.4.ht2  \nCparvumIOWA_ATCC_v68.5.ht2\n</code></pre> <p>Check that the file sizes are correct:</p> <pre><code>-rw-r--r-- 1 pdrodrig pi-jdragon 7.0M Feb 19 11:45 CparvumIOWA_ATCC_v68.1.ht2\n-rw-r--r-- 1 pdrodrig pi-jdragon 2.2M Feb 19 11:45 CparvumIOWA_ATCC_v68.2.ht2\n-rw-r--r-- 1 pdrodrig pi-jdragon   80 Feb 19 11:45 CparvumIOWA_ATCC_v68.3.ht2\n-rw-r--r-- 1 pdrodrig pi-jdragon 2.2M Feb 19 11:45 CparvumIOWA_ATCC_v68.4.ht2\n-rw-r--r-- 1 pdrodrig pi-jdragon 3.9M Feb 19 11:45 CparvumIOWA_ATCC_v68.5.ht2\n-rw-r--r-- 1 pdrodrig pi-jdragon 2.3M Feb 19 11:45 CparvumIOWA_ATCC_v68.6.ht2\n-rw-r--r-- 1 pdrodrig pi-jdragon   12 Feb 19 11:45 CparvumIOWA_ATCC_v68.7.ht2\n-rw-r--r-- 1 pdrodrig pi-jdragon    8 Feb 19 11:45 CparvumIOWA_ATCC_v68.8.ht2\n</code></pre>"},{"location":"ch06/tutorial-bash/","title":"Tutorial bash","text":"<pre><code>qiime fragment-insertion sepp \\\n  --i-representative-sequences ./dada2_rep_set.qza \\\n  --i-reference-database sepp-refs-gg-13-8.qza \\\n  --o-tree ./tree.qza \\\n  --o-placements ./tree_placements.qza \\\n  --p-threads 1  # update to a higher number if you can\n</code></pre> <p>Note: for this do x, y, z</p> <p>Download Visual Studio Code : https://code.visualstudio.com/ </p>"},{"location":"ch07/14_installing-RSeQC-using-conda/","title":"14 installing RSeQC using conda","text":""},{"location":"ch07/14_installing-RSeQC-using-conda/#installing-rseqc-using-conda","title":"Installing RSeQC using Conda","text":"<p>Follow these steps to install RSeQC using Conda in your VACC account.</p>"},{"location":"ch07/14_installing-RSeQC-using-conda/#step-1-load-python","title":"Step 1. Load Python","text":"<p>Before installing Conda and RSeQC, load the appropriate Python module:</p> <pre><code>module load python3.10-anaconda/2023.03-1 \n</code></pre>"},{"location":"ch07/14_installing-RSeQC-using-conda/#step-2-verify-the-loaded-module","title":"Step 2: Verify the Loaded Module","text":"<p>Check that the Python module is correctly loaded by listing active modules:</p> <pre><code>module list\n</code></pre> <p>Expected output:</p> <pre><code>Currently Loaded Modules:\n  1) python3.10-anaconda/2023.03-1\n</code></pre>"},{"location":"ch07/14_installing-RSeQC-using-conda/#step-3-confirm-python-and-conda-installation","title":"Step 3: Confirm Python and Conda Installation","text":"<p>Verify that Python and Conda are available:</p> <pre><code>python --version\nconda --version\n</code></pre> <p>Expected output:</p> <pre><code>Python 3.10.9\nconda 22.9.0\n</code></pre>"},{"location":"ch07/14_installing-RSeQC-using-conda/#step-4-initialize-conda-for-your-shell","title":"Step 4: Initialize Conda for Your Shell","text":"<p>Run the following command to initialize Conda for your specific shell:</p> <p><code>conda init</code> only needs to be run once after installing Conda. If the user has already initialized Conda, they don\u2019t need to run it again.</p> <pre><code>conda init\n</code></pre> <p>Then reload your shell configuration. </p><pre><code>exec $SHELL\n</code></pre>"},{"location":"ch07/14_installing-RSeQC-using-conda/#step-5-create-a-conda-environment","title":"Step 5: Create a Conda Environment","text":"<p>Create a dedicated Conda environment for <code>RSeQC</code> with Python 3.10:</p> <pre><code>conda create -n rseqc_env python=3.10 -y\n</code></pre> <p>During installation, you may see messages like:</p> <pre><code>Collecting package metadata (current_repodata.json): done\nSolving environment: done\n</code></pre> <p>Conda will check for required dependencies. If prompted, type y to proceed:</p> <pre><code>Proceed ([y]/n)? y \n</code></pre> <ul> <li><code>y</code> confirms the installation of required dependencies</li> <li><code>n</code> cancels the installation</li> </ul> <p>At the end, you will see a message like this:</p> <pre><code>Preparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n#\n# To activate this environment, use\n#\n#     $ conda activate rseqc_env\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n\nRetrieving notices: ...working... done\n</code></pre> <p>Now, proceed to Step 5.</p>"},{"location":"ch07/14_installing-RSeQC-using-conda/#step-6-activate-the-rseqc_env-environment","title":"Step 6: Activate the <code>rseqc_env</code> Environment","text":"<p>Activate the newly created Conda environment:</p> <pre><code>conda activate rseqc_env\n</code></pre> <p>Once activated, your terminal prompt will change from (base) to (rseqc_env), indicating that you are inside the new environment.</p> <pre><code>[pdrodrig@vacc-login4 pdrodrig]$ conda activate rseqc_env \n(rseqc_env) [pdrodrig@vacc-login4 pdrodrig]\n</code></pre>"},{"location":"ch07/14_installing-RSeQC-using-conda/#step-7-install-rseqc-using-pip","title":"Step 7: Install <code>RSeQC</code> Using <code>pip</code>","text":"<p>With the environment activated, install RSeQC using pip:</p> <pre><code>pip install RSeQC\n</code></pre> <p>Once installed successfully, you should see output similar to:</p> <pre><code>Successfully installed RSeQC-5.0.4 bx-python-0.13.0 \n</code></pre>"},{"location":"ch07/14_installing-RSeQC-using-conda/#step-8-verify-the-installation","title":"Step 8: Verify the Installation","text":"<p>Test the installation by displaying the help page for read_distribution.py:</p> <pre><code>read_distribution.py --help\n</code></pre> <p>Expected output:</p> <pre><code>Usage: read_distribution.py [options]\n\nCheck reads distribution over exon, intron, UTR, intergenic ... etc\nThe following reads will be skipped:\n    qc_failed\n    PCR duplicate\n    Unmapped\n    Non-primary (or secondary)\n</code></pre> <p>Congratulations! You have successfully installed RSeQC!</p>"},{"location":"ch07/14_installing-RSeQC-using-conda/#important-notes","title":"Important Notes","text":"<ul> <li>Each time you use RSeQC, you must first activate the Conda environment:</li> </ul> <pre><code>conda activate rseqc_env\n</code></pre> <ul> <li> <p>RSeQC will not work in the <code>base</code> environment.</p> </li> <li> <p>To deactivate the environment after use, run:</p> </li> </ul> <pre><code>conda deactivate\n</code></pre>"},{"location":"ch07/14_instructions-for-course-catchup/","title":"14 instructions for course catchup","text":""},{"location":"ch07/14_instructions-for-course-catchup/#instructions-for-monday-february-24th","title":"Instructions for Monday, February 24th","text":"<p>For today\u2019s class, we will use this session as a course catch-up day. If you can answer \u201cyes\u201d to all the following questions, you do not need to attend class as no new materials will be discussed:</p> <ol> <li>Were you able to download the publicly available dataset for your final project? (Note: not required yet but many have tried and are having trouble)</li> <li>Do you feel confident copying data from the shared course directory during class?</li> <li>Are you confident submitting scripts using the SLURM job submission system?</li> <li>Can you transfer files between the VACC and your local desktop?</li> <li>Are you able to read the sections below (before Wednesday's class) to understand what <code>conda</code> and <code>RSeQC</code> are and how these programs are used? </li> <li>Can you follow the instructions provided in the next lesson (L14) to independently install the program <code>RSeQC</code> using <code>conda</code>? </li> </ol> <p>In Wednesday\u2019s class, we will continue using <code>RSeQC</code> and reviewing outputs from <code>HISAT2</code>. Since <code>RSeQC</code> is not available via <code>module load</code>, you must install the program beforehand. You will not be able to participate in Wednesday class tutorial without installing the program first. </p> <p>Below is background information on the <code>RSeQC</code> and <code>conda</code> programs. Step-by-step instructions are provided in the next lesson (L14). Do not attempt to run the code shown below; it is for reading purposes only.</p> <p>Class Survey</p> <p>Please fill out this google form</p>"},{"location":"ch07/14_instructions-for-course-catchup/#introducing-conda","title":"Introducing Conda","text":"<p>Conda is an open-source package management system and environment management system that runs on Windows, macOS, and Linux. Conda quickly installs, runs, and updates packages and their dependencies. Conda also easily creates, saves, loads, and switches between environments on your local computer. Conda was created for Python programs but it can package and distribute software for any language.</p> <p>More information can be found here. </p>"},{"location":"ch07/14_instructions-for-course-catchup/#managing-environments-with-conda","title":"Managing environments with Conda","text":"<p>Conda allows you to create separate environments containing files, packages, and their dependencies, ensuring they do not interfere with one another. By default, you already have a base environment called base. This environment serves as your \"home base,\" and any packages installed here will always be accessible, regardless of your location on the server.</p> <p>However, installing all programs in the base environment can lead to software conflicts. Instead, it is best practice to create a separate environment for each program. For example, <code>RSeQC</code> recommends using Python 3.10, but your default Python version may be different. To avoid compatibility issues, we will create a dedicated environment for <code>RSeQC</code>.</p>"},{"location":"ch07/14_instructions-for-course-catchup/#creating-a-conda-environment","title":"Creating a Conda Environment","text":"<p>To create a new environment with Conda, use the following command:</p> <pre><code>conda create -n new-env\n</code></pre> <ul> <li><code>create</code> - Tells Conda to create a new environment.</li> <li><code>-n new-env</code> - Specifies the name of the environment (new-env in this case). You can change \"new-env\" to any descriptive name (e.g., rseqc_env) for each environment you create. </li> </ul> <p>Example shown below:</p> <pre><code>conda create -n rseqc_env python=3.10\n</code></pre>"},{"location":"ch07/14_instructions-for-course-catchup/#handling-dependencies","title":"Handling Dependencies","text":"<p>Conda will check if additional packages (\"dependencies\") are required. If prompted, type 'y' to proceed:</p> <pre><code>Proceed ([y]/n)? y \n</code></pre> <ul> <li><code>y</code> confirms the installation of required dependencies</li> <li><code>n</code> cancels the installation</li> </ul>"},{"location":"ch07/14_instructions-for-course-catchup/#activating-the-environment","title":"Activating the Environment","text":"<p>After creating the environment, activate it with: </p> <pre><code>conda activate new-env\n</code></pre> <ul> <li><code>activate</code> - Switches from the <code>base</code> environment to the newly created one</li> <li><code>new-env</code> - Name of environment to activate (must match what you used in <code>-n new-env</code>)</li> </ul> <p>Once activated, your prompt will change from <code>(base)</code> to <code>(new-env)</code>, indicating that you are now working inside the new environment. </p>"},{"location":"ch07/14_instructions-for-course-catchup/#installing-a-package","title":"Installing a Package","text":"<p>Now you will be able to install the desired package inside the environment you just created. </p> <pre><code>conda install -c bioconda multiqc\n</code></pre> <ul> <li><code>install</code> installs a package </li> <li><code>-c bioconda</code> specifies the channel. <code>bioconda</code> hosts many bioinformatics tools. </li> <li><code>multiqc</code> is the name of package being installed in this example. </li> </ul>"},{"location":"ch07/14_instructions-for-course-catchup/#deactivating-the-environment","title":"Deactivating the Environment","text":"<p>When you are done using the environment, deactivate it using:</p> <pre><code>conda deactivate \n</code></pre> <ul> <li><code>deactivate</code> exits the current environment and returns to the <code>base</code> environment </li> </ul>"},{"location":"ch07/14_instructions-for-course-catchup/#introducing-rseqc","title":"Introducing <code>RSeQC</code>","text":"<p><code>RSeQC</code> (RNA-Seq Quality Control) is a widely used toolkit designed to evaluate the quality of RNA sequencing (RNA-Seq) data. It provides a collection of scripts to assess the integrity of sequencing reads, alignment quality, gene body coverage, GC content distribution, and other essential metrics. By identifying potential biases or errors in RNA-Seq data,<code>RSeQC</code> helps ensure accurate downstream analysis and interpretation.  </p> <p>This toolkit is particularly useful for verifying the quality of mapped reads from aligners such as <code>HISAT2</code>, <code>STAR</code>, or <code>TopHat</code>. </p> <p>Commonly used features of RSeQC include:  </p> <ul> <li>Read Distribution Analysis: Determines the proportion of reads mapping to different genomic features (exons, introns, intergenic regions).  </li> <li>Gene Body Coverage: Evaluates uniformity of read distribution across genes.  </li> <li>Junction Annotation: Identifies and annotates splice junctions.  </li> <li>GC Content and Read Duplication Metrics: Helps detect sequencing biases.  </li> </ul> <p>Since <code>RSeQC</code> is not typically available as a module on high-performance computing (HPC) systems, users need to install it manually using Conda and pip. The following tutorial, will guide you through installing <code>RSeQC</code>.</p> <p>More information on <code>RseQC</code> can be found here. </p>"},{"location":"ch08/16_RSeQC/","title":"16 RSeQC","text":""},{"location":"ch08/16_RSeQC/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Use of RSeQC (a package of scripts) to evaluate the quality of RNA-Seq data</li> </ul>"},{"location":"ch08/16_RSeQC/#recap-from-last-week","title":"Recap from last week","text":"<p>HISAT2 produces multiple output files: </p> <ul> <li><code>.bam</code>: unsorted bam file</li> <li><code>sorted.bam</code>: sorted bam file</li> <li><code>.bam.bai</code>: index for sorted bam file </li> <li><code>.log</code>: log file with containing detailed information about the run. This file is most useful for troubleshooting and debugging. </li> </ul>"},{"location":"ch08/16_RSeQC/#planning-and-organization","title":"Planning and Organization","text":"<p>For each experiment you work on and analyze data for, it is considered best practice to get organized by creating a planned storage space (directory structure).</p> <p>The outputs from HISAT2 were many and looked like this:</p> <pre><code>Irrel_kd_1.subset.bam             Irrel_kd_2.subset_sorted.bam.bai  Mov10_oe_1.subset.bam             Mov10_oe_2.subset_sorted.bam.bai\nIrrel_kd_1.subset.log             Irrel_kd_2.subset.txt             Mov10_oe_1.subset.log             Mov10_oe_2.subset.txt\nIrrel_kd_1.subset_sorted.bam      Irrel_kd_3.subset.bam             Mov10_oe_1.subset_sorted.bam      Mov10_oe_3.subset.bam\nIrrel_kd_1.subset_sorted.bam.bai  Irrel_kd_3.subset.log             Mov10_oe_1.subset_sorted.bam.bai  Mov10_oe_3.subset.log\nIrrel_kd_1.subset.txt             Irrel_kd_3.subset_sorted.bam      Mov10_oe_1.subset.txt             Mov10_oe_3.subset_sorted.bam\nIrrel_kd_2.subset.bam             Irrel_kd_3.subset_sorted.bam.bai  Mov10_oe_2.subset.bam             Mov10_oe_3.subset_sorted.bam.bai\nIrrel_kd_2.subset.log             Irrel_kd_3.subset.txt             Mov10_oe_2.subset.log             Mov10_oe_3.subset.txt\nIrrel_kd_2.subset_sorted.bam                                        Mov10_oe_2.subset_sorted.bam\n</code></pre> <p>These data outputs have been organized into the following folders: </p> <pre><code>RSeQC_exercise\n  \u251c\u2500\u2500 bams\n  \u251c\u2500\u2500 logs\n  \u251c\u2500\u2500 RSeQC_bed_files\n  \u251c\u2500\u2500 scripts\n  \u2514\u2500\u2500 sorted_bams\n</code></pre> <ul> <li><code>logs</code>: used to keep track of the commands run and the specific parameters used, but also to have a record of any standard output that is generated while running the command.</li> <li><code>RSeQC_bed_files</code>: contain BED12 files created from GTF file </li> </ul>"},{"location":"ch08/16_RSeQC/#rseqc","title":"RSeQC","text":"<p>Today we will be using RSeQC. RSeQC is a package of scripts designed to evaluate the quality of RNA-seq data. We will run the following: <code>bam_stat.py</code>, <code>infer_experiment.py</code>, <code>read_distribution.py</code>.</p> <p>The majority of RSeQC scripts generate output files which can be plotted and summarized in the MultiQC report.</p>"},{"location":"ch08/16_RSeQC/#getting-started","title":"Getting Started","text":"<p>Last week we installed <code>RSeQC</code> using conda. To load RSeQC, we will need to perform:</p> <p></p><pre><code>conda activate rseqc_env\n</code></pre> Once activated, your terminal prompt will change from (base) to (rseqc_env), indicating that you are inside the new environment. <p>To test that everything is in working order we can run:</p> <pre><code>infer_experiment.py --help\n</code></pre> <p>Expected Output: </p> <pre><code>Usage: infer_experiment.py [options]\nOptions:\n  --version             show program's version number and exit\n  -h, --help            show this help message and exit\n  -i INPUT_FILE, --input-file=INPUT_FILE\n                        Input alignment file in SAM or BAM format\n  -r REFGENE_BED, --refgene=REFGENE_BED\n                        Reference gene model in bed fomat.\n  -s SAMPLE_SIZE, --sample-size=SAMPLE_SIZE\n                        Number of reads sampled from SAM/BAM file.\n                        default=200000\n  -q MAP_QUAL, --mapq=MAP_QUAL\n                        Minimum mapping quality (phred scaled) for an\n                        alignment to be considered as \"uniquely mapped\".\n                        default=30\n(rseqc_env) [pdrodrig@vacc-login4 RSeQC_exercise]$ \n</code></pre> <p>Download dataset for today's lesson:</p> <pre><code>/gpfs1/cl/mmg3320/course_materials/RSeQC_exercise\n</code></pre>"},{"location":"ch08/16_RSeQC/#strand-specificity","title":"Strand-Specificity","text":"<p>Purpose: This script predicts the \u201cstrandedness\u201d of the protocol (i.e. unstranded, sense or antisense) that was used to prepare the sample for sequencing by assessing the orientation in which aligned reads overlay gene features in the reference genome. This information is not always available in public datasets. </p> <p>Why use it?:</p> <ul> <li>Crucial for differential expression analysis, as strand-specific libraries require correct orientation for accurate read assignment. </li> <li>Avoids misinterpretation of gene expression levels caused by incorrect strand assignment </li> </ul> <p>Basic Usage:</p> <pre><code>infer_experiment.py -r ref.bed -i input.bam\n</code></pre> <ul> <li>-i input.bam -&gt; specifies the sorted BAM file with aligned RNA-Seq reads </li> <li>-r ref.bed -&gt; specifies the BED file containing gene annotation (in BED12 format)</li> </ul> <p>Expected Output:</p> <ul> <li><code>.infer_experiment.txt</code>: File containing fraction of reads mapping to given strandedness configurations.</li> </ul> <p>Class Exercise #1: <code>infer_experiment.py</code></p> <ul> <li>Create a directory within RSeQC_exercise called <code>IKD1_RSeQC</code> </li> <li>Move <code>Irrel_kd_1.subset_sorted.bam</code> and <code>Irrel_kd_1.subset_sorted.bam.bai</code> into <code>IKD1_RSeQC</code></li> <li>Within the <code>RSeQC_exercise/IKD1_RSeQC</code> folder run <code>infer_experiment.py</code> on <code>Irrel_kd_1.subset_sorted.bam</code>. </li> <li>Use the hg38 bed12 file </li> <li>Be sure to redirect the final output to <code>Irrel_kd_1.subset_infer.txt</code>. You will need this file for our final step.  </li> </ul>"},{"location":"ch08/16_RSeQC/#bam-statistics","title":"BAM Statistics","text":"<p>Purpose: Provides a summary of the alignment results from a BAM file </p> <p>Why use it?: </p> <ul> <li>Quickly checks overall alignment quality</li> <li>Reports metrics such as total reads, mapped reads, properly paired reads (if appropriate), and uniquely mapped reads </li> <li>Helps identify issues like low mapping rates </li> </ul> <p>Basic Usage:</p> <pre><code>bam_stat.py -i input.bam\n</code></pre> <ul> <li>-i input.bam -&gt; specifies the sorted BAM file with aligned RNA-Seq reads </li> </ul> <p>Expected Output:</p> <ul> <li>Summary statistics of the BAM file, including:<ul> <li>Total number of reads</li> <li>Mapped reads</li> <li>Properly paired reads</li> <li>Uniquely mapped reads</li> <li>Reads mapped to multiple loci</li> </ul> </li> </ul> <p>Interpretation:</p> <ul> <li>High mapping rates (~95%) are expected for well-prepared RNA-Seq libraries.</li> <li>A low mapping rate (&lt;80%) could indicate issues with genome annotation or contamination.</li> </ul> <p>Class Exercise #2: <code>bam_stat.py</code></p> <ul> <li>Within the <code>RSeQC_exercise/IKD1_RSeQC</code> folder run <code>bam_stat.py</code> on <code>Irrel_kd_1.subset_sorted.bam</code>. </li> <li>Be sure to redirect the final output to <code>Irrel_kd_1.subset_bamstat.txt</code>. You will need this file for our final step. </li> </ul>"},{"location":"ch08/16_RSeQC/#splice-junction-detection","title":"Splice Junction Detection","text":"<p>Purpose: Used to assess the sequencing depth and completeness of splice junction detection in RNA-Seq data. It helps to determine whether an RNA-Seq experiment has sufficient coverage to detect splice junctions reliably. </p> <p>Why use it?: </p> <ul> <li>Aids in determining if additional sequencing would improve splice junction discovery</li> <li>Ensures that the experiment is capturing enough splice junctions for meaningful downstream analysis</li> </ul> <p>Basic Usage:</p> <pre><code>junction_saturation.py -i input.bam -r reference.bed -o output_prefix\n</code></pre> <ul> <li>-i input.bam -&gt; specifies the sorted BAM file with aligned RNA-Seq reads </li> <li>-r reference BED file with gene annotation</li> <li>-o output_prefix -&gt; prefix of output files</li> </ul> <p>Expected Output:</p> <ul> <li>PDF file of Junction Saturation Plot; helps you assess whether sequencing depth is sufficient or if more reads are needed to discover new junctions </li> <li>The .r (R script) used to generate the PDF file </li> </ul> <p>Interpretation:</p> <ul> <li>X-axis -&gt; Read depth (number of reads supporting a junction).</li> <li>Y-axis -&gt;  Number of detected splice junctions.</li> <li>Curve shape:<ul> <li>If the curve plateaus -&gt; You have sufficient sequencing depth; additional reads won\u2019t improve junction detection.</li> <li>If the curve keeps increasing -&gt; More sequencing might still reveal new junctions.</li> </ul> </li> </ul> <p>Class Exercise #3: <code>junction_saturation.py</code></p> <ul> <li>Within the <code>RSeQC_exercise/IKD1_RSeQC</code> folder run <code>junction_saturation.py</code> on <code>Irrel_kd_1.subset_sorted.bam</code>. </li> <li>Use <code>IKD1-output</code> as the output_prefix</li> <li>Do not generate a .txt file for this step</li> </ul>"},{"location":"ch08/16_RSeQC/#splice-junction-annotation","title":"Splice Junction Annotation","text":"<p>Purpose: Identifies and annotates splice junctions by comparing detected junctions to known junctions in a reference genome annotation (GTF/GFF)</p> <p>Why use it?: </p> <ul> <li>Validates detected splice sites against known annotations </li> <li>Detects novel splice junctions, which may indicate alternative splicing or sequencing errors </li> <li>Identifies unexpected splicing patterns </li> </ul> <p>Basic Usage:</p> <pre><code>junction_annotation.py -i input.bam -o output -r reference.bed\n</code></pre> <ul> <li>-i input.bam -&gt; specifies the sorted BAM file with aligned RNA-Seq reads </li> <li>-r reference.bed -&gt; specifies the BED file containing gene annotation (in BED12 format)</li> <li>-o output_prefix -&gt; prefix of output files</li> </ul> <p>Expected Output: </p> <ul> <li>A summary of detected splice junctions, categorized into:<ul> <li>Known (found in the reference)</li> <li>Novel (newly identified)</li> <li>Unannotated (potential mapping issues)</li> </ul> </li> </ul> <p>Interpretation:</p> <ul> <li>A high fraction of novel junctions may suggest alternative splicing events.</li> <li>Too many unannotated junctions may indicate alignment problems.</li> </ul> <p>Class Exercise #4: <code>junction_annotation.py</code></p> <ul> <li>Within the <code>RSeQC_exercise/IKD1_RSeQC</code> folder run <code>junction_annotation.py</code> on <code>Irrel_kd_1.subset_sorted.bam</code>. </li> <li>Use <code>IKD1-output</code> as the output_prefix</li> <li>Be sure to redirect the final output to <code>Irrel_kd_1.subset_janno.txt</code>. You will need this file for our final step. </li> </ul>"},{"location":"ch08/16_RSeQC/#read-distribution-across-genomic-features-class-exercise-5","title":"Read Distribution Across Genomic Features (Class Exercise #5)","text":"<p>Purpose: Analyzes how reads are distributed across different genomic features (e.g., exons, introns, UTRs, intergenic regions).</p> <p>Why use it?: </p> <ul> <li>Ensures the expected proportion of reads falls within exons for RNA-Seq experiments</li> </ul> <p>Basic Usage:</p> <pre><code>read_distribution.py -i input.bam -r ref.bed\n</code></pre> <ul> <li>-i input.bam -&gt; specifies the sorted BAM file with aligned RNA-Seq reads </li> <li>-r reference.bed -&gt; specifies the BED file containing gene annotation (in BED12 format)</li> </ul> <p>Expected Output: </p> <ul> <li>The fraction of reads mapping to different genomic regions, including:<ul> <li>Exons</li> <li>Introns</li> <li>Intergenic regions</li> <li>5' and 3' UTRs</li> </ul> </li> </ul> <p>Interpretation:</p> <ul> <li>For RNA-Seq, most reads (&gt;70%) should align to exons.</li> <li>A high proportion of intronic reads (&gt;30%) may indicate rRNA contamination or unprocessed RNA.</li> </ul> <p>Class Exercise #5: <code>read_distribution.py</code></p> <ul> <li>Within the <code>RSeQC_exercise/IKD1_RSeQC</code> folder run <code>read_distribution.py</code> on <code>Irrel_kd_1.subset_sorted.bam</code>. </li> <li>Be sure to redirect the final output to <code>Irrel_kd_1.subset_read.txt</code> You will need this file for our final step. </li> </ul>"},{"location":"ch08/16_RSeQC/#final-step-run-multiqc","title":"Final Step: Run MultiQC","text":"<ol> <li> <p>Navigate to top <code>RSeQC_exercise</code> folder. Make sure you are not in any of the subdirectories folders. </p> </li> <li> <p>Load <code>py-multiqc/1.15-fmpaaj7</code></p> </li> <li> <p>Run multiqc. Be sure to name the output file <code>RSeQC_stats-multiqc</code></p> </li> <li> <p>Take a look at the generated HTML file. </p> </li> </ol>"},{"location":"ch09/17_counting_reads/","title":"17 counting reads","text":""},{"location":"ch09/17_counting_reads/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>Conversion of BAM file to a counts file using <code>htseq-count</code></li> <li>Conversion of BAM file to a bigWig file using <code>deeptools</code></li> </ul>"},{"location":"ch09/17_counting_reads/#counting-reads-as-a-measure-of-gene-expression","title":"Counting reads as a measure of gene expression","text":"<p>Once we have our reads aligned to the genome, the next step is to count how many reads have mapped to each gene. Many tools can take BAM files as input and output the number of reads (counts) associated with each feature of interest (genes, exons, transcripts, etc.). Two commonly used counting tools are featureCounts and htseq-count.</p> <ul> <li> <p>These tools report raw counts, meaning they only count reads that map uniquely to a single location in the genome. They are most effective for counting at the gene level. In this approach, the total read count for a gene (meta-feature) is the sum of reads assigned to each of its exons (features).</p> </li> <li> <p>Some other tools account for multiple transcripts per gene, assigning fractional counts instead of whole numbers. For example, if one read aligns to two transcripts, it may be counted as 0.5 for each transcript rather than a whole number.</p> </li> <li> <p>There are also tools that count multimapping reads, but this approach can lead to overcounting, which affects normalization and ultimately compromises the accuracy of differential gene expression analysis.</p> </li> </ul> <p>Input for counting = multiple BAM files + 1 GTF file</p> <p>Simply speaking, the genomic coordinates of where the read is mapped (BAM) are cross-referenced with the genomic coordinates of whichever feature you are interested in counting expression of (GTF), it can be exons, genes or transcripts.</p> <p>Output of counting = A count matrix, with genes as rows and samples are columns</p> <p>These are the \"raw\" counts and will be used in statistical programs downstream for differential gene expression. Below is a representative counts matrix. </p> <p>We will begin by demonstrating the usage of HTSeq. HTSeq-count is a command-line tool used in RNA-Seq data analysis to count the number of sequencing reads that align to specific genomic features, such as genes. It is part of the HTSeq Python package and is commonly used in differential gene expression analysis pipelines.</p>"},{"location":"ch09/17_counting_reads/#basic-workflow","title":"Basic Workflow","text":""},{"location":"ch09/17_counting_reads/#input-files","title":"Input Files:","text":"<ul> <li>SAM/BAM file: Contains aligned RNA-Seq reads (output from tools like HISAT2 or STAR).</li> <li>GTF/GFF file: Contains gene annotations specifying genomic coordinates.</li> </ul>"},{"location":"ch09/17_counting_reads/#counting-reads","title":"Counting Reads:","text":"<ul> <li>HTSeq-count assigns each read to a genomic feature (usually a gene) based on predefined rules.</li> <li>It outputs a table listing each gene and the number of reads assigned to it.</li> </ul>"},{"location":"ch09/17_counting_reads/#output","title":"Output:","text":"<ul> <li>A tab-delimited text file with two columns:<ul> <li>Gene ID</li> <li>Read count</li> </ul> </li> </ul>"},{"location":"ch09/17_counting_reads/#counting-reads-in-features-with-htseq-count","title":"Counting reads in features with <code>htseq-count</code>","text":"<p>Given a file with aligned sequencing reads and a list of genomic features, a common task is to count how many reads map to each feature.</p> <p>Here, a feature refers to a specific interval (i.e., a range of positions) on a chromosome or a union of such intervals.</p> <p>Since our example data comes from an RNA-Seq experiment, we aim to count how many reads fall within the exonic regions of each gene. To do this, we first need information about exon positions, which can be obtained from GTF files\u2014a common format provided by Ensembl (available here).</p> <p>Special care must be taken when handling reads that align to or overlap with multiple features. The htseq-count script offers three different modes to handle such cases.</p> <ol> <li> <p>Union (Recommended for most cases):</p> <ul> <li>A read is assigned to a feature if any part of the read overlaps with it.</li> <li>If a read overlaps multiple features, it is not counted at all (to avoid ambiguity).</li> </ul> </li> <li> <p>Intersection-strict:</p> <ul> <li>A read is assigned to a feature only if every position of the read overlaps with that feature.</li> <li>If a read overlaps multiple features but not completely within one, it is not counted.</li> </ul> </li> <li> <p>Intersection-nonempty:</p> <ul> <li>A read is assigned to a feature only if it overlaps with at least one feature at every position.</li> <li>Unlike intersection-strict, it ignores positions where no features are present.</li> </ul> </li> </ol> <p>The figure below illustrates the effect of these three modes: </p>"},{"location":"ch09/17_counting_reads/#running-htseq-count","title":"Running <code>htseq-count</code>","text":"<p>Begin by loading the <code>htseq-count</code> module with <code>module load</code></p> <pre><code>module load gcc/13.3.0-xp3epyt \npy-htseq/2.0.3-mb7ap7s\n</code></pre> <p>Run the following to check the module is loaded: </p> <pre><code>htseq-count --help\n</code></pre> <pre><code>usage: htseq-count [-h] [--version] [-f {sam,bam,auto}] [-r {pos,name}] [--max-reads-in-buffer MAX_BUFFER_SIZE] [-s {yes,no,reverse}] [-a MINAQUAL]\n                   [-t FEATURE_TYPE] [-i IDATTR] [--additional-attr ADDITIONAL_ATTRIBUTES] [--add-chromosome-info]\n                   [-m {union,intersection-strict,intersection-nonempty}] [--nonunique {none,all,fraction,random}]\n                   [--secondary-alignments {score,ignore}] [--supplementary-alignments {score,ignore}] [-o SAMOUTS] [-p {SAM,BAM,sam,bam}]\n                   [-d OUTPUT_DELIMITER] [-c OUTPUT_FILENAME] [--counts-output-sparse] [--append-output] [-n NPROCESSES] [--feature-query FEATURE_QUERY]\n                   [-q] [--with-header]\n                   samfilenames [samfilenames ...] featuresfilename\nThis script takes one or more alignment files in SAM/BAM format and a feature file in GFF format and calculates for each feature the number of reads\nmapping to it. See http://htseq.readthedocs.io/en/master/count.html for details.\n</code></pre>"},{"location":"ch09/17_counting_reads/#basic-command-syntax","title":"Basic Command Syntax","text":"<pre><code>htseq-count -f bam -s no -i gene_id sample1.bam genes.gtf &gt; gene_counts.txt\n</code></pre> <ul> <li><code>-f bam</code> \u2192 Specifies input format (can be <code>sam</code> or <code>bam</code>).</li> <li><code>-s no</code> \u2192 Defines strandedness (<code>yes</code>, <code>no</code>, or <code>reverse</code>).</li> <li><code>-i gene_id</code> \u2192 used to identify the GTF feature attribute (<code>gene_name</code> or <code>gene_id</code>)</li> <li><code>aligned_reads.bam</code> \u2192 Input file with mapped reads.</li> <li><code>genes.gtf</code> \u2192 Reference annotation file.</li> <li><code>&gt; gene_counts.txt</code> \u2192 Redirects output to a file.</li> </ul>"},{"location":"ch09/17_counting_reads/#options","title":"Options","text":"<pre><code>-f &lt;format&gt;, --format=&lt;format&gt;\n\n    Format of the input data. Possible values are sam (for text SAM files) and bam (for binary BAM files). Default is sam.\n</code></pre> <pre><code>-s &lt;yes/no/reverse&gt;, --stranded=&lt;yes/no/reverse&gt;\n\n    whether the data is from a strand-specific assay (default: `yes`)\n\n    For stranded=no, a read is considered overlapping with a feature regardless of whether it is mapped to the same or the opposite strand as the feature. For stranded=yes and single-end reads, the read has to be mapped to the same strand as the feature. For paired-end reads, the first read has to be on the same strand and the second read on the opposite strand. For stranded=reverse, these rules are reversed.\n</code></pre> <pre><code>-i &lt;id attribute&gt;, --idattr=&lt;id attribute&gt;\n\n    GFF attribute to be used as feature ID. Several GFF lines with the same feature ID will be considered as parts of the same feature. The feature ID is used to identity the counts in the output table. The default, suitable for RNA-Seq analysis using an Ensembl GTF file, is `gene_id`.\n</code></pre>"},{"location":"ch09/17_counting_reads/#key-considerations","title":"Key Considerations","text":"<p>Stranded vs. Unstranded Data:</p> <ul> <li>Many RNA-Seq protocols preserve strand information. Setting -s yes or -s reverse ensures proper assignment.</li> </ul> <p>Feature Type:</p> <ul> <li>HTSeq-count assigns reads based on featuretype (default is exon in GTF). If needed, use -t to specify other features.</li> </ul> <p>Overlap Mode:</p> <ul> <li>Some reads may overlap multiple features; the -m flag determines how they are assigned (union, intersection-strict, intersection-nonempty).</li> </ul>"},{"location":"ch09/17_counting_reads/#limitations-alternatives","title":"Limitations &amp; Alternatives","text":"<ul> <li>HTSeq-count is a gene-level summarization tool; it does not handle transcript-level quantification.</li> <li>It requires sorted BAM/SAM files.</li> <li>Alternative tools like featureCounts (..which is faster) or Salmon/RSEM (for transcript-level quantification) may be preferred in some cases.</li> </ul>"},{"location":"ch09/17_counting_reads/#class-exercise-folder-content","title":"Class Exercise Folder Content","text":"<pre><code>\u251c\u2500\u2500 bams\n\u2502   \u251c\u2500\u2500 KO_hg19_rep2_sorted.bam\n\u2502   \u251c\u2500\u2500 KO_hg19_rep2_sorted.bam.bai\n\u2502   \u251c\u2500\u2500 KO_hg19_rep3_sorted.bam\n\u2502   \u251c\u2500\u2500 KO_hg19_rep3_sorted.bam.bai\n\u2502   \u251c\u2500\u2500 WT_hg19_rep1_sorted.bam\n\u2502   \u251c\u2500\u2500 WT_hg19_rep1_sorted.bam.bai\n\u2502   \u251c\u2500\u2500 WT_hg19_rep2_sorted.bam\n\u2502   \u251c\u2500\u2500 WT_hg19_rep2_sorted.bam.bai\n\u2502   \u251c\u2500\u2500 WT_hg19_rep3_sorted.bam\n\u2502   \u2514\u2500\u2500 WT_hg19_rep3_sorted.bam.bai\n\u251c\u2500\u2500 chr1-hg19_genes.gtf\n\u251c\u2500\u2500 logs\n\u2502   \u251c\u2500\u2500 KO_hg19_rep1.log\n\u2502   \u251c\u2500\u2500 KO_hg19_rep1.txt\n\u2502   \u251c\u2500\u2500 KO_hg19_rep2.log\n\u2502   \u251c\u2500\u2500 KO_hg19_rep2.txt\n\u2502   \u251c\u2500\u2500 KO_hg19_rep3.log\n\u2502   \u251c\u2500\u2500 KO_hg19_rep3.txt\n\u2502   \u251c\u2500\u2500 WT_hg19_rep1.log\n\u2502   \u251c\u2500\u2500 WT_hg19_rep1.txt\n\u2502   \u251c\u2500\u2500 WT_hg19_rep2.log\n\u2502   \u251c\u2500\u2500 WT_hg19_rep2.txt\n\u2502   \u251c\u2500\u2500 WT_hg19_rep3.log\n\u2502   \u2514\u2500\u2500 WT_hg19_rep3.txt\n\u2514\u2500\u2500 refseq.hg19.bed12\n</code></pre>"},{"location":"ch09/17_counting_reads/#class-exercise-part-a","title":"Class Exercise Part A","text":"<p>Class Exercise: Running RSeQC</p> <ol> <li>Copy Folder: Make a copy of the following folder into your home directory: </li> </ol> <pre><code>/gpfs1/cl/mmg3320/course_materials/htseq_2025_demo\n</code></pre> <ol> <li>Determine Strandedness: HtSeq-count requires setting the <code>-s</code> parameter based on the RNA-Seq library preparation protocol. You will need to run RSeQC to determine strandedness of the demo data. There are three options to select from: <ul> <li><code>-s yes</code>, reads are mapped to the same strand as the sense strand </li> <li><code>-s no</code>, reads can map to either strand (unstranded)</li> <li><code>-s reverse</code>, reads are mapped to the opposite strand (anti-sense)</li> </ul> </li> </ol>"},{"location":"ch09/17_counting_reads/#how-to-run-rseqc","title":"How To Run RSeQC:","text":"<ul> <li>Activate conda environment </li> </ul> <pre><code>conda activate rseqc_env\n</code></pre> <ul> <li>Test that <code>RSeQC</code> is loaded:</li> </ul> <pre><code>infer_experiment.py --help\n</code></pre> <p>Expected Output: </p><pre><code>Usage: infer_experiment.py [options]\nOptions:\n  --version             show program's version number and exit\n  -h, --help            show this help message and exit\n  -i INPUT_FILE, --input-file=INPUT_FILE\n                        Input alignment file in SAM or BAM format\n  -r REFGENE_BED, --refgene=REFGENE_BED\n                        Reference gene model in bed fomat.\n</code></pre> <ul> <li>The <code>rseqc-loop.sh</code> script is provided below. Make a copy and modify the path for variables <code>BAM_DIR</code> and <code>BED_FILE</code>. </li> <li>This was recently updated as of April 8, 2025 to mirror the updated script for RseQC. </li> </ul> <pre><code>#!/bin/bash\n#SBATCH --partition=general\n#SBATCH --nodes=1\n#SBATCH --ntasks=8  # Single task running sequentially\n#SBATCH --mem=20G  # Adjust based on the number of BAM files\n#SBATCH --time=24:00:00\n#SBATCH --job-name=rseqc-loop\n#SBATCH --output=run-%x_%j.out  # %x=job name, %j=job ID\n\n# Load the Apptainer module\nmodule load apptainer/1.3.4\n\n# Path to the RSeQC container\nCONTAINER_PATH=\"/gpfs1/cl/mmg3320/course_materials/containers/rseqc.sif\"\n\n# Define input directory (update if needed)\nBAM_DIR=\"/users/p/d/pdrodrig/htseq_2025/bams\"\nBED_FILE=\"/users/p/d/pdrodrig/htseq_2025/refseq.hg19.bed12\"\nOUTPUT_DIR=\"rseqc_results\"\n\n# Create output directory if it doesn't exist\nmkdir -p \"$OUTPUT_DIR\"\n\n# Loop through all BAM files in the directory\nfor BAM_FILE in \"$BAM_DIR\"/*.bam; do\n    # Extract filename without extension\n    NAME=$(basename \"$BAM_FILE\" .bam)\n\n    echo \"Processing: $NAME\"\n\n    # Run infer_experiment.py inside the Apptainer container\n    apptainer exec \"$CONTAINER_PATH\" infer_experiment.py -r \"$BED_FILE\" -i \"$BAM_FILE\" &gt; \"$OUTPUT_DIR/${NAME}.infer_experiment.txt\"\n\n    # Run read_distribution.py inside the Apptainer container\n    apptainer exec \"$CONTAINER_PATH\" read_distribution.py -r \"$BED_FILE\" -i \"$BAM_FILE\" &gt; \"$OUTPUT_DIR/${NAME}.read_distribution.txt\"\ndone\n</code></pre> <ul> <li>Submit the script and then check your outputs are the same size. This script will take ~5 minutes to run. </li> </ul> <pre><code>-rw-r--r-- 1 pdrodrig pi-jdragon  165 Mar 16 11:38 KO_hg19_rep2_sorted.infer_experiment.log\n-rw-r--r-- 1 pdrodrig pi-jdragon 1.1K Mar 16 11:39 KO_hg19_rep2_sorted.read_distribution.log\n-rw-r--r-- 1 pdrodrig pi-jdragon  165 Mar 16 11:40 KO_hg19_rep3_sorted.infer_experiment.log\n-rw-r--r-- 1 pdrodrig pi-jdragon 1.1K Mar 16 11:40 KO_hg19_rep3_sorted.read_distribution.log\n-rw-r--r-- 1 pdrodrig pi-jdragon  165 Mar 16 11:41 WT_hg19_rep1_sorted.infer_experiment.log\n-rw-r--r-- 1 pdrodrig pi-jdragon 1.1K Mar 16 11:41 WT_hg19_rep1_sorted.read_distribution.log\n-rw-r--r-- 1 pdrodrig pi-jdragon  165 Mar 16 11:42 WT_hg19_rep2_sorted.infer_experiment.log\n-rw-r--r-- 1 pdrodrig pi-jdragon 1.1K Mar 16 11:42 WT_hg19_rep2_sorted.read_distribution.log\n-rw-r--r-- 1 pdrodrig pi-jdragon  165 Mar 16 11:43 WT_hg19_rep3_sorted.infer_experiment.log\n-rw-r--r-- 1 pdrodrig pi-jdragon 1.1K Mar 16 11:44 WT_hg19_rep3_sorted.read_distribution.log\n</code></pre> <ul> <li> <p>Run multiqc in the <code>rseqc_results/</code> folder to determine strandedness of the FASTQ files. </p> </li> <li> <p>Note: I tried running the multiqc-rseqc module yesterday and continued to get the issue: <code>The 'rseqc' MultiQC module broke...</code> </p> </li> </ul> <p>If this happens to you, please download the <code>rseqc_results</code> folder and use the site Seqera-Multiqc instead.  Did you forget how to download files? Go to Frequently Asked Questions</p> <p>I will need to work with the VACC to find a permanent solution.</p>"},{"location":"ch09/17_counting_reads/#class-exercise-part-b","title":"Class Exercise Part B","text":"<p>Class Exercise: Running HTSeq-count</p> <ol> <li>Modify <code>htseq-count.sh</code> script: You will need to alter the following:<ul> <li>add the required program modules </li> <li>add the correct path to the GTF file </li> <li><code>-s</code> options include yes, no, or reverse</li> <li><code>-i</code> specify <code>gene_id</code></li> </ul> </li> <li>Submit the <code>htseq-count.sh</code> script after modifying it. This should only take a few minutes. </li> <li>Look inside of the <code>htseq-count_XXXXXX.out</code> file after the job is completed. It should appear identical as below: </li> </ol> <pre><code>Processing: KO_hg19_rep2_sorted\nProcessing: KO_hg19_rep3_sorted\nProcessing: WT_hg19_rep1_sorted\nProcessing: WT_hg19_rep2_sorted\nProcessing: WT_hg19_rep3_sorted\n</code></pre> <p>The <code>htseq-count.sh</code> script is below: </p> <pre><code>#!/bin/bash\n#SBATCH --partition=general \n#SBATCH --nodes=1\n#SBATCH --ntasks=4\n#SBATCH --mem=10G\n#SBATCH --time=30:00:00\n#SBATCH --job-name=htseq-count    \n#SBATCH --output=%x_%j.out  # %x=job-name, %j=jobid\n\n# Load HTSeq module\n\n\n# Iterate through all BAM files in the current directory\nfor BAM_FILE in *.bam; do\n\n# Extract the filename without the .bam extension\nNAME=$(basename \"$BAM_FILE\" .bam)\necho \"Processing: $NAME\"\n\n# Run HTSeq-count\nhtseq-count -f bam  -s -i -m union \"$BAM_FILE\" /users/p/d/pdrodrig/htseq_2025/chr1-hg19_genes.gtf &gt; \"${NAME}.gene_id.count.txt\" 2&gt; \"${NAME}.gene_id.summary\"\n\ndone\n</code></pre> <ul> <li><code>&gt;</code> \u2192 Redirects the gene counts output to results/counts/sample_counts.txt.</li> <li><code>2&gt;</code> \u2192 Redirects the summary of assigned/unassigned reads to results/counts/sample_counts.summary.</li> </ul>"},{"location":"ch09/17_counting_reads/#class-exercise-part-c","title":"Class Exercise Part C","text":"<p>Class Exercise: Multiqc break</p> <ol> <li>Read the section below titled <code>htseq-count</code> output</li> <li>Generate a final multiqc output; this time do so inside of the <code>bams\\</code> folder.  This should work with no issue, it was only the RSeQC Module which is broken</li> </ol>"},{"location":"ch09/17_counting_reads/#htseq-count-output","title":"<code>htseq-count</code> output","text":"<p>The output of htseq-count consists of two main files for each sample:</p> <ul> <li>A summary/log file that reports how many reads were assigned to features and why some reads were unassigned.</li> <li>View it with:</li> </ul> <pre><code>head KO_hg19_rep2_sorted.gene_id.summary \n</code></pre> <pre><code>76767 GFF lines processed.\n100000 alignment records processed.\n199252 alignment records processed.\n</code></pre> <ul> <li>A counts file that lists the number of reads mapped to each gene or feature. This is a tab-delimited file with gene IDs and their associated read counts. </li> <li>View it with:</li> </ul> <pre><code>head KO_hg19_rep2_sorted.gene_id.count.txt\n</code></pre> <pre><code>AADACL3 0\nAADACL4 0\nABCA4   0\nABCB10  0\nABCD3   1\nABL2    0\nACADM   0\nACAP3   0\nACBD3   0\nACBD6   1\n</code></pre> <pre><code>tail KO_hg19_rep2_sorted.gene_id.count.txt\n</code></pre> <pre><code>__no_feature    32813\n__ambiguous     4372\n__too_low_aQual 0\n__not_aligned   15209\n__alignment_not_unique  3667\n</code></pre>"},{"location":"ch09/17_counting_reads/#resource-recommendations-for-htseq-count","title":"Resource Recommendations for <code>htseq-count</code>","text":"<p>An array will submit independent jobs and process each BAM file independently. </p> <pre><code>#!/bin/bash\n#SBATCH --partition=general\n#SBATCH --array=1-8  # Adjust for the number of samples (1 per sample)\n#SBATCH --nodes=1\n#SBATCH --ntasks=1  \n#SBATCH --mem=4G  # 2-6GB, 4GB is typically enough but specify 6GB for 100M+ reads\n#SBATCH --time=2:00:00\n#SBATCH --job-name=htseq-array\n#SBATCH --output=htseq-%A_%a.out  # %A = job ID, %a = array task ID\n</code></pre>"},{"location":"ch09/17_counting_reads/#creating-bigwig-files","title":"Creating bigWig files","text":"<p>We will now take our BAM files and convert them into bigWig files. The bigWig format is an indexed binary format useful for dense, continuous data that can be displayed in a genome browser as a graph/track.</p> <p>To create bigWig files we will use <code>deepTools</code>, a suite of Python tools developed for the efficient analysis of high-throughput sequencing data, such as ChIP-seq, RNA-seq or MNase-seq. <code>deepTools</code> has a wide variety of commands that go beyond what we will cover today. </p>"},{"location":"ch09/17_counting_reads/#setting-up","title":"Setting up","text":"<p>Checking/Creating index file for the BAM file: Often, when working with BAM files you will find that many tools require an index (an associated <code>.bai</code> file). You can think of an index similar to that which is located at the back of a textbook - when you are interested in a particular subject, you look for the keyword in the index and identify the pages that contain the relevant information. Similarily, indexing the BAM file aims to achieve fast retrieval of alignments overlapping a specified region without going through the whole alignment file. Essentially, a <code>bai</code> file along with the <code>bam</code> ensures that downstream applications are able to use the information with the <code>bam</code> file much more speedily.</p> <p>As a reminder, we used SAMtools, specifically the <code>samtools index</code> command, to index the BAM files.</p>"},{"location":"ch09/17_counting_reads/#bamcoverage-from-deeptools","title":"bamCoverage from deepTools","text":"<p>This command takes a BAM file as input and evaluates which areas of the genome have reads associated with them, i.e. how much of the genome is \"covered\" with reads. The coverage is calculated as the number of reads per bin, where bins are short consecutive sections of the genome (bins) that can be defined by the user. The output of this command is a bigWig file. </p> <p>These are some parameters of bamCoverage that are worth considering:</p> <ul> <li><code>normalizeUsing</code>: Possible choices: RPKM, CPM, BPM, RPGC. By default, no normalization is applied. More on this below. </li> <li><code>binSize</code>: size of bins in bases (default is 50)</li> <li><code>--effectiveGenomeSize</code>: the portion of the genome that is mappable. It is useful to consider this when computing your scaling factor.</li> <li><code>smoothLength</code>: defines a window, larger than the <code>binSize</code>, to average the number of reads over. This helps produce a more continuous plot.</li> <li><code>centerReads</code>: reads are centered with respect to the fragment length as specified by <code>extendReads</code>. This option is useful to get a sharper signal around enriched regions.</li> </ul> <p>Selecting Normalization method: The methods for bigWig creation (<code>bamCoverage</code> and <code>bamCompare</code>) allows for normalization, which is great if we want to compare different samples to each other and they vary in terms of sequencing depth. DeepTools offers different methods of normalization as listed below, each is perfomed per bin. The default is no normalization.</p> <ul> <li>Reads Per Kilobase per Million mapped reads (RPKM)<ul> <li>number of reads per bin / (number of mapped reads (in millions) * bin length (kb))</li> </ul> </li> <li>Counts per million (CPM); this is similar to CPM in RNA-seq<ul> <li>number of reads per bin / number of mapped reads (in millions)</li> </ul> </li> <li>Bins Per Million mapped reads (BPM); same as TPM in RNA-seq<ul> <li>number of reads per bin / sum of all reads per bin (in millions)</li> </ul> </li> <li>Reads per genomic content (RPGC)<ul> <li>number of reads per bin / scaling factor for 1x average coverage </li> <li>scaling factor is determined from the sequencing depth: total number of mapped reads * fragment length) / effective genome size</li> <li>this option requires an effectiveGenomeSize</li> </ul> </li> </ul> <p>We will be using the bare minimum of parameters as shown in the code below. We decrease the bin size to increase the resolution of the track (this also means larger file size). If you are interested, feel free to test out some of the other parameters to create different bigWig files. You can load them into a genome viewer like IGV and observe the differences.</p> <p>Class Exercise: Running deeptools/bamCoverage</p> <p>Let's create a bigWig file for <code>KO_hg19_rep2_sorted.bam</code> and <code>WT_hg19_rep2_sorted.bam</code>: </p> <pre><code>module load deeptools/3.5.5 \n</code></pre> <p></p><pre><code>bamCoverage -b KO_hg19_rep2_sorted.bam -o KO_hg19_rep2_sorted.bw \n</code></pre> Note: Normally, this command can take up to 10 minutes to complete. <p>Visualize with IGV:</p> <ul> <li>Start IGV, You may have this previously installed on your laptop. If not no worries, use the IGV Web App. </li> <li>Load the Human genome (hg19) into IGV using the dropdown menu at the top left of your screen. </li> <li>Load the .bw file using the \u201cLoad from File\u2026\u201c or \"Tracks\" option. </li> <li>Type MOV10 into the search bar.</li> </ul> <p>This lesson has been developed by members of the teaching team at the Harvard Chan Bioinformatics Core (HBC). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>"},{"location":"ch10/18_setup_R/","title":"18 setup R","text":""},{"location":"ch10/18_setup_R/#setup","title":"Setup","text":""},{"location":"ch10/18_setup_R/#r-and-rstudio","title":"R and RStudio","text":"<ul> <li>R and RStudio are separate downloads and installations. R is the underlying statistical computing environment, but using R alone is no fun. RStudio is a graphical integrated development environment (IDE) that makes using R much easier and more interactive. You need to install R before you install RStudio. After installing both programs, you will need to install some specific R packages within RStudio. Follow the instructions below for your operating system, and then follow the instructions to install packages.</li> </ul>"},{"location":"ch10/18_setup_R/#you-are-running-windows","title":"You are running Windows","text":""},{"location":"ch10/18_setup_R/#if-you-already-have-r-and-rstudio-installed","title":"If you already have R and RStudio installed","text":"<ul> <li> <p>Open RStudio, and click on \"Help\" &gt; \"Check for updates\". If a new version is   available, quit RStudio, and download the latest version for RStudio.</p> </li> <li> <p>To check which version of R you are using, start RStudio and the first thing   that appears in the console indicates the version of R you are   running. Alternatively, you can type <code>sessionInfo()</code>, which will also display   which version of R you are running. Go on   the CRAN website and check   whether a more recent version is available. If so, please download and install   it. You can check here for   more information on how to remove old versions from your system if you wish to do so.</p> </li> <li> <p>Follow the steps in the instructions for everyone at the   bottom of this page.</p> </li> </ul>"},{"location":"ch10/18_setup_R/#if-you-dont-have-r-and-rstudio-installed","title":"If you don't have R and RStudio installed","text":"<ul> <li> <p>Download R from   the CRAN website.</p> </li> <li> <p>Run the <code>.exe</code> file that was just downloaded</p> </li> <li> <p>Go to the RStudio download page</p> </li> <li> <p>Under All Installers select RStudio xxxx.yy.zz-uuu.exe - Windows 10/11 (where x, y, z, and u represent version numbers)</p> </li> <li> <p>Double click the file to install it</p> </li> <li> <p>Once it's installed, open RStudio to make sure it works and you don't get any   error messages</p> </li> <li> <p>Follow the steps in the instructions for everyone at the   bottom of this page.</p> </li> </ul>"},{"location":"ch10/18_setup_R/#you-are-running-macos","title":"You are running macOS","text":""},{"location":"ch10/18_setup_R/#if-you-already-have-r-and-rstudio-installed_1","title":"If you already have R and RStudio installed","text":"<ul> <li> <p>Open RStudio, and click on \"Help\" &gt; \"Check for updates\". If a new version is   available, quit RStudio, and download the latest version for RStudio.</p> </li> <li> <p>To check the version of R you are using, start RStudio and the first thing   that appears on the terminal indicates the version of R you are running. Alternatively, you can type <code>sessionInfo()</code>, which will   also display which version of R you are running. Go on   the CRAN website and check   whether a more recent version is available. If so, please download and install   it.</p> </li> <li> <p>Follow the steps in the instructions for everyone at the   bottom of this page.</p> </li> </ul>"},{"location":"ch10/18_setup_R/#if-you-dont-have-r-and-rstudio-installed_1","title":"If you don't have R and RStudio installed","text":"<ul> <li> <p>Download R from   the CRAN website.</p> </li> <li> <p>Select the <code>.pkg</code> file for the latest R version</p> </li> <li> <p>Double click on the downloaded file to install R</p> </li> <li> <p>It is also a good idea to install XQuartz (needed   by some packages)</p> </li> <li> <p>Go to the RStudio download page</p> </li> <li> <p>Under All Installers select RStudio xxxx.yy.zz-uuu.dmg - macOS 10.15+ (where x, y, z, and u represent version numbers)</p> </li> <li> <p>Double click the file to install RStudio</p> </li> <li> <p>Once it's installed, open RStudio to make sure it works and you don't get any   error messages.</p> </li> <li> <p>Follow the steps in the instructions for everyone at the   bottom of this page.</p> </li> </ul>"},{"location":"ch10/18_setup_R/#you-are-running-linux","title":"You are running Linux","text":""},{"location":"ch10/18_setup_R/#install-r-using-your-package-manager-and-rstudio","title":"Install R using your package manager and RStudio","text":"<ul> <li>Follow the instructions for your distribution   from CRAN, they provide information   to get the most recent version of R for common distributions. For most   distributions, you could use your package manager (e.g., for Debian/Ubuntu run   <code>sudo apt-get install r-base</code>, and for Fedora <code>sudo yum install R</code>), but we   don't recommend this approach as the versions provided by this are   usually out of date. In any case, make sure you have at least R 4.2.0.</li> <li>Go to the RStudio download   page</li> <li>Under All Installers select the version that matches your distribution, and   install it with your preferred method (e.g., with Debian/Ubuntu <code>sudo dpkg -i rstudio-xxxx.yy.zz-uuu-amd64.deb</code> at the terminal).</li> <li>Once it's installed, open RStudio to make sure it works and you don't get any   error messages.</li> <li>Follow the steps in the instructions for everyone</li> </ul>"},{"location":"ch10/18_setup_R/#for-everyone","title":"For everyone","text":"<p>After installing R and RStudio, you need to install a couple of packages that will use for the semester. We will also learn about package installation during the course to explain the following commands. For now, simply follow the instructions below:</p> <ul> <li>Start RStudio by double-clicking the icon and then type:</li> </ul> <pre><code>install.packages(c(\"BiocManager\"))\nBiocManager::install(c(\"tidyverse\", \"DESeq2\"))\n</code></pre> <p>The tutorial we will be following for today's basics in R class can be found here </p>"},{"location":"ch11/19_intro-to-tidy/","title":"19 intro to tidy","text":""},{"location":"ch11/19_intro-to-tidy/#introduction-to-tidyverse","title":"Introduction to Tidyverse","text":"<p>The Tidyverse suite of integrated packages are designed to work together to make common data science operations more user friendly. The packages have functions for data wrangling, tidying, reading/writing, parsing, and visualizing, among others. There is a freely available book, R for Data Science, with detailed descriptions and practical examples of the tools available and how they work together. We will explore the basic syntax for working with these packages, as well as, specific functions for data wrangling with the \u2018dplyr\u2019 package and data visualization with the \u2018ggplot2\u2019 package. </p>"},{"location":"ch11/19_intro-to-tidy/#getting-started","title":"Getting Started","text":"<ol> <li>Please make a copy of the following folder:</li> </ol> <pre><code>/gpfs1/cl/mmg3320/course_materials/Intro-to-tidyverse\n</code></pre> <ol> <li> <p>Open <code>.Rmd</code> file in R/RStudio </p> </li> <li> <p>Install the following R packages: </p> </li> </ol> <pre><code>install.packages(\"palmerpenguins\")\ninstall.packages(\"tidyverse\")\n</code></pre>"},{"location":"ch11/19_intro-to-tidy/#for-march-26th-class","title":"For March, 26th class","text":"<p>Please copy the file into your Intro-to-tidyverse folder</p> <pre><code>/gpfs1/cl/mmg3320/course_materials/Intro-to-tidyverse/Intro_to_Tidyverse_partII_edit.Rmd\n</code></pre>"},{"location":"ch12/21_rnaseq_dataset/","title":"DESeq2 dataset","text":""},{"location":"ch12/21_rnaseq_dataset/#deseq2-dataset","title":"DESeq2 dataset","text":"<p>For today's lesson we will be working with RNA-seq data from a recent publication in Nature Communications by Shan et al. (2021). Before spring break, our final end product was the creation of counts files using HTSeq-count. </p> <p>So what does the count data actually represent? The count data used for differential expression analysis represents the number of sequence reads that originated from a particular gene. The higher the number of counts, the more reads associated with that gene, and the assumption that there was a higher level of expression of that gene in the sample. </p> <p>Our overall goal is to identify a list of genes that are statistically distinct between the groups being compared. </p>"},{"location":"ch12/21_rnaseq_dataset/#the-publication","title":"The publication","text":"<p>T cell identity is established during thymic development, but how it is maintained in the periphery remains unknown. </p> <p>The authors in this paper discover that by ablating Tcf1 and Lef1 transcription factors in mature CD8+ T cells this induces the expression of genes from non-T cell lineages. We will focus on analyzing the Tcf7 dataset. Specifically, Tcf7 fl/fl mice were crossed with hCD2-Cre mice to create mature CD8+ T cells that lacked Tcf7. The protein coded by Tcf7 is Tcf1. </p>"},{"location":"ch12/21_rnaseq_dataset/#the-figures-we-will-create-and-interpret","title":"The figures we will create and interpret","text":"<ul> <li>PCA &amp; correlation heatmap to show clusters of samples based on their similarity </li> <li>Box plots to show expression (normalized counts) of select genes </li> <li>Heatmap to show the expression of the significantly up- and down-regulated genes in the Tcf7 knock-out (KO)  versus the wildtype (WT) </li> <li>Volcano plots (essentially a scatter plot) to show the significance (adjusted p-value) and magnitude (log2FoldChange) of expression change between the WT samples versus the Tcf7 KO samples </li> <li>Pathway analysis to show the relevant pathways that are associated with up and down-regulated genes. </li> </ul>"},{"location":"ch12/21_rnaseq_dataset/#getting-started","title":"Getting Started","text":"<p>Copy this folder into your home directory:</p> <p></p><pre><code>/gpfs1/cl/mmg3320/course_materials/R_tutorials/ENSG_counts\n</code></pre> + Open the file RNA-Seq_DESeq2_tutorial_EDIT.Rmd to start."},{"location":"ch12/21_rnaseq_dataset/#install-the-following-packages-if-you-need-to","title":"Install the following packages if you need to!","text":"<p>You will see an image like this if you are missing any packages required to run the .Rmd file </p> <pre><code>#install.packages(c(\"knitr\", \"RColorBrewer\", \"pheatmap\"))\nBiocManager::install(\"apeglm\") # Update all/some/none? [a/s/n] - type n\n\nlibrary(knitr)\nlibrary(DESeq2) \nlibrary(RColorBrewer)\nlibrary(pheatmap)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(apeglm)\n</code></pre> <p>Install these later on to generate the final image. These will take a bit longer to install. </p> <pre><code>library(ggplotify)\nlibrary(ggpubr)\nlibrary(grid)\nlibrary(cowplot)\nlibrary(magick) # not sure this works on VACC-OOD\n</code></pre>"},{"location":"ch12/21_rnaseq_dataset/#citation","title":"Citation","text":"<p>Shan, Q., Li, X., Chen, X. et al. Tcf1 and Lef1 provide constant supervision to mature CD8+ T cell identity and function by organizing genomic architecture. Nat Commun 12, 5863 (2021). https://doi.org/10.1038/s41467-021-26159-1</p>"},{"location":"ch13/22_rnaseq_dataset/","title":"Visualizing RNA-Seq data","text":""},{"location":"ch13/22_rnaseq_dataset/#visualizing-rna-seq-data","title":"Visualizing RNA-Seq data","text":"<p>For today's lesson we will be working with RNA-seq data from another recent publication in ImmunoHorizons by Sabikunnahar et al. (2025).</p> <p>Summary: Innate immune cells rapidly respond to microbial threats, but if unchecked, these responses can harm the host, as seen in septic shock. To explore how genetic variation influences these responses, researchers compared standard lab mice (C57BL/6) with genetically diverse wild-derived mice (PWD). </p> <p>Experimental Design: The authors isolated bone marrow derived dendritic cells from B6, c11.2, and PWD mice and then either left them unstimulated or stimulated with LPS. </p> <p>This experiment has:</p> <ul> <li>Two treatments: <code>untreated</code>, <code>LPS</code></li> <li>Three genotypes: <code>B6</code>, <code>c11.2</code>, and <code>PWD</code> </li> </ul> <p>The goal:</p> <ul> <li>To test how LPS stimulation affects each genotype </li> <li>Whether the response to LPS differs between genotype </li> </ul>"},{"location":"ch13/22_rnaseq_dataset/#getting-started-monday-april-7th","title":"Getting Started: Monday, April 7th","text":"<p>Copy this folder into your home directory:</p> <pre><code>/gpfs1/cl/mmg3320/course_materials/R_tutorials/ENSM_counts\n</code></pre> <ul> <li>Open the file RNA-Seq_DESeq2_PartII_EDIT.Rmd to start. </li> </ul>"},{"location":"ch13/22_rnaseq_dataset/#the-figures-we-will-create-will-be","title":"The figures we will create will be:","text":"<ul> <li>Figure 2, panel H: LPS treated (PWD vs B6) Volcano Plot</li> <li>Figure 5, panel B: PCA of the three strains used in this experiment</li> </ul>"},{"location":"ch13/22_rnaseq_dataset/#getting-started-wednesday-april-9th","title":"Getting Started: Wednesday, April 9th","text":"<p>Copy this folder into your home directory:</p> <pre><code>/gpfs1/cl/mmg3320/course_materials/R_tutorials/ENSM_partIII\n</code></pre> <ul> <li>Open the file RNA-Seq_Visualization_PartIII_EDIT.Rmd to start. </li> <li>We will need to check if <code>dds.RDS</code> is usable. </li> </ul>"},{"location":"ch13/22_rnaseq_dataset/#the-figures-we-will-create-will-be_1","title":"The figures we will create will be:","text":"<ul> <li>Figure 3, boxplot representation for IL6 and Nos2 expression </li> <li>Heatmap of all +5,000 DE genes (PWD_LPS vs B6_LPS)</li> </ul>"},{"location":"ch14/23_rnaseq_dataset/","title":"Visualizing RNA-Seq data","text":""},{"location":"ch14/23_rnaseq_dataset/#visualizing-rna-seq-data","title":"Visualizing RNA-Seq data","text":"<p>For today's lesson we will be working with RNA-seq data from the publication in ImmunoHorizons by Sabikunnahar et al. (2025).</p> <p>Summary: Innate immune cells rapidly respond to microbial threats, but if unchecked, these responses can harm the host, as seen in septic shock. To explore how genetic variation influences these responses, researchers compared standard lab mice (C57BL/6) with genetically diverse wild-derived mice (PWD). </p> <p>Experimental Design: The authors isolated bone marrow derived dendritic cells from B6, c11.2, and PWD mice and then either left them unstimulated or stimulated with LPS. </p> <p>This experiment has:</p> <ul> <li>Two treatments: <code>untreated</code>, <code>LPS</code></li> <li>Three genotypes: <code>B6</code>, <code>c11.2</code>, and <code>PWD</code> </li> </ul> <p>The goal:</p> <ul> <li>To test how LPS stimulation affects each genotype </li> <li>Whether the response to LPS differs between genotype </li> </ul>"},{"location":"ch14/23_rnaseq_dataset/#getting-started-monday-april-14th","title":"Getting Started: Monday, April 14th","text":"<p>Copy this folder into your home directory:</p> <pre><code>/gpfs1/cl/mmg3320/course_materials/R_tutorials/ENSM_partIV\n</code></pre> <ul> <li>Open the file RNASeq_Visualizations_PartIV_EDIT.Rmd to start. </li> <li>Slides discussed can be found here</li> </ul>"},{"location":"ch14/23_rnaseq_dataset/#getting-started-wednesday-april-16th","title":"Getting Started: Wednesday, April 16th","text":"<ul> <li>No in-person class (see Monday's slides for details)</li> </ul>"},{"location":"ch14/Choose_Your_Adventure/","title":"Choose Your Adventure","text":""},{"location":"ch14/Choose_Your_Adventure/#choose-your-own-adventure","title":"Choose Your Own Adventure","text":"<p>Class: Wednesday, April 16th no in-person class</p> <p>You\u2019ve built a strong foundation in R\u2014now it\u2019s time to put those skills to work! In these quick challenges, you can choose to do 1, 2, or all of them! </p>"},{"location":"ch14/Choose_Your_Adventure/#multipanel-figure-creation","title":"Multipanel Figure Creation","text":"<p>Use <code>ggplot2</code>, <code>patchwork</code>, or <code>cowplot</code> to craft a polished, PDF \"publication-ready\" figure that tells a clear visual story.</p> <p>Completed tutorial can be found here</p> <p>Find the data folder here: </p> <pre><code>/gpfs1/cl/mmg3320/course_materials/R_tutorials/multi_panel_figure\n</code></pre>"},{"location":"ch14/Choose_Your_Adventure/#microbiome-analysis-with-ampvis2","title":"Microbiome Analysis with ampvis2","text":"<p>Dive into microbiome data using <code>ampvis2</code>, a powerful R package designed for simplified analysis and visualization. Easily generate ordination plots, heatmaps, and alpha/diversity metrics. </p> <p>Completed tutorial can be found here</p> <p>Find the data folder here: </p> <pre><code>/gpfs1/cl/mmg3320/course_materials/R_tutorials/microbiome\n</code></pre>"},{"location":"ch14/Choose_Your_Adventure/#gene-set-enrichment-analysis-gsea","title":"Gene Set Enrichment Analysis (GSEA)","text":"<p>Continue using <code>clusterProfiler</code> to interpret gene expression results and uncover enriched biological pathways. Rather than just focusing on individual genes, GSEA analyzes gene sets as a whole, making it particularly useful for understanding complex biological processes and disease mechanisms.</p>"},{"location":"img/","title":"Index","text":""},{"location":"img/#all-images-for-session-ii-of-ngs-data-analysis-course","title":"All images for Session II of NGS Data Analysis Course","text":""},{"location":"pdf_materials/","title":"Overview","text":""},{"location":"pdf_materials/#pdf-overview","title":"PDF Overview","text":"<p>Each tutorial will also be made available as PDF printout document. These lessons can be reviewed prior to class start. </p> <p>Introducing the Shell (L1)</p> <p>Navigating the Shell (L2)</p> <p>Wildcards &amp; Nano (L3)</p> <p>Searching &amp; Navigation (L4)</p> <p>Intro to Shell Scripting (L5)</p> <p>RNA-Seq Overview (L7)</p> <p>Accessing Public Data (L8)</p>"},{"location":"pdf_materials/#assessing-fastqc-outputs-l909_assessing_fastqc_output-printoutpdf","title":"Assessing FASTQC Outputs (L9)](09_assessing_fastqc_output-printout.pdf)","text":""},{"location":"pdf_materials/#trimming-filtering-l1010_trimming-printoutpdf","title":"Trimming &amp; Filtering (L10)](10_trimming-printout.pdf)","text":""},{"location":"slides/","title":"Overview","text":""},{"location":"slides/#slide-overview","title":"Slide Overview","text":"<p>Slides will be made available as a PDF printout document. These lessons can be reviewed prior to class start. </p> <p>Introducing the Shell (L1)</p> <p>Navigating the Shell (L2)</p> <p>Wildcards &amp; Nano (L3)</p> <p>Searching &amp; Redirection (L4)</p> <p>Intro to Shell Scripting (L5)</p> <p>RNA-Seq Overview (L7)</p> <p>Accessing Public Data (L8)</p> <p>Assessing FASTQC Outputs (L9)</p> <ul> <li>File Path to CE2: /gpfs1/cl/mmg3320/course_materials/FASTQC_example</li> </ul>"},{"location":"slides/#trimming-filtering-l1010_trimming-slidespdf","title":"Trimming &amp; Filtering (L10)](10_trimming-slides.pdf)","text":""},{"location":"slides/#alignment-l11-1311_align-slidespdf","title":"Alignment (L11-13)](11_align-slides.pdf)","text":""},{"location":"slides/#alignment-outputs-l1512_align-outputspdf","title":"Alignment Outputs (L15)](12_align-outputs.pdf)","text":""},{"location":"slides/#rseqc-l1613_rseqcpdf","title":"RSeQC (L16)](13_rseqc.pdf)","text":""},{"location":"slides/#counting-reads-l1714_counting_reads-slidespdf","title":"Counting Reads (L17)](14_counting_reads-slides.pdf)","text":""}]}